<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>ElevenLabs Blog</title>
    <link>https://elevenlabs.io/blog</link>
    <description><![CDATA[Latest updates from ElevenLabs]]></description>
    <language>en-US</language>
    <lastBuildDate>Mon, 22 Dec 2025 23:20:36 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>TVS Motor Company deploys multimodal AI agents using ElevenLabs</title>
      <link>https://elevenlabs.io/blog/tvs-motor-multimodal-agents</link>
      <description><![CDATA[TVS Motor Company, a global manufacturer and mobility solutions provider, is redefining digital customer engagement with multimodal AI agents that combine voice and text – built usingElevenLabs’ Agents Platform. Early results show a35% lift in lead capture and a 80% reduction in customer feedback turnaroundtimes across 25+ countries. The agents are live across TVS’ international websites in Africa, Latin America, Middle East, Asia, and ASEAN, powering both pre-sales and post-sales interactions. Each interaction is localized and real-time, improving conversion and accelerating feedback loops. Why TVS Motor chose ElevenLabs In Q1 of the current financial year, TVS evaluated multiple architectures for its agent platform, including speech-to-speech and pipeline-based systems. After benchmarking various solutions, they selected ElevenLabs for its: unmatched low latency natural voice quality multilingual support critical to their global footprint Integration was completed in under a week. ElevenLabs’ Agent APIs, combined with support for major cloud telephony providers such as Twilio and Plivo, allowed TVS to deploy, test, and refine agents rapidly without compromising on voice quality or responsiveness. Anand Das, Chief Digital & AI Officer, International Business, TVS Motor Company. From static CTAs to high-converting AI agents TVS replaced static web forms with Shop AI agents that now handle product discovery and lead capture autonomously through a multimodal widget that combines voice and text on their websites. Post-sales interactions, such as NPS and feedback collection, are handled via outbound voice calls initiated by Voice AI agents. Early data shows a 35% lead capture rate – compared to traditional CTAs. TVS Motor Website Agent: Real-time customer feedback, localized voices For post-sales, TVS deployed Voice AI agents to conduct NPS calls and collect customer feedback. By localizing voice personas across Latin America, ASEAN, Middle East, and Africa, and fine-tuning tone per use case – empathetic for support, energetic for sales – TVS ensured that every interaction felt natural and culturally aligned. As a result, TVS reduced the cost per feedback by more than 35% and cut turnaround times by 80%. Built for global markets TVS agents now support nine languages: English, Hindi, Tamil, Bahasa, Spanish, Arabic, French, Turkish, and Italian. All product discovery and lead capture flows are fully automated. Each voice persona reflects regional accents and speech patterns, enhancing user trust and relatability. Naga Budigam, Enterprise AI Innovation & Acceleration Lead at TVS Motor Company. Scaling with ElevenLabs’ platform TVS primarily leverages the Agents API, integrating conversational AI directly into their websites. This enables seamless multimodal interactions, local voice rendering, and continuous optimization across markets. With no agents previously deployed, this partnership represents a step-function improvement – scaling from zero to 25+ countries with fully localized automation in weeks. TVS Motor’s deployment shows how fast global teams can scale multimodal AI with natural-sounding, low-latency voice. If you are building real-time customer engagement or automated service workflows across markets, get in touch.]]></description>
      <pubDate>Mon, 22 Dec 2025 16:11:12 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/tvs-motor-multimodal-agents</guid>
    </item>
    <item>
      <title>Introducing Eleven v3 (alpha)</title>
      <link>https://elevenlabs.io/blog/eleven-v3</link>
      <description><![CDATA[We're pleased to revealEleven v3 (alpha)— the most expressive Text to Speech model. This research preview brings unprecedented control and realism to speech generation with: 70+ languages Multi-speaker dialogue Audio tagslike [excited], [whispers], and [sighs] Eleven v3 (alpha) requires moreprompt engineeringthan previous models — but the generations are breathtaking. If you’re working on videos, audiobooks, or media tools — this unlocks a new level of expressiveness. For real-time and conversational use cases, we recommend staying with v2.5 Turbo or Flash for now. A real-time version of v3 is in development. Eleven v3 is available today on our website and in theAPI. Why we built v3 Since launching Multilingual v2, we’ve seen voice AI adopted in professional film, game development, education, and accessibility. But the consistent limitation wasn’t sound quality — it wasexpressiveness. More exaggerated emotions, conversational interruptions, and believable back-and-forth were difficult to achieve. Eleven v3 addresses this gap. It was built from the ground up to deliver voices that sigh, whisper, laugh, and react — producing speech that feels genuinely responsive and alive. What’s new in Eleven v3 (alpha) Hear v3 for yourself Using audio tags Audio tags live inline with your script and are formatted with lowercase square brackets. You can see more about audio tags in ourprompting guide for v3 in the docs. Professional Voice Clones (PVCs) are currently not fully optimized for Eleven v3, resulting in potentially lower clone quality compared to earlier models. During this research preview stage it would be best to find an Instant Voice Clone (IVC) or designed voice for your project if you need to use v3 features. PVC optimization for v3 is coming in the near future. For example, you could prompt: “[whispers] Something’s coming… [sighs] I can feel it.” Or for more expressive control, you can combine multiple tags: Crafting multi-speaker dialogue Eleven v3 is supported in our existing Text to Speech endpoint. Additionally, we introduce a newText to Dialogue API endpoint. Provide a structured array of JSON objects — each representing a speaker turn — and the model generates a cohesive, overlapping audio file: The endpoint automatically manages speaker transitions, emotional changes, and interruptions. Learn morehere. v3 is our most expressive model Pricing and availability To enable v3: Use theModel Pickerand selectEleven v3 (alpha) API access and support in Studio are coming soon. For early access, pleasecontact sales. When not to use v3 Eleven v3 (alpha) requires more prompt engineering than our previous models. When it works the output is breathtaking but the reliability and higher latency means it’s not suitable for real-time and conversational use cases. For these, we recommend Eleven v2.5 Turbo/Flash. For more, refer to the fullv3 documentationand FAQ. Try it today Log in toElevenLabs UI Select v3 (alpha)in the model dropdown Paste your script — use tags or dialogue Generate audio We’re excited to see how you bring v3 to life across new use cases — from immersive storytelling to cinematic production pipelines. How does the Eleven v3 80% discount work? How were the samples in the video and website generated? How does dialogue generation work? Is this available over API? What audio tags are supported? What languages does it support?]]></description>
      <pubDate>Thu, 18 Dec 2025 16:12:15 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/eleven-v3</guid>
    </item>
    <item>
      <title>Introducing Scribe v2 Realtime</title>
      <link>https://elevenlabs.io/blog/introducing-scribe-v2-realtime</link>
      <description><![CDATA[Scribe v2 Realtime: the most accurate model for live transcription Scribe v2 Realtime sets a new standard for low-latencySpeech to Text. Designed for live use cases—voice agents, meeting assistants, and real-time captioning—it transcribes speech in under 150 ms across English, French, German, Italian, Spanish, and Portuguese, and 90 languages. Scribe v2 Realtime is specifically built for agentic use cases. On 500 hard samples containing background noise and complex information, it significantly outperforms all other models. Key features Negative latency:Next word and punctuation prediction Automatic language detection:Speak in any language, switch language mid conversation Text conditioning: Scribe v2 Realtime continues the transcription based on the previous batch, useful when restarting a connection Voice Activity Detection(VAD) Manual commit: Full control over when to finalize transcript segments Multiple audio formats: Support for PCM (48kHz) and μ-law encoding Enterprise readywith SOC 2, ISO 27001, PCI DSS L1, HIPAA, and GDPR compliance, EU and India data residency options and Zero retention mode for sensitive workloads Scribe v2 Realtime delivers human-level understanding in real time, enabling natural conversation and immediate response in live environments. Scribe v2 Realtime achieves 93.5% accuracy across 30 commonly used European and Asian languages. Build with the API Scribe v2 Realtime is available today through the ElevenLabs API. Explore the documentation:https://elevenlabs.io/docs/cookbooks/speech-to-text/streaming Use Scribe v2 Realtime in ElevenLabs Agents Deploy natural, human-sounding agents powered by Scribe v2 Realtime. Build voice assistants for support, sales, or in-product experiences that can understand and respond in real time. Learn more:https://elevenlabs.io/agents Start building today Use Scribe v2 Realtime through our API or directly within ElevenLabs Agents. Sign up here:https://elevenlabs.io/app/sign-up]]></description>
      <pubDate>Wed, 17 Dec 2025 23:04:09 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-scribe-v2-realtime</guid>
    </item>
    <item>
      <title>Eleven Music is Here</title>
      <link>https://elevenlabs.io/blog/eleven-music-is-here</link>
      <description><![CDATA[Today, we launchedEleven Music– the next step on our mission to build the most comprehensive AI audio platform in the world. With Eleven Music, businesses, creators, artists, and every single one of our users can generate studio-grade music from natural language prompts, with: - Complete control over genre, style, and structure - Vocals or just instrumental - Multi-lingual, including English, Spanish, German, Japanese and more - Edit the sound and lyrics of individual sections or the whole song A few of our favorite samples Check out a few of our favorite songs generated by the ElevenLabs team thus far: Echoes of Midnight Prompt: “Dreamy, psychedelic, slow Indie Rock, reverb-soaked vocals, retro keys, catchy chorus, analog, phased guitars, liminal, nostalgic feeling, anthem.” Saddles and Shadows Prompt: “An epic track for a cowboy show, wild west, cinematic sound design, guitar twanging with awesome orchestral elements crescendoing to a powerful finale, soundtrack.” Don’t Let Me Go Prompt: “A very retro track from the 1950s with an old crooner male vocalist, charming, vintage, classic, nostalgic, golden oldies, vinyl crackle, catchy vocal hooks.” Obsidian Prompt: “Extremely dark, tense and powerful, cinematic sound design, electronic hybrid, trailer music, evil, braam, braam horns, impacts, boom, rising tension, completely instrumental.” Wanderer of the Moor Prompt: “A young english girl singing an old english folk song, stunning, lonely, thoughtful and almost haunting, fiddle and english folk instrumentation, reverb, short song.” Yellow Bus Jam Jam band song about driving through new york city in a big yellow school bus with 2 long guitar solos and lots of harmonizing We can’t wait to see what you create. Commercial use Created in collaboration with labels, publishers, and artists, Eleven Music is cleared for nearly all commercial uses, from film and television to podcasts and social media videos, and from advertisements to gaming. For more information on supported usage across our different plans,head here. Eleven Music is available today on our website and public API access (please seeeleven music API documentation), with public API access and integration into ourConversational AIplatform coming soon. Check out ourprompt engineering guideto help you master the full range of the model’s capabilities.]]></description>
      <pubDate>Wed, 17 Dec 2025 13:19:07 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/eleven-music-is-here</guid>
    </item>
    <item>
      <title>ElevenLabs Welcomes Matthew McConaughey as New Investor</title>
      <link>https://elevenlabs.io/blog/elevenlabs-welcomes-matthew-mcconaughey-as-new-investor</link>
      <description><![CDATA[At ElevenLabs’ inaugural Summit, the company revealed thatAcademy Award-winning actor Matthew McConaughey has been a part of ElevenLabs story for years– as an investor, early supporter, and now, as a creator. In a video played at the Summit, Matthew McConaughey shared that his newsletter,Lyrics of Livin’, is expanding with a Spanish language version – powered by ElevenLabs. Now, Matthew’s stories and content will be made available in Spanish audio, using his unmistakable voice, and reaching an even wider audience. Subscribe here:https://lyricsoflivin.com/hola “I’m proud to share that I’ve been an investor in ElevenLabs for several years now,” said Matthew McConaughey. “It’s been amazing to see the growth from those early days to where the company, and the technology, is now. What’s remained constant is theextraordinary storytelling capabilities and creative potentialthat ElevenLabs unlocks – something that stood out to me from the start and that speaks to me as a professional storyteller.” “When I first met Matthew, I was struck by how genuinely he connected with our vision and what we’re trying to do at ElevenLabs,” addedMati Staniszewski, CEO and Co-Founder of ElevenLabs. “He wasn’t just excited by the tech – he understood what we’re aiming to achieve creatively. We’re so grateful for his continued insight and support, especially as we expand our work in the creative space.” [To see more highlights from the ElevenLabs Summit, visit:https://summit.elevenlabs.io/]]]></description>
      <pubDate>Fri, 12 Dec 2025 18:05:38 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/elevenlabs-welcomes-matthew-mcconaughey-as-new-investor</guid>
    </item>
    <item>
      <title>Toyota engages fans with AI-powered Brock Purdy experience</title>
      <link>https://elevenlabs.io/blog/toyota</link>
      <description><![CDATA[Toyota’s Northern California Dealers Association andcreative agency H/Llaunched a new kind of branded experience - a dynamic voice-driven activation hosted by an AI-powered version of 49ers quarterback Brock Purdy. Built on ElevenLabs’Agents platform, the experience offers fans a natural, interactive conversation with Brock. It lives on a dedicated landing page -toyotaletsgo.com/brock-purdy- where fans play a voice-driven trivia game, guided by Brock’s digital persona as a method to win a VIP 49ers experience. Conversational fan experience built with ElevenLabs Fans enter a voice-controlled game show hosted by Brock. Behind the scenes, ElevenLabs AI voice agents power the full interaction - from trivia questions to contextual responses - while real-time webhooks sync voice outputs with filmed animations of Brock reacting live. Key product features: Dynamic voice interactions:Brock asks questions, adapts its tone, and responds in real time Custom brand personality:The agent was tuned to reflect Brock’s voice and personality, consistent with Toyota's brand values Live-action visual sync:Responses trigger synchronized on-page animations via webhooks Sweepstakes integration:A completed game triggers a contest entry flow, capturing leads directly from the experience Brand safety protocols:A custom knowledge base maintained compliance with Toyota and NFL brand guidelines The result is a high-engagement campaign that brings Brock’s presence into fans’ homes - not as a video ad, but as a voice-first, two-way interaction. Early results The campaign outperformed traditional marketing channels: 12k+ voice interactionsin the first few weeks ~2 minutes average engagement time- compared to 15-30 seconds for TV or 3-6 seconds for social media More than25% of conversations resulted in meaningful actions, like submitting a lead form or exploring products A new format for brand storytelling This is the first time Toyota has implemented an AI-driven voice experience in its marketing stack. The activation marks a shift from passive media consumption to active, two-way interaction - where users engage with branded content that talks back. By combining ElevenLabsAgents Platformwith custom webhook integrations and real-time animation sync,H/Land Toyota built a fully branded voice-first microsite that deepens fan affinity while driving qualified leads. Built for creative teams and marketing agencies Creative agencies and marketing teams can use ElevenLabs Creative Platform to design and generate content in any format - audio, music, image and video - and use our Agents Platform for immersive and engaging brand experiences. Voice agents can be customized for personality, tone, behavior, and safety - with native webhooks to power dynamic media and animations.]]></description>
      <pubDate>Thu, 04 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/toyota</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs UI: Open-source audio &amp; agent components for the web</title>
      <link>https://elevenlabs.io/blog/elevenlabs-ui</link>
      <description><![CDATA[ElevenLabs UIis a new open source library of customizable React components for building interfaces with theElevenLabs Agents& Audio SDKs. Built onshadcn/ui, it provides full control over UI primitives like waveforms, orbs, messages & more. Examples transcriber-01- An open-source voice dictation component you can drop into any web app: voice-chat-03- a rich multimodal chat interface with state management built in. Pass your Elevenlabs Agent ID as a prop and ship it: Getting started Components are available via the@elevenlabs/agents-clicommand. For example, to install theOrbcomponent, you can run: Read the docsand start building better agent & audio interfaces, faster.]]></description>
      <pubDate>Mon, 01 Dec 2025 12:41:27 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/elevenlabs-ui</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs Agents</title>
      <link>https://elevenlabs.io/blog/introducing-elevenlabs-agents</link>
      <description><![CDATA[FromConversational AIto ElevenLabs Agents We’re renaming Conversational AI to ElevenLabs Agents. It’s a complete platform where you can build, launch and monitor conversational agents that talk, type and take action across phone, web and apps. ElevenLabs Agents is a better name for this. Since launch, customers have created more than 2 million agents. Together, they’ve handled over 33 million conversations this year. What makes ElevenLabs Agents different Connected to your knowledge base, tools, and telephony, our multimodal agents can manage complex workflows while maintaining enterprise-grade reliability and control. Customers are already usingElevenLabs Agentsto: Resolve customer issues Qualify leads Run outbound calling at scale Support employee learning and development Power dynamic NPCs in games Act as personal assistants or tutors Our goal is simple: make conversations with technology as natural as speaking with a person. With our audio research and orchestration platform, startups, SMBs, and enterprises can deliver personalized one-to-one interactions at scale. What’s next for ElevenLabs Agents We’re expanding the platform to give developers more control and better performance. Coming soon: Visual workflow builderto handle complex business logic Testing suiteto run simulations to ensure agents act as expected Expressive modefor more realistic voice interactions Expanded integrationswith Google Calendar, Salesforce, Zendesk, and more Build your first ElevenLabs Agent We believe conversationalvoice agentswill become a core part of how people interact with technology. With ElevenLabs Agents, you can build, deploy, and monitor them in one place. Start building today:https://elevenlabs.io/app/agents]]></description>
      <pubDate>Tue, 25 Nov 2025 21:06:53 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-elevenlabs-agents</guid>
    </item>
    <item>
      <title>Expanding access: patients and clinicians can now apply directly on the ElevenLabs website</title>
      <link>https://elevenlabs.io/blog/expanding-access-patients-and-clinicians-can-now-apply-directly-on-the-elevenlabs-website</link>
      <description><![CDATA[Patients with permanent speech loss - and their speech-language pathologists, occupational therapists, or AAC specialists - can now apply for free voice licenses directly on the ElevenLabs website through our Impact partners. How it works To begin, applicants need to create a free ElevenLabs account. After clicking on their profile photo, they will select"Apply for Impact Program,"then follow the on-screen steps to apply through the nonprofit organization that supports their region and diagnosis.Clinicianscan also apply for a free 1-year licenses by selecting "Clinicians/Staff" rather than a nonprofit organization when applying. This enables them to guide their patients through the process of creating personalized synthetic voices. There are no discount codes or credit card details required. Approved applicants receive a 5-year, extendable free license. How individuals with permanent speech loss can apply for a free voice How clinicians supporting individuals with permanent speech loss can apply for a free voice Who is eligible We partner with nonprofit organizations who help us distribute free access to ElevenLabs for individuals affected by permanent voice loss or visual impairment. Our current application partners support individuals across a range of diagnoses and regions, including: ALS/MND(USA –Bridging Voice; UK –MND Association; Australia –MND and Me,MND NSW - FlexEquip; Global –Scott-Morgan Foundation,UCL) PSP, MSA, CBD(USA/Canada –CurePSP; UK/Ireland –MSA Trust; Global –Mission MSA) Stroke(USA –Stroke Onward; New Zealand –TalkLink Trust) Tay-Sachs & SandhoffDisease (England, Wales, Northern Ireland –CATS Foundation) Head & Neck Cancer, Laryngectomy, Glossectomy(Global –Lary’s Speakeasy,TalkLink Trust) Permanent speech impairment and specific AAC users(Global –Smartbox,Jabbla,Therapy Box,Cboard,REHAVISTA,UCL) Blind and Low-Vision(USA –National Federation of the Blind) Building toward one million voices By enabling users to apply directly through the ElevenLabs website, we’re removing friction and making accessibility faster and simpler. Each improvement brings us closer to our goal of giving one million people their voices back. Note: This new workflow isonly for individuals with permanent speech loss and their clinicians, who can use it to support patients through onboarding andvoice cloning. Organizations seeking to help distribute free access to patients, especially to represent a new region or diagnosis not currently supported, should apply through our standardImpact Program Application. If your nonprofit organization would like to complete a project using ElevenLabs technology, please also applythere.]]></description>
      <pubDate>Tue, 25 Nov 2025 21:06:04 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/expanding-access-patients-and-clinicians-can-now-apply-directly-on-the-elevenlabs-website</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs Image &amp; Video</title>
      <link>https://elevenlabs.io/blog/introducing-elevenlabs-image-and-video</link>
      <description><![CDATA[Today we’re introducing ElevenLabs Image & Video (Beta). The best audio, image, and video models all in one platform. Within ElevenLabs, you can now bring ideas to life in one complete creative workflow. Use leading models like Veo, Sora, Kling, Wan and Seedance to create high-quality visuals, then bring them to life with the best voices,music, andsound effectsfrom ElevenLabs. Creators, marketers, and content teams can generate images, compose clips, adjust narration, and export final content all inside a single, unified workflow. Create with leading image and video models ElevenLabs Image & Video (Beta) brings together the best models for visual creation. You can: Createstill images using leading models including Nanobanana, Flux Kontext, GPT Image, and Seedream. Usethese as storyboards, thumbnails, or as source material for video projects. Generatevideos with models including Veo, Sora, Kling, Wan, and Seedance. Refineoutputs and compose multiple clips for seamless storytelling. Upscaleyour images and videos for higher-quality results. Add lipsyncto your generated videos using ElevenLabs voices for perfectly aligned narration. Refine and edit in Studio Once your visuals are ready, export to Studio to complete your project. Studio lets you: Add expressivevoiceoversusing voices from our library or your own clones. Compose bespoke background music and layer in sound effects. Adjust timing and refine narration on a single timeline. Export polished, production-ready videos. Built for creators, marketers, and content teams Image & Video is designed for creators of every kind, from filmmakers and freelancers to marketers and educators. Whether you’re creating product videos, social content, or educational materials, ElevenLabs provides the full toolset to go from idea to final export in one platform. This launch marks a major step toward true multimodal creation, where every element — from visuals to sound — can be generated, edited, and refined together. Available in ElevenLabs Creative Platform Start creating with ElevenLabsImage & Video(Beta).]]></description>
      <pubDate>Tue, 25 Nov 2025 21:05:16 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-elevenlabs-image-and-video</guid>
    </item>
    <item>
      <title>Giving voice back to stroke survivors</title>
      <link>https://elevenlabs.io/blog/giving-voice-back-to-stroke-survivors</link>
      <description><![CDATA[Each year,more than 12 million people worldwide experience a stroke. For many, recovery involves relearning how to walk, think, and communicate. Some regain their speech fully. Others live with long-term motor-speech challenges that make verbal communication difficult or impossible. Today, on World Stroke Day, we’re proud to announce our partnership withStroke Onward. Founded by stroke survivors, Stroke Onward helps people navigate the emotional and identity challenges of rebuilding life after a stroke. Together, we aim to make voice restoration technology available to everyone who needs it, and to strengthen the community of those rebuilding life after stroke. Bringing voice restoration to the stroke survivor community Through this partnership, individuals affected by permanent speech loss can now apply for the ElevenLabs Impact Program. Approved applicants receive free access to our advancedvoice cloningandText to Speechtools, allowing them to create and use a digital voice that represents them authentically. For those living with dysarthria or other motor-speech impairments, this technology can help restore a vital part of their identity. It enables them to communicate naturally with loved ones, participate in conversations, and express themselves in their own voice. Stroke Onward Community Circle Stroke Onward has also launched theStroke Onward Community Circle (SOCC)—a free, online community where survivors, carepartners, and professionals connect around the emotional and identity sides of recovery. In a world that often focuses only on physical rehabilitation, SOCC creates space for everything else—the emotional, invisible, and deeply personal parts of rebuilding life after stroke. Members can join live events, share experiences, and access curated tools that support emotional recovery. By introducing ElevenLabs voice technology into this community, we hope to empower members to rediscover and reclaim their voices as a part of that recovery process. Our shared goal We believe everyone should have the ability to express themselves in their own voice. Partnering with Stroke Onward helps us reach people living with speech loss, ensuring that technology serves as an enabler, not a barrier, to human connection. On World Stroke Day, we’re reminded that recovery from stroke is about more than physical healing. It’s about rebuilding identity, connection, and purpose. Together with Stroke Onward, we aim to make that journey more accessible, helping survivors regain their voices and their sense of self. Learn more about the partnership and how to apply.]]></description>
      <pubDate>Tue, 25 Nov 2025 20:47:09 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/giving-voice-back-to-stroke-survivors</guid>
    </item>
    <item>
      <title>We’re partnering with Liberty Global to accelerate voice AI expansion across Europe</title>
      <link>https://elevenlabs.io/blog/liberty-global-partnership</link>
      <description><![CDATA[We’ve started a commercial partnership with Liberty Global Ventures, the technology investment arm of Liberty Global. As part of this, Liberty Global will also be making a strategic investment in ElevenLabs. Our work together will focus on howvoice AIcan make everyday interactions with technology feel more natural - especially across telecommunications and entertainment products. Liberty Global is looking at several applications of our technology - from AI-powered customer service agents that offer faster, more human support, to voice interfaces for connected TV and streaming products that make content discovery simple and intuitive. We’re also looking at new ways voice can shape customer communication and marketing. The investment was led by Rebecca Hunt, Partner at Liberty Global Ventures, based in London, and supports our continued work to scale our foundational voice model globally and bring lifelike, multilingual voice AI to more people. Our Co-Founder Mati says: “Entering a strategic partnership with Liberty Global is a welcome endorsement from one of the world’s biggest providers of connectivity services and marks a further step forward in bringing emotionally rich, lifelike voice AI to millions of households.” Rebecca Hunt, Partner, Liberty Global Ventures, adds: “Voice is becoming the next major interface for technology, and ElevenLabs is defining what’s possible in this space. The investment continues our track record of providing early backing for transformational tech infrastructure companies; we are honoured to be backing this category-defining team.” We founded ElevenLabs in 2022 to build voice AI that lets people and technology speak naturally - in every language. This partnership brings that vision closer.]]></description>
      <pubDate>Fri, 21 Nov 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/liberty-global-partnership</guid>
    </item>
    <item>
      <title>Harvey and ElevenLabs Partner to Give Lawyers a Global Voice</title>
      <link>https://elevenlabs.io/blog/harvey</link>
      <description><![CDATA[We've partnered with Harvey, the AI legal platform used by leading law firms and enterprises, to create the first global, multilingual voice for the legal profession. Our collaboration brings spoken intelligence to law, enabling Harvey to communicate naturally in dozens of languages, dialects, or accents.Powered by ElevenLabs’Text to SpeechandSpeech to Text, Harvey will make legal knowledge more accessible and human across jurisdictions and cultures. “This partnership makes legal AI more global, accessible, and human,” said Winston Weinberg, CEO at Harvey. “With ElevenLabs, we’re ensuring every lawyer can engage with Harvey in their own language and context." “Our mission has always been to break down language barriers,” said Mati Staniszewski, CEO of ElevenLabs. “By bringing Harvey’s legal intelligence to voice, in dozens of dialects and accents, we are helping transform how law is experienced globally.” The first phase will allow Harvey to deliver answers audibly in almost any language or dialect. Future developments will introduce new features like multi-lingual voice translation, voice mode, spoken trial simulations, tone customization, and more. Together, we're are redefining how legal professionals interact with AI, with knowledge that isn’t just written, butspokenwith clarity and precision.]]></description>
      <pubDate>Wed, 12 Nov 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/harvey</guid>
    </item>
    <item>
      <title>Honoring veterans and their voices: Lt Col Thomas Brittingham’s story</title>
      <link>https://elevenlabs.io/blog/honoring-veterans-and-their-voices-lt-col-thomas-brittinghams-story</link>
      <description><![CDATA[When Lt Col Thomas Brittingham first heard his own voice again, it was Mother’s Day and his wedding anniversary. His wife Jessi was sitting beside him when he typed out a short message using his new ElevenLabs voice: “Hey Jessi, does this sound like me? Happy Mother’s Day and Happy Anniversary. I love you.” The moment stopped her. “It brought tears to my eyes,” she recalled. “It was the most amazing gift I could have ever received—hearing his voice again.” Lt Col Thomas Brittingham A lifetime of service Lt Col Brittingham has dedicated his life to service and excellence. A 2006 graduate of the Coast Guard Academy and a 2004 Air Force Academy Exchange Cadet, he began his career aboard the Coast Guard Cutter Sequoia in Guam, where he conducted the first bilateral boardings of foreign ships in the Western Pacific. From there, his path led to commanding his own vessel, the Coast Guard Cutter Haddock, and later serving as Military Aide to the Coast Guard’s Chief Acquisition Officer, overseeing $30 billion in modernization programs. In 2011, he was selected for Air Force pilot training—a transition that would mark the next chapter in a distinguished career. Over the following decade, Lt Col Brittingham served as flight lead and mission commander across the Mediterranean, Atlantic, and Pacific. He deployed four times in support of Operation Inherent Resolve, logging nearly 1,000 combat hours. Confronting ALS In 2023, Thomas received a diagnosis that would change his life—amyotrophic lateral sclerosis (ALS). The disease began in his legs and moved upward, affecting his arms, diaphragm, and eventually his ability to speak. “His voice was always strong,” Jessi said. “Even as ALS progressed, it was something that made him feel like himself.” But as muscle weakness advanced, Thomas’s voice grew quieter until it disappeared entirely in April 2024. Without a natural voice, Thomas initially relied on a generic computer-generated one that was robotic and difficult to understand. “We depended mostly on reading his screen,” Jessi said. “It didn’t sound like him. It didn’t sound human.” Finding his voice again Through Team Gleason, a nonprofit that supports people living with ALS, Thomas connected withBridging Voice, an organization that helps individuals preserve and recreate their voices. There, he met Trinity, who guided him through the process of restoring his natural voice with ElevenLabs. Bridging Voice guided his family through the process—collecting past videos, preparing samples, and building a model of how Thomas sounded before ALS. “It was healing to go back through those clips,” Jessi said. “We watched them with our two boys, who loved hearing their dad’s voice again.” Once the recordings were submitted, the ElevenLabs team created a Professional Voice Clone for Thomas, a precise recreation of how he sounded before the disease. The moment it all came back When Thomas used his new AI voice for the first time, he chose to surprise Jessi. The words he typed carried all the warmth and cadence of his real speech. “I made him say it over and over again,” Jessi laughed. “Our family couldn’t believe how real it sounded. The boys thought it was hilarious hearing their dad’s voice saying silly things.” It wasn’t just a technological milestone, it was a return of identity, presence, and connection. “With two young children, it means everything for them to hear their dad’s voice,” Jessi said. “It keeps him present in their lives in a way that text alone can’t.” A message for Veterans Day For Lt Col Brittingham, Veterans Day carries deep meaning. It’s a reminder not only of service and sacrifice, but of the strength that comes from community and innovation. He hopes his story shows what’s possible when technology serves humanity, especially for veterans facing the challenges of illness or injury. Recent studies have shown that Air Force pilots are ten times more likely to be diagnosed with ALS than civilians. For Thomas, that statistic is personal. His experience underscores the urgency of advancing accessible technology that restores independence and dignity to those who have given so much in service. Continuing the mission TheElevenLabs Impact Programexists to help individuals like Lt Col Brittingham regain their voice and agency through AI. By combining advanced voice synthesis with human-centered design, the program ensures that every person can preserve the sound of who they are. Thomas’s journey, from commanding aircraft across the world to communicating again in his own voice, shows what this technology makes possible. When so much is taken away, being able to speak again, even through an artificial voice, gives something profoundly human back. This Veterans Day, we honor Lt Col Brittingham and all who have served, and recommit to building technology that gives them their voices back. It’s not just about speech. It’s about memory, identity, and the ability to stay connected to the people who matter most.]]></description>
      <pubDate>Tue, 11 Nov 2025 16:25:55 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/honoring-veterans-and-their-voices-lt-col-thomas-brittinghams-story</guid>
    </item>
    <item>
      <title>Building the First Agentic Government with Ukraine</title>
      <link>https://elevenlabs.io/blog/building-the-first-agentic-government-with-ukraine</link>
      <description><![CDATA[We travelled to Kyiv to deepen our partnership with the Government of Ukraine and signed a memorandum of understanding with Minister Mykhailo Fedorov to take AI public services from concept to production. The visit showed how Ukraine continues to build with purpose, clarity, and an engineering-first mindset focused on outcomes. Guided by the Ministry of Digital Transformation, we met with several ministries - Health, Education, Economy, and Foreign Affairs - each exploring how AI can make public services more efficient, accessible, and human. Ukraine is leading on that front, creating the world’s first agentic government - where AI agents work on citizens’ behalf across ministries, connecting national systems with individual needs. At the Ministry of Education, teams are developing Mriia, an app for personalised AI tutors that adapt to each learner’s knowledge and pace, and exploring agents to make educational content more accessible throughdubbingand translation. The Ministry of Economy is integrating agents into Obriy, a platform supporting businesses and gathering public feedback. In healthcare, speech technology is being used to reduce administrative work and make care more accessible. All these efforts are part of Ukraine’s plan, led by the Ministry of Digital Transformation, to become a leader in applying AI to public services by 2030. The ministry has already taken a first step with Diia.AI - the world’s first public-service agent app and platform - and we’re proud to support the ambition of making public services available through voice. Ukraine’s governance model - putting engineering at the heart of decision-making - is both natural and effective. Each ministry has its own technical team, building fast and iterating in the open. It’s a structure we relate to at ElevenLabs, where we embed engineers directly across functions to move ideas from prototype to production. The memorandum we signed marks the beginning of a shared effort to bring AI from concept to production across government. Our Forward Deployed Engineers remain in Kyiv, working alongside Ukrainian teams to turn these ideas into working systems. The main bottleneck in AI adoption has never been discovery - it’s deployment - and Ukraine is leading the way in making AI work for its people. We’re grateful to our partners - Mykhailo Fedorov, Oleksandr Bornyakov, Valeriya Ionan, Danylo Tsvok, Dmytro Ovcharenko, and Nelli Blinova - for their hospitality, openness, and collaboration. Ukraine’s pace of work is a reminder that innovation is not a luxury of peace but the quiet work of moving forward.]]></description>
      <pubDate>Thu, 06 Nov 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/building-the-first-agentic-government-with-ukraine</guid>
    </item>
  </channel>
</rss>
