<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Mon, 22 Dec 2025 10:05:18 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>NVIDIA, US Government to Boost AI Infrastructure and R&amp;D Investments Through Landmark Genesis Mission</title>
      <link>https://blogs.nvidia.com/blog/nvidia-us-government-to-boost-ai-infrastructure-and-rd-investments/</link>
      <description><![CDATA[NVIDIA will join the U.S. Department of Energyâ€™s (DOE) Genesis Mission as a private industry partner to keep U.S. AI both the leader and the standard in technology around the world. The Genesis Mission, which is part of an Executive Order recently signed by President Trump, aims to redefine American leadership in AI across three key areas: energy, scientific research and national security. NVIDIA will offer its services to the Department to integrate a discovery platform that unites the U.S. government, industry and academia. DOE officials expect the Genesis Mission to double the productivity and impact of American science and engineering, and deliver decisive breakthroughs to secure American energy dominance, accelerate scientific discovery and strengthen national security. NVIDIA and the DOE are already achieving groundbreaking results in key areas, including: Open AI science models, such as with NVIDIA Apollo family, to advance weather forecasting, computational fluid dynamics and structural mechanics; AI for optimizing manufacturing and supply chain processes; Robotics, edge AI and autonomous labs, including through the use of high-fidelity simulation and AI-enabled digital twins; Nuclear energy, including fission and fusion research; Quantum computing research, using supercomputers and AI to accelerate the discovery of new algorithms; Biology, materials science and synthetic design for healthcare; and critical-materials breakthroughs. NVIDIA, DOE Announce MOU In addition, NVIDIA announced that it has signed a memorandum of understanding (MOU) with the Department to outline priorities of the collaboration in support of accelerating scientific discovery. The MOU includes, but is not limited to, AI for manufacturing and supply chain, open-source AI, fission energy, robotics, AI-enabled digital twins, fusion energy, quantum computing and science. Together, these priorities focus on using advanced AI, robotics and highâ€‘performance computing to transform energy, manufacturing and scientific discovery across the Department of Energy mission space. They emphasize AI-enabled design, operation and control of complex systems, such as nuclear fission and fusion reactors, experimental facilities, digital twins of infrastructure and autonomous laboratories, including at the edge for real-time decision-making. Additional opportunities for collaboration may include accelerating breakthroughs in quantum computing, materials science, biology and synthetic design, subsurface and geothermal resources, and environmental cleanup, as well as open science-optimized AI models and AI â€œco-scientistsâ€ that speed algorithm development and code generation for demanding scientific applications.â€‹ Building on Supercomputing Collaborations NVIDIAâ€™s support of the Genesis Mission comes on the heels of a series of collaborations between NVIDIA and the DOE made during the NVIDIA GTC Washington, D.C., conference, including NVIDIA and Oracle working together to build the Departmentâ€™s largest supercomputer for scientific research at Argonne National Laboratory. This is in addition to news that NVIDIA will support seven new systems across Argonne and Los Alamos National Laboratories, accelerating the DOEâ€™s mission of driving technological leadership. NVIDIA pioneered the accelerated computing architecture that makes modern AI possible â€” a platform that enables researchers to train large models, simulate physical systems and advance science at unprecedented scale and speed. AI is driving a new industrial revolution in the U.S. and across the world. The Genesis Mission is expected to expand and accelerate that revolution. Categories: Corporate Tags: Artificial Intelligence | Energy | Healthcare and Life Sciences | High-Performance Computing | Quantum Computing | Science]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 18 Dec 2025 19:02:12 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-us-government-to-boost-ai-infrastructure-and-rd-investments/</guid>
    </item>
    <item>
      <title>Now Generally Available, NVIDIA RTX PRO 5000 72GB Blackwell GPU Expands Memory Options for Desktop Agentic AI</title>
      <link>https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</link>
      <description><![CDATA[Top-notch options for AI at the desktops of developers, engineers and designers are expanding. The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available, bringing robust agentic and generative AI capabilities powered by the NVIDIA Blackwell architecture to more desktops and professionals across the world. The new GPU configuration offers AI developers, data scientists and creative professionals the hardware for modern, memory-hungry workflows â€” and arrives at a time when demand for NVIDIA Blackwell-class compute is higher than ever. With the flexibility to choose between this 72GB variant and the existing NVIDIA RTX PRO 5000 48GB model, AI developers can right-size their systems for a wider range of budgets and project requirements. The NVIDIA RTX PRO 5000 Blackwell GPU. Fueling the Next Generation of AI Development As generative AI evolves into complex, multimodal agentic AI , more demand is placed on the hardware required to develop and deploy these technologies. One defining challenge of AI development is memory capacity. Running cutting-edge AI workflows â€” especially those involving large language models ( LLMs ) and AI agents â€” places significant stress on GPU memory, particularly as models, context windows and multimodal pipelines grow in size and complexity. Agentic AI systems involve chains of tools, retrieval-augmented generation ( RAG ), and multimodal understanding. These systems often need to keep multiple AI models, data sources and code formats active simultaneously within the GPUâ€™s memory. Built on NVIDIA Blackwell â€” which delivers high throughput for AI, neural rendering and simulation with multi-workload scheduling and other architectural innovations â€” the RTX PRO 5000 72GB helps solve this bottleneck, offering 2,142 TOPS of AI performance. Plus, with 72GB of ultrafast GDDR7 memory â€” a 50% increase over the 48GB model â€” developers can train, fine-tune and prototype larger models locally. This enables users to maintain data privacy, low latency and cost efficiency, allowing teams to serve models directly from their workstations rather than relying on data-center-scale infrastructure for every AI task. Performance Enhancements For local AI development, raw compute is only half the battle â€” memory capacity determines what users can run, and throughput determines how fast it runs. In industry-standard benchmarks for generative AI, the RTX PRO 5000 72GB offers 3.5x the performance of prior-generation NVIDIA hardware for image generation, and 2x the performance of prior-generation hardware for text generation. In creative workflows, time saved in rendering is time gained for iteration. Across path-tracing engines like Arnold, Chaos V-Ray and Blender, as well as real-time GPU renderers like D5 Render and Redshift, the RTX PRO 5000 72GB slashes render times by up to 4.7x. And for computer-aided engineering and product design, the RTX PRO 5000 72GB offers more than 2x graphics performance. InfinitForm Optimizes Generative AI-Powered Design With RTX PRO 5000 InfinitForm, a provider of generative AI software for engineering design and a member of the NVIDIA Inception program for startups, is an early adopter of the RTX PRO 5000 72GB Blackwell GPU. The company is using the GPU to optimize its software with enhanced performance and speed, enabling advanced simulations to streamline processes for computer-aided design and manufacturing. â€œInfinitForm is thrilled to evaluate its CUDA-accelerated generative AI design optimization software on NVIDIA RTX PRO 5000 72GB to help customers like Yamaha Motor and NASA accelerate innovation and optimize products for performance and manufacturability,â€ said Michael Bogomolny, founder and CEO of InfinitForm. Versatile Media Accelerates Virtual Production Workflows With RTX PRO 5000 For creative professionals, such as those at Versatile Media â€” a global media company specializing in virtual production â€” the RTX PRO 5000 can significantly enhance real-time rendering performance for large-scale scenes and complex assets, delivering a meaningful leap in efficiency. As 3D pipelines now often integrate AI denoisers, generative tools and real-time physics, the new GPUâ€™s 72GB memory capacity allows for the manipulation of massive 3D scenes and asset libraries without slowing down the creative flow. An early adopter of the new GPU, Versatile Media plans to use it to design a series of complex, high-resolution real-time rendering scenarios that take advantage of the GPUâ€™s memory capacity. â€œFor film-grade virtual production, memory capacity directly translates into creative freedom,â€ said Eddy Shen, general manager of the production center at Versatile Media. â€œWith 72GB of GPU memory, the RTX PRO 5000 enables us to iterate with higher-resolution scenes and more complex lighting in real time without compromising performance.â€ Availability and Ecosystem Support The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available from partners including Ingram Micro, Leadtek, Unisplendour and xFusion, providing manufacturers and systems integrators with a powerful new option to anchor AI-ready workstations. Broader availability through global system builders will start early next year. As industries race to integrate AI into every facet of operation â€” from generative design to coding copilots â€” RTX PRO 5000 72GB is equipped to meet the moment. Learn more about the NVIDIA RTX PRO 5000 GPU family and join NVIDIA at SIGGRAPH Asia to discover how graphics and simulation innovations come together to drive creativity, industrial digitalization, robotics, and physical and spatial AI. Categories: Data Center | Generative AI | Pro Graphics Tags: Agentic AI | Artificial Intelligence | GPU | Hardware | NVIDIA RTX | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 18 Dec 2025 16:00:28 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</guid>
    </item>
    <item>
      <title>Deck the Vaults: â€˜Fallout: New Vegasâ€™ Joins the Cloud This Holiday Season</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-new-vegas/</link>
      <description><![CDATA[Step out of the vault and into the future of gaming with Fallout: New Vegas streaming on GeForce NOW , just in time to celebrate the newest season of the hit Amazon TV show Fallout . To mark the occasion, GeForce NOW members can claim Fallout 3 and Fallout 4 as special rewards, completing a wasteland-ready trilogy in the cloud. Whether scavenging the ruins of post-apocalyptic America or rebuilding hope in the Wasteland, gamers can experience every choice â€” and every radroach â€” even sharper in the cloud. Take a detour from the desert dust to the enchanted halls of Hogwarts in Hogwarts Legacy â€” and relive Harryâ€™s years at school, brick by whimsical brick, in LEGO Harry Potter: Years 1-7. This weekâ€™s GFN Thursday lineup of five games proves adventure comes in every form, from radiation to incantations. Plus, GeForce NOW is joining Ubisoft to celebrate 10 years of Rainbow Six Siege : a decade of clutch plays and tactical showdowns streaming strong in the cloud. Have Yourself a Merry Little â€˜Falloutâ€™ Itâ€™s a S.P.E.C.I.A.L. GFN Thursday. Bethesdaâ€™s Fallout: New Vegas is rolling onto GeForce NOW with a suitcase full of wasteland wit. Set in the Mojave Desert after a bombing, the title follows a courier carrying a valuable chip through a desert full of schemers, gamblers and nuclear scars. The adventure drops players into a three- or more-way tug-of-war for the fate of New Vegas, where every decision can shift who runs the Strip and who ends up buried in the sand. Choices matter at every turn: talk fast, sneak smart or go loud, all while navigating sharp, darkly funny writing and factions that rarely fit cleanly into good or evil.â€‹ In the cloud, the Mojave feels right at home, with high-fidelity visuals, fast response times and no need for a monster rig or huge downloads. The wasteland spins up in just seconds on underpowered laptops, phones and more, making it easier than ever to drop in for a quick caravan run or a long night on the Strip. To sweeten the deal, GeForce NOW Ultimate members can claim a stack of classics â€” including Fallout 3 and Fallout 4 â€” while supplies last. Itâ€™s a tour of the Fallout universe, ready to stream from the cloud. Members can look for an email with details on how to redeem. Magic Without the Wait GeForce NOW is making spirits bright this season with two enchanting adventures in the cloud. Step into the wizarding world with Hogwarts Legacy and relive the full saga of the â€œBoy Who Livedâ€ in brick form with the LEGO Harry Potter Collection . Whether exploring the Forbidden Forest or causing mischief in Hogwarts, gamers can experience plenty of magic to go around. Itâ€™s not sorcery, itâ€™s the cloud. In Hogwarts Legacy, players can chart their own path as a fifth-year student at Hogwarts in the 1800s. From learning powerful spells to taming magical beasts, the castle and its grounds are packed with secrets waiting to be discovered. Itâ€™s a world of intrigue, danger and wonder â€” with rich storytelling and breathtaking visuals that shine perfectly in the cloud. GeForce NOW Ultimate members can experience the magic of Hogwarts like never before â€” every spell, shadow and shimmering corridor brought to life with GeForce RTX 5080-class power and NVIDIA DLSS 4 technology for spellbinding performance and ultrasmooth gameplay. No floo powder needed â€” just jump right in with GeForce NOW. The LEGO Harry Potter Collection offers a lighter, more playful spin on the saga, combining all seven years at Hogwarts into one charming package. Revisit major moments, cast silly spells and collect enough studs to make Gringotts jealous. Itâ€™s the ultimate magical mashup â€” family-friendly, full of humor and packed with puzzles and co-op fun. No need for spells or waiting around for sorting â€” the magic starts right away with GeForce NOW. Members can stream both titles in jaw-dropping detail from nearly any device. Seize the Cloud No floo powder needed â€” just jump right in with GeForce NOW. Rainbow Six Siege is turning 10, and the tactical chaos is streaming strong on GeForce NOW. For a full decade, squads have been breaching, droning and clutching rounds in one of the most precise, team-focused shooters around, instantly accessible from the cloud on a range of devices. Rainbow Six Siege is rolling out free in-game rewards all month to mark a decade of creativity, competition and tactical mastery. Players jumping into the game can look forward to a steady stream of goodies â€” from themed cosmetics to special packs and daily surprises â€” to commemorate the anniversary, on top of seasonal updates and the ever-evolving Operator roster. Celebrate and stream the iconic game on GeForce NOW, with anniversary matches just a few clicks away â€” from quick warmups on a laptop to long sessions on the living room TV. Squads old and new can mark 10 unforgettable years of Rainbow Six Siege together in the cloud. In addition, members can look for the following: Pioner (New release on Steam , Dec. 16) Fallout: New Vegas ( Steam , Epic Games Store and Xbox , available on PC Game Pass) For the King II ( Steam ) Hogwarts Legacy ( Steam , Epic Games Store , and Xbox , available on PC Game Pass, GeForce RTX 5080-ready) LEGO Harry Potter Collection ( Steam ) GeForce RTX 5080-ready games, in addition to Hogwarts Legacy : Avatar: Frontiers of Pandora (Steam and Ubisoft Connect) Starting in January 2026, some games currently available to all members will no longer be available on the free tier as the basic rig type does not meet these gamesâ€™ â€” Enshrouded, Alan Wake 2 and Cities: Skylines II â€” minimum system requirements. Premium members can continue to play these games uninterrupted. What are you planning to play this weekend? Let us know on X or in the comments below. â˜ƒï¸ Treat yo'elf to our Half Price Holiday offer â€” for a limited time, premium memberships are half off for the first month. â„ï¸ Go here âž¡ï¸ https://t.co/eLwaS9Go5Z #GeForceSeason pic.twitter.com/QLdVpROB7T â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) December 17, 2025 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 18 Dec 2025 14:00:12 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-fallout-new-vegas/</guid>
    </item>
    <item>
      <title>Into the Omniverse: OpenUSD and NVIDIA Halos Accelerate Safety for Robotaxis, Physical AI Systems</title>
      <link>https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of Into the Omniverse , a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in OpenUSD and NVIDIA Omniverse . Physical AI is moving from research labs into the real world, powering intelligent robots and autonomous vehicles (AVs) â€” such as robotaxis â€” that must reliably sense, reason and act amid unpredictable conditions. To safely scale these systems, developers need workflows that connect real-world data, high-fidelity simulation and robust AI models atop the common foundation provided by the OpenUSD framework. The recently published OpenUSD Core Specification 1.0 , OpenUSD â€” aka Universal Scene Description â€” now defines standard data types, file formats and composition behaviors, giving developers predictable, interoperable USD pipelines as they scale autonomous systems. Powered by OpenUSD, NVIDIA Omniverse libraries combine NVIDIA RTX rendering, physics simulation and efficient runtimes to create digital twins and simulation-ready ( SimReady ) assets that accurately reflect real-world environments for synthetic data generation and testing. NVIDIA Cosmos world foundation models can run on top of these simulations to amplify data variation, generating new weather, lighting and terrain conditions from the same scenes so teams can safely cover rare and challenging edge cases. Learn more by watching the OpenUSD livestream today at 11 a.m. PT or in replay, part of the NVIDIA Omniverse OpenUSD Insiders series: In addition, advancements in synthetic data generation , multimodal datasets and SimReady workflows are now converging with the NVIDIA Halos framework for AV safety, creating a standards-based path to safer, faster, more cost-effective deployment of next-generation autonomous machines. Building the Foundation for Safe Physical AI Open Standards and SimReady Assets The OpenUSD Core Specification 1.0 establishes the standard data models and behaviors that underpin SimReady assets, enabling developers to build interoperable simulation pipelines for AI factories and robotics on OpenUSD . Built on this foundation, SimReady 3D assets can be reused across tools and teams and loaded directly into NVIDIA Isaac Sim , where USDPhysics colliders, rigid body dynamics and composition-arcâ€“based variants let teams test robots in virtual facilities that closely mirror real operations. Open-Source Learning The Learn OpenUSD curriculum is now open source and available on GitHub, enabling contributors to localize and adapt templates, exercises and content for different audiences, languages and use cases. This gives educators a ready-made foundation to onboard new teams into OpenUSD-centric simulation workflows.â€‹ Generative Worlds as Safety Multiplier Gaussian splatting â€” a technique that uses editable 3D elements to render environments quickly and with high fidelity â€” and world models are accelerating simulation pipelines for safe robotics testing and validation. At SIGGRAPH Asia, the NVIDIA Research team introduced Play4D , a streaming pipeline that enables 4D Gaussian splatting to accurately render dynamic scenes and improve realism. Spatial intelligence company World Labs is using its Marble generative world model with NVIDIA Isaac Sim and Omniverse NuRec so researchers can turn text prompts and sample images into photorealistic, Gaussian-based physics-ready 3D environments in hours instead of weeks. Those worlds can then be used for physical AI training, testing and sim-to-real transfer. This high-fidelity simulation workflow expands the range of scenarios robots can practice in while keeping experimentation safely in simulation. Lightwheel Helps Teams Scale Robot Training With SimReady Assets Powered by OpenUSD, Lightwheel â€™s SimReady asset library includes a common scene description layer, making it easy to assemble high-fidelity digital twins for robots. The SimReady assets are embedded with precise geometry, materials and validated physical properties, which can be loaded directly into NVIDIA Isaac Sim and Isaac Lab for robot training. This allows robots to experience realistic contacts, dynamics and sensor feedback as they learn. End-to-End Autonomous Vehicle Safety End-to-end autonomous vehicle safety advancements are accelerating with new research, open frameworks and inspection services that make validation more rigorous and scalable. NVIDIA researchers, with collaborators at Harvard University and Stanford University, recently introduced the Sim2Val framework to statistically combine real-world and simulated test results, reducing AV developersâ€™ need for costly physical mileage while demonstrating how robotaxis and AVs can behave safely across rare and safety-critical scenarios. Learn more by watching NVIDIAâ€™s â€œSafety in the Loopâ€ livestream: These innovations are complemented by a new, open-source NVIDIA Omniverse NuRec Fixer, a Cosmos-based model trained on AV data that removes artifacts in neural reconstructions to produce higher-quality SimReady assets. To align these advances with rigorous global standards, the NVIDIA Halos AI Systems Inspection Lab â€” accredited by ANAB â€” provides impartial inspection and certification of Halos elements across robotaxi fleets, AV stacks, sensors and manufacturer platforms through the Halos Certification Program . AV Ecosystem Leaders Putting Physical AI Safety to Work Bosch , Nuro and Wayve are among the first participants in the NVIDIA Halos AI Systems Inspection Lab, which aims to accelerate the safe, large-scale deployment of robotaxi fleets. Onsemi, which makes sensor systems for AVs, industrial automation and medical applications, has recently become the first company to pass inspection for the NVIDIA Halos AI Systems Inspection Lab. The open-source CARLA simulator integrates NVIDIA NuRec and Cosmos Transfer to generate reconstructed drives and diverse scenario variations, while Voxel51 â€™s FiftyOne engine, linked to Cosmos Dataset Search, NuRec and Cosmos Transfer, helps teams curate, annotate and evaluate multimodal datasets across the AV pipeline.â€‹ Mcity at the University of Michigan is enhancing the digital twin of its 32-acre AV test facility using Omniverse libraries and technologies. The team is integrating the NVIDIA Blueprint for AV simulation and Omniverse Sensor RTX application programming interfaces to create physics-based models of camera, lidar, radar and ultrasonic sensors. By aligning real sensor recordings with high-fidelity simulated data and sharing assets openly, Mcity enables safe, repeatable testing of rare and hazardous driving scenarios before vehicles operate on public roads. Get Plugged Into the World of OpenUSD and Physical AI Safety Learn more about OpenUSD, NVIDIA Halos and physical AI safety by exploring these resources: Watch the on-demand NVIDIA GTC session, â€œ Reconstructing Reality: Simulating Indoor and Outdoor Environments for Physical AI .â€ Visit the NVIDIA Halos AI Systems Inspection Lab webpage. Follow the NVIDIA DRIVE LinkedIn newsletter: â€œ NVIDIA Safety in the Loop .â€ Read the corporate blog explainer: How AI Is Unlocking Level 4 Autonomy . Get started with the Learn OpenUSD curriculum , now open source. Stay up to date by subscribing to NVIDIA news , joining the community and following NVIDIA Omniverse on Instagram , LinkedIn , Medium and X . Categories: Driving | Pro Graphics | Robotics Tags: Cosmos | Into the Omniverse | NVIDIA Blueprints | NVIDIA Isaac Sim | NVIDIA Omniverse | Physical AI | Synthetic Data Generation]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 17 Dec 2025 17:00:49 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/openusd-halos-safety-robotaxi-physical-ai/</guid>
    </item>
    <item>
      <title>UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System</title>
      <link>https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</link>
      <description><![CDATA[The Hao AI Lab research team at the University of California San Diego â€” at the forefront of pioneering AI model innovation â€” recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference . Many LLM inference platforms in production today, such as NVIDIA Dynamo , use research concepts that originated in the Hao AI Lab, including DistServe . How Is Hao AI Lab Using the DGX B200? Members of the Hao AI Lab standing with the NVIDIA DGX B200 system. With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciencesâ€™ San Diego Supercomputer Center , the research opportunities are boundless. â€œDGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,â€ said Hao Zhang, assistant professor in the HalÄ±cÄ±oÄŸlu Data Science Institute and department of computer science and engineering at UC San Diego. â€œIt enables us to prototype and experiment much faster than using previous-generation hardware.â€ Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark. FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt â€” in just five seconds. The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system. Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including Tetris and Super Mario Bros . Users can test one model at a time or put two models up against each other to measure their performance. The illustrated workflow of Hao AI Labâ€™s Lmgame-Bench project. Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness. â€œOur current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,â€ said Junda Chen, a doctoral candidate in computer science at UC San Diego. How DistServe Influenced Disaggregated Serving Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests. The benefit of disaggregated inference lies in optimizing what DistServe calls â€œgoodputâ€ instead of â€œthroughputâ€ in the LLM-serving engine. Hereâ€™s the difference: Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another. While throughput measures the aggregate performance of the system, it doesnâ€™t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput. This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, â€œgoodputâ€: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience. DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model. How Can Developers Achieve Optimal Goodput? When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each tokenâ€™s future behavior based on past requestsâ€™ outcomes. This process is known as decode. https://blogs.nvidia.com/wp-content/uploads/2025/12/distserve.mp4 Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput. â€œPreviously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,â€ Chen said. â€œNow, if I split the jobs onto two different sets of GPUs â€” one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive â€” we can fundamentally eliminate the interference between the two jobs, making both jobs run faster. This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput. Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses. NVIDIA Dynamo â€” an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost â€” enables scaling disaggregated inference. In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation. Learn more about the NVIDIA DGX B200 system. Categories: Data Center | Generative AI | Research | Supercomputing Tags: Artificial Intelligence | Education | Inference | NVIDIA DGX | Open Source]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 17 Dec 2025 16:00:15 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</guid>
    </item>
    <item>
      <title>NVIDIA Acquires Open-Source Workload Management Provider SchedMD</title>
      <link>https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</link>
      <description><![CDATA[NVIDIA today announced it has acquired SchedMD â€” the leading developer of Slurm, an open-source workload management system for high-performance computing (HPC) and AI â€” to help strengthen the open-source software ecosystem and drive AI innovation for researchers, developers and enterprises. NVIDIA will continue to develop and distribute Slurm as open-source, vendor-neutral software, making it widely available to and supported by the broader HPC and AI community across diverse hardware and software environments. HPC and AI workloads involve complex computations running parallel tasks on clusters that require queuing, scheduling and allocating computational resources. As HPC and AI clusters get larger and more powerful, efficient resource utilization is critical. As the leading workload manager and job scheduler in scalability, throughput and complex policy management, Slurm is used in more than half of the top 10 and top 100 systems in the TOP500 list of supercomputers. Slurm, which is supported on the latest NVIDIA hardware, is also part of the critical infrastructure needed for generative AI, used by foundation model developers and AI builders to manage model training and inference needs. â€œWeâ€™re thrilled to join forces with NVIDIA, as this acquisition is the ultimate validation of Slurmâ€™s critical role in the worldâ€™s most demanding HPC and AI environments,â€ said Danny Auble, CEO of SchedMD. â€œNVIDIAâ€™s deep expertise and investment in accelerated computing will enhance the development of Slurm â€” which will continue to be open source â€” to meet the demands of the next generation of AI and supercomputing.â€ NVIDIA has been collaborating with SchedMD for over a decade and will continue investing in Slurmâ€™s development to ensure it remains the leading open-source scheduler for HPC and AI. NVIDIA will accelerate SchedMDâ€™s access to new systems â€” allowing users of NVIDIAâ€™s accelerated computing platform to optimize workloads across their entire compute infrastructure â€” while also supporting a diverse hardware and software ecosystem, so customers can run heterogeneous clusters with the latest Slurm innovations. NVIDIA will continue to offer open-source software support, training and development for Slurm to SchedMDâ€™s hundreds of customers, which include cloud providers, manufacturers, AI companies and research labs spanning industries such as autonomous driving, healthcare and life sciences, energy, financial services, manufacturing and government. Together with SchedMD, NVIDIA is bolstering the open-source software ecosystem to catalyze HPC and AI innovation across industries, at every scale. Categories: Corporate | Data Center | Software Tags: Artificial Intelligence | Open Source | Parallel Computing | Supercomputing]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 15 Dec 2025 16:30:11 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-acquires-schedmd/</guid>
    </item>
    <item>
      <title>How to Fine-Tune an LLM on NVIDIA GPUs With Unsloth</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</link>
      <description><![CDATA[Modern workflows showcase the endless possibilities of generative and agentic AI on PCs. Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing oneâ€™s schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks. Thatâ€™s where fine-tuning comes in. Unsloth , one of the worldâ€™s most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. Itâ€™s optimized for efficient, low-memory training on NVIDIA GPUs â€” from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark , the worldâ€™s smallest AI supercomputer. Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning. Teaching AI New Tricks Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand. Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods: Parameter-efficient fine-tuning (such as LoRA or QLoRA) : How it works: Updates only a small portion of the model for faster, lower-cost training. Itâ€™s a smarter and efficient way to enhance a model without altering it drastically. Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied â€” including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior. Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs). Full fine-tuning : How it works: Updates all of the modelâ€™s parameters â€” useful for teaching the model to follow specific formats or styles. Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner. Requirements: Large dataset (1,000+ prompt-sample pairs). Reinforcement learning : How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference â€” and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unslothâ€™s Reinforcement Learning Guide for details. Target use case: Improving the accuracy of a model in a particular domain â€” such as law or medicine â€” or building autonomous agents that can orchestrate actions on a userâ€™s behalf. Requirements: A process that contains an action model, a reward model and an environment for the model to learn from. Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth. Fine-tuning requirements on Unsloth. Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently. Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training. Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unslothâ€™s ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers. The framework is built and optimized for NVIDIA hardware â€” from GeForce RTX laptops to RTX PRO workstations and DGX Spark â€” providing peak performance while reducing VRAM consumption. Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows. Check out some of these Unsloth guides: Fine-Tuning LLMs With NVIDIA RTX 50 Series GPUs and Unsloth Fine-Tuning LLMs With NVIDIA DGX Spark and Unsloth Learn how to install Unsloth on NVIDIA DGX Spark . Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform. For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below. Available Now: NVIDIA Nemotron 3 Family of Open Models The new Nemotron 3 family of open models â€” in Nano, Super, and Ultra sizes â€” built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications. Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. Itâ€™s optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers: Up to 60% fewer reasoning tokens, significantly reducing inference cost. A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks. Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026. NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth. Download Nemotron 3 Nano now from Hugging Face , or experiment with it through Llama.cpp and LM Studio. DGX Spark: A Compact AI Powerhouse DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC. Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally. For fine-tuning, DGX Spark enables: Larger model sizes. Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Sparkâ€™s unified memory. More advanced techniques. Full fine-tuning and reinforcement-learning-based workflows â€” which demand more memory and higher throughput â€” run significantly faster on DGX Spark. Local control without cloud queues. Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments. DGX Sparkâ€™s strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines. The table below shows performance for fine-tuning the Llama family of models on DGX Spark. Performance for fine-tuning Llama family of models on DGX Spark. As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark. Learn more about how DGX Spark enables intensive AI tasks . #ICYMI â€” The Latest Advancements in NVIDIA RTX AI PCs ðŸš€ FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%. âœ¨ Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs. ðŸ¤Mistral AI Unveils New Model Family Optimized for NVIDIA GPUs The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp. ðŸŽ¨Blender 5.0 Lands With HDR Color and Major Performance Gains The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | GeForce | NVIDIA RTX | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 15 Dec 2025 14:00:11 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</guid>
    </item>
    <item>
      <title>Cheers to AI: ADAM Robot Bartender Makes Drinks at Vegas Golden Knights Game</title>
      <link>https://blogs.nvidia.com/blog/adam-robot-vegas-golden-knights-thor/</link>
      <description><![CDATA[In Las Vegasâ€™s T-Mobile Arena, fans of the Golden Knights are getting more than just hockey â€” theyâ€™re getting a taste of the future. ADAM, a robot developed with NVIDIA Isaac libraries , is pouring drinks and turning heads in one of the NHLâ€™s most exciting venues. ADAM, short for Automated Dual Arm Mixologist, was developed by Las-Vegas based Richtech Robotics. Itâ€™s not just a novelty â€” itâ€™s a solution to real-world challenges in hospitality: labor shortages and demands for unique customer experiences. â€œThe hospitality industry faces significant labor challenges, and ADAM is our answer to meeting those needs while elevating the customer experience,â€ said Matt Casella, president of Richtech Robotics. â€œWith NVIDIAâ€™s Isaac platform, weâ€™ve developed a solution thatâ€™s scalable, consistent, and frankly, creates memorable moments for fans. The response at T-Mobile Arena has been phenomenalâ€”people love interacting with ADAM.â€ Learning to Serve Drinks in Simulation Before ADAM ever poured a drink, it trained in a virtual bar. Richtech used NVIDIA Isaac Sim , an open-source, reference robotic simulation framework built on NVIDIA Omniverse , to build a high-fidelity and physically accurate simulation of ADAMâ€™s workstation, complete with cups, utensils and lighting variations. The team generated synthetic data to teach ADAM how to recognize objects even in tricky conditions like glare or reflection. ADAMâ€™s skills such as pouring and shaking were refined in simulation using Isaac Lab , NVIDIAâ€™s open source robot learning framework. The result: a robot that doesnâ€™t just follow instructions â€” it adapts to its environment with precision. Running Real-Time AI at the Edge With Jetson ADAM runs on NVIDIA Jetson AGX Orin , the powerful edge AI platform capable of 275 TOPS of compute. Using Isaac ROS 2 libraries, ADAM captures camera feeds, detects objects and calibrates the workspace in real time. ADAMâ€™s perception stack â€” built with TAO Toolkit and optimized with TensorRT â€” enables it to identify cups, measure liquid levels and adjust movements with less than 40 milliseconds of latency. That means ADAM can spot a misplaced cup, detect when foam reaches the rim and correct a pour â€” all without missing a beat. Creating Industrial Dexterity With NVIDIA Thor While ADAM is busy serving drinks at Golden Knights games, Richtech Robotics is also making major strides in industrial automation with Dex, a new mobile humanoid robot built for factory and warehouse environments. Recently unveiled at GTC DC , Dex combines the mobility of an autonomous wheeled platform with the precision of dual-arm dexterity. Itâ€™s designed to handle such light-to-medium industrial tasks as machine operation, parts sorting, material handling and packaging â€” all with the flexibility to take on different tools and workflows. Dex runs on NVIDIA Jetson Thor , a next-generation robotics processor that gives it the ability to deliver real-time sensor processing and AI reason in dynamic industrial settings. Dex was trained from a blend of real world and synthetic data generated from Isaac Sim. This allowed Dexâ€™s model to be generalized across a multitude of scenarios. Learn more about Jetson Thor and about festive Jetson platform holiday prices . Categories: Corporate | Robotics Tags: NVIDIA Isaac Sim | NVIDIA Jetson | NVIDIA Omniverse | TensorRT]]></description>
      <author>NVIDIA</author>
      <pubDate>Fri, 12 Dec 2025 16:00:04 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/adam-robot-vegas-golden-knights-thor/</guid>
    </item>
    <item>
      <title>NVIDIA Awards up to $60,000 Research Fellowships to PhD Students</title>
      <link>https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</link>
      <description><![CDATA[For 25 years, the NVIDIA Graduate Fellowship Program has supported graduate students doing outstanding work relevant to NVIDIA technologies. Today, the program announced the latest awards of up to $60,000 each to 10 Ph.D. students involved in research that spans all areas of computing innovation. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding the fellowship year. Their work puts them at the forefront of accelerated computing â€” tackling projects in autonomous systems, computer architecture, computer graphics, deep learning, programming systems, robotics and security. The NVIDIA Graduate Fellowship Program is open to applicants worldwide. The 2026-2027 fellowship recipients are: Jiageng Mao , University of Southern California â€” Solving complex physical AI problems by using diverse priors from internet-scale data to enable robust, generalizable intelligence for embodied agents in the real world. Liwen Wu , University of California San Diego â€” Enriching realism and efficiency in physically based rendering with neural materials and neural rendering. Manya Bansal , Massachusetts Institute of Technology â€” Designing programming languages for modern accelerators that enable developers to write modular, reusable code without sacrificing the low-level control required for peak performance. Sizhe Chen , University of California, Berkeley â€” Securing AI in real-world applications, currently securing AI agents against prompt injection attacks with general and practical defenses that preserve the agentâ€™s utility. Yunfan Jiang , Stanford University â€” Developing scalable approaches to build generalist robots for everyday tasks through hybrid data sources spanning real-world whole-body manipulation, large-scale simulation and internet-scale multimodal supervision. Yijia Shao , Stanford University â€” Researching human-agent collaboration by developing AI agents that can communicate and coordinate with humans during task execution, and designing new human-agent interaction interfaces. Shangbin Feng , University of Washington â€” Advancing model collaboration: multiple machine learning models, trained on different data and by different people, collaborate, compose and complement each other for an open, decentralized and collaborative AI future. Shvetank Prakash , Harvard University â€” Advancing hardware architecture and systems design with AI agents built on new algorithms, curated datasets and agent-first infrastructure. Irene Wang , Georgia Institute of Technology â€” Developing a holistic codesign framework that integrates accelerator architecture, network topology and runtime scheduling to enable energy-efficient and sustainable AI training at scale. Chen Geng , Stanford University â€” Modeling 4D physical worlds with scalable data-driven algorithms and physics-inspired principles, advancing physically grounded 3D and 4D world models for robotics and scientific applications. We also acknowledge the 2026-2027 fellowship finalists: Zizheng Guo , Peking University Peter Holderrieth , Massachusetts Institute of Technology Xianghui Xie , Max Planck Institute for Informatics Alexander Root , Stanford University Daniel Palenicek , Technical University of Darmstadt Categories: Generative AI | Research Tags: Artificial Intelligence | Education]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 17:00:44 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</guid>
    </item>
    <item>
      <title>NVIDIA and AWS Expand Full-Stack Partnership, Providing the Secure, High-Performance Compute Platform Vital for Future Innovation</title>
      <link>https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</link>
      <description><![CDATA[At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI. As part of this expansion, AWS will support NVIDIA NVLink Fusion â€” a platform for custom AI infrastructure â€” for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure. Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities. AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion. AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms. AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems. By supporting AWSâ€™s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWSâ€™s cloud infrastructure and accelerating new AI service rollout. â€œGPU compute demand is skyrocketing â€” more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,â€ said Jensen Huang, founder and CEO of NVIDIA. â€œWith NVIDIA NVLink Fusion coming to AWS Trainium4, weâ€™re unifying our scale-up architecture with AWSâ€™s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution â€” bringing advanced AI to every company, in every country, and accelerating the worldâ€™s path to intelligence.â€ â€œAWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,â€ said Matt Garman, CEO of AWS. â€œWith NVIDIA, weâ€™re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.â€ Convergence of Scale and Sovereignty AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industryâ€™s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks. These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations. NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements. For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWSâ€™s industry-leading cloud infrastructure and services â€” known for its reliability, security and scalability â€” with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches. The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks. NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations Beyond hardware, the partnership expands integration of NVIDIAâ€™s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock , enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy. The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrockâ€™s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents. NVIDIA Software on AWS Simplifies Developer Experience NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS , an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost. These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when itâ€™s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs. Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production. This expanded support builds on AWSâ€™s existing integrations with NVIDIA technologies â€” including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo , as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock â€” that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever. Accelerating Physical AI With AWS Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment. NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS , enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation , Cosmos WFMs are also available on AWS Batch as containers. Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab . Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development. Sustained Collaboration Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats. Learn more about NVIDIA and AWSâ€™s collaboration and join sessions at AWS re:Invent , running through Friday, Dec. 5, in Las Vegas. Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Cosmos | Events | Isaac | Nemotron | NVIDIA Blackwell | NVIDIA NIM | NVIDIA Spectrum-X | NVLink | Open Source | Physical AI | Riva]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 02 Dec 2025 16:00:27 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</guid>
    </item>
    <item>
      <title>At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI</title>
      <link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link>
      <description><![CDATA[Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools â€” with potential applications in virtually every research field. At NeurIPS , one of the worldâ€™s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the worldâ€™s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety. NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more. These initiatives deepen NVIDIAâ€™s commitment to open source â€” an effort recognized by a new Openness Index from Artificial Analysis , an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details. NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving NVIDIA DRIVE Alpamayo-R1 (AR1) , the worldâ€™s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning â€” a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy . While previous iterations of self-driving models struggled with nuanced situations â€” a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane â€” reasoning gives autonomous vehicles the common sense to drive more like humans do. AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route. For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces â€” explanations on why it took certain actions â€” and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers. https://blogs.nvidia.com/wp-content/uploads/2025/12/construction_worker.mp4 AR1â€™s open foundation, based on NVIDIA Cosmos Reason , lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications. For post-training AR1, reinforcement learning has proven especially effective â€” researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model. NVIDIA DRIVE Alpamayo-R1 is now available on GitHub and Hugging Face , and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets . NVIDIA has also released the open-source AlpaSim framework to evaluate AR1. Learn more about reasoning VLA models for autonomous driving . Customizing NVIDIA Cosmos for Any Physical AI Use Case Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook . Itâ€™s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation. There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include: LidarGen , the first world model that can generate lidar data for AV simulation. Omniverse NuRec Fixer , a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data. Cosmos Policy , a framework for turning large pretrained video models into robust robot policies â€” a set of rules that dictate a robotâ€™s behavior. ProtoMotions3 , an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs) . Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen. Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics. Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM. NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs. AV developer Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X , Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation. NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include: MultiTalker Parakeet : An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations. Sortformer : A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream â€” a process called diarization â€” in real time. Nemotron Content Safety Reasoning : A reasoning-based AI safety model that dynamically enforces custom policies across domains. Nemotron Content Safety Audio Dataset : A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities. NeMo Gym : an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR). NeMo Data Designer Library : Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation. NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow. NeurIPS attendees can explore these innovations at the Nemotron Summit , taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. NVIDIA Research Furthers Language AI Innovation Of the dozens of NVIDIA-authored research papers at NeurIPS , here are a few highlights advancing language models: Audio Flamingo 3: Advancing Audio Intelligence With Fully Open Large Audio Language Models : This large audio language model is capable of reasoning across speech, sound and music. It can understand and reason audio segments up to 10 minutes in length, achieving state-of-the-art results on over 20 benchmarks. Minitron-SSM: Efficient Hybrid Language Model Compression Through Group-Aware SSM Pruning : This poster introduces a pruning method capable of compressing hybrid models, demonstrated by pruning and distilling Nemotron-H 8B from 8 billion to 4 billion parameters. The resulting model surpasses the accuracy of similarly sized models while achieving 2x faster inference throughput. Jet-Nemotron: Efficient Language Model With Post Neural Architecture Search : This work presents a cost-efficient post-training pipeline for developing new efficient language model architectures, and introduces a hybrid-architecture model family produced with the pipeline. These models match or surpass the accuracy of leading full-attention baselines while delivering substantially higher generation throughput. Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models : This project introduces a new small language model (SLM) architecture that redesigns SLMs around real-world latency rather than parameter count â€” achieving state-of-the-art speed and accuracy. ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models : Prolonged reinforcement learning, or ProRL, is a technique that extends model training over longer periods. In this NeurIPS poster, NVIDIA researchers describe how this methodology results in models that consistently outperform base models for reasoning. View the full list of events at NeurIPS , running through Sunday, Dec. 7, in San Diego. See notice regarding software product information. Categories: Corporate | Driving | Generative AI | Research | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | NVIDIA Research | Open Source | Physical AI | Synthetic Data Generation | Transportation]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 01 Dec 2025 17:00:48 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid>
    </item>
    <item>
      <title>From Government to Gaming, AI Is â€˜Strengthening Koreaâ€™s Digital Foundation,â€™ NVIDIA Leader Says at AI Day Seoul</title>
      <link>https://blogs.nvidia.com/blog/ai-day-seoul/</link>
      <description><![CDATA[Last week, more than 1,000 attendees joined NVIDIA AI Day Seoul to learn about sovereign AI â€” including breakout sessions on agentic and physical AI, hands-on workshops and a startupâ€¦Read Article]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 26 Nov 2025 17:00:32 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-day-seoul/</guid>
    </item>
    <item>
      <title>FLUX.2 Image Generation Models Now Released, Optimized for NVIDIA RTX GPUs</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</link>
      <description><![CDATA[Black Forest Labs â€” the frontier AI research lab developing visual generative AI models â€” today released the FLUX.2 family of state-of-the-art image generation models. FLUX.2 is packed with new tools and capabilities, including a multi-reference feature that can generate dozens of similar image variations, in photorealistic detail and with cleaner fonts â€” even at scale. NVIDIA has worked with Black Forest Labs and ComfyUI to make the models available with FP8 quantizations and RTX GPU performance optimizations at launch, decreasing the VRAM required to run them by 40% and improving performance by 40%. Requiring no special software package to run, the models are available directly in ComfyUI . State-of-the-Art Visual Intelligence Images generated by FLUX.2 are photorealistic, even at scale, featuring up to 4 megapixel resolution with real-world lighting and physics to eliminate that â€œAI lookâ€ that undermines visual fidelity. The models add direct pose control to explicitly specify the pose of a subject or character in an image, as well as deliver clean, readable text across infographics, user interface screens and even multilingual content. Plus, the new multi-reference feature enables artists to select up to six reference images where the style or subject stays consistent â€” eliminating the need for extensive model fine-tuning. Stunning, photorealistic details. Image courtesy of Black Forest Labs. For a complete overview of new FLUX.2 features, read Black Forest Labsâ€™ blog . Optimized for RTX The new FLUX.2 models are impressive, but also quite demanding. They run a staggering 32-billion-parameter model requiring 90GB VRAM to load completely. Even using lowVRAM mode â€” a popular setting that allows artists to only load the active model at a time â€” the VRAM requirement is still 64GB, which puts the model virtually out of reach for any consumer card to use effectively. To broaden FLUX.2 model accessibility, NVIDIA and Black Forest Labs collaborated to quantize the model to FP8 â€” reducing the VRAM requirements by 40% at comparable quality. FLUX.2 is here. And to make this model accessible on GeForce RTX GPUs, NVIDIA has partnered with ComfyUI â€” a popular application to run visual generative AI models on PC â€” to improve the appâ€™s RAM offload feature, known as weight streaming. Using the upgraded feature, users can offload parts of the model to system memory, extending the available memory on their GPUs â€” albeit with some performance loss, as system memory is slower than GPU memory. NVIDIA has also been collaborating with ComfyUI to optimize model performance on NVIDIA and GeForce RTX GPUs, including optimizations for FP8 checkpoints. Get started with FLUX.2 today. Update ComfyUI and check out the FLUX.2 templates, or visit Black Forest Labsâ€™ Hugging Face page to download the model weights. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . Categories: Generative AI Tags: Artificial Intelligence | Conversational AI | Creators | GeForce | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 25 Nov 2025 15:53:21 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</guid>
    </item>
    <item>
      <title>AI On: 3 Ways Specialized AI Agents Are Reshaping Businesses</title>
      <link>https://blogs.nvidia.com/blog/specialized-ai-agents/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of the AI On blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries. As agentic AI adoption continues to grow, with open-source models and tools maturing, companies across industries are increasingly asking: what AI agents should we build to solve our unique business challenges? Although faster outcomes are a core benefit of using AI, organizations are finding that specialization is the key to business impact and long-term AI adoption. Rather than relying on one-size-fits-all models and services, leading companies are developing specialized AI agents designed to understand and act within the needs of a specific use case. CrowdStrike, PayPal and Synopsys are examples of companies combining NVIDIA Nemotron open foundation models with their proprietary data and institutional knowledge to create specialized applications. The results are intelligent agents that have the level of expertise required to work alongside human colleagues and boost business operations. 1. CrowdStrike Defends Against Modern Cyber Threats In cybersecurity, speed and precision are essential, especially as cyber threats become more advanced and grow to larger scales. To meet these rapidly evolving digital threats, CrowdStrike is building specialized AI agents that can work alongside security teams through Charlotte AI AgentWorks. These agents, powered by NVIDIA Nemotron open models and NVIDIA NIM microservices, automate high-volume tasks such as alert triage and remediation, allowing human analysts to focus on higher-order decision-making. Built on open models and continuously trained by incident responders, CrowdStrikeâ€™s Agentic Security Platform increases accuracy of alert triage from 80% to 98.5% , reducing security analyst teamsâ€™ manual effort tenfold. The platform can adapt to new risks and collaborates across the security operations center. 2. PayPalâ€™s AI Agents Power Frictionless Commerce at Scale PayPal, a leader in payments and e-commerce, is building agent-driven infrastructure to accelerate intelligent commerce . The companyâ€™s specialized AI agents, developed on Nemotron open models, will enable the first wave of conversational commerce experiences, where agents can shop, buy and pay on a userâ€™s behalf. With this approach, PayPal built a fine-tuning pipeline in two weeks and reduced latency by nearly 50% while maintaining the high accuracy required to serve its 430 million customers and 30 million merchants. PayPalâ€™s agents rely on open, modular models that are fine-tuned specifically for payments and commerce, giving the company the control to balance performance, accuracy and cost at a massive scale. 3. Synopsys Advances Agentic AI for Chip Design Workflows The complexity of modern semiconductor design and manufacturing calls for expertise, precision and speed. Synopsys is pioneering an agentic AI framework that can be deployed throughout the chip development workflow. Synopsysâ€™ vision for agentic AI includes Synopsys AgentEngineer technology that can significantly boost productivity in research and development, identifying critical design bugs and helping reduce costly delays that traditional techniques can miss. In early trials of a formal verification workflow, Synopsys AI agents running on NVIDIA accelerated infrastructure achieved a 72% boost in productivity. Using open models fine-tuned for each engineering task, as well as software like the NVIDIA NeMo Agent Toolkit and Blueprints, Synopsys is enabling a new frontier of AI-enabled chip design. Building Specialized AI Agents With NVIDIA Technologies Companies across industries are taking the following steps to transform their proprietary knowledge into specialized AI agents: Evaluate open models, like NVIDIA Nemotron , that provide a powerful building block to create specialized models for any domain. Curate, generate and secure domain data using NVIDIA NeMo for agent lifecycle management. Create specialized agents using customized models that have access to proprietary data. Continue to fine-tune agents over time with a data flywheel . Learn how NVIDIA Nemotron can help businesses build specialized AI agents for maximum productivity and return on investment. Categories: Data Center | Generative AI Tags: Agentic AI | AI On | Artificial Intelligence | Cybersecurity | Financial Services | Industrial and Manufacturing | Nemotron | NVIDIA Blueprints | NVIDIA NeMo | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 24 Nov 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/specialized-ai-agents/</guid>
    </item>
  </channel>
</rss>
