<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Fri, 09 Jan 2026 10:04:44 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>AI Copilot Keeps Berkeleyâ€™s X-Ray Particle Accelerator on Track</title>
      <link>https://blogs.nvidia.com/blog/ai-copilot-berkeley-x-ray-particle-accelerator/</link>
      <description><![CDATA[In the rolling hills of Berkeley, California, an AI agent is supporting high-stakes physics experiments at the Advanced Light Source (ALS) particle accelerator. Researchers at the Lawrence Berkeley National Laboratory ALS facility recently deployed the Accelerator Assistant, a large language model (LLM)-driven system to keep X-ray research on track. The Accelerator Assistant â€” powered by an NVIDIA H100 GPU harnessing CUDA for accelerated inference â€” taps into institutional knowledge data from the ALS support team and routes requests through Gemini, Claude or ChatGPT. It writes Python and solves problems, either autonomously or with a human in the loop. This is no small task. The ALS particle accelerator sends electrons traveling near the speed of light in a 200-yard circular path, emitting ultraviolet and X-ray light, which is directed through 40 beamlines for 1,700 scientific experiments per year. Scientists worldwide use this process to study materials science, biology, chemistry, physics and environmental science. At the ALS, beam interruptions can last minutes, hours or days, depending on the complexity, halting concurrent scientific experiments in process. And much can go wrong: the ALS control system has more than 230,000 process variables. â€œItâ€™s really important for such a machine to be up, and when we go down, there are 40 beamlines that do X-ray experiments, and they are waiting,â€ said Thorsten Hellert, staff scientist from the Accelerator Technology and Applied Physics Division at Berkeley Lab and lead author of a research paper on the groundbreaking work. Until now, facility staff troubleshooting issues have had to quickly identify the areas, retrieve data and gather the right personnel for analysis under intense time pressure to get the system back up and running. â€œThe novel approach offers a blueprint for securely and transparently applying large language model-driven systems to particle accelerators, nuclear and fusion reactor facilities, and other complex scientific infrastructures,â€ said Hellert. The research team demonstrated that the Accelerator Assistant can autonomously prepare and run a multistage physics experiment, cutting setup time and reducing efforts by 100x. Applying Context Engineering Prompts to Accelerator Assistant The ALS operators interact with the system through either a command line interface or Open WebUI, which enables interaction with various LLMs and is accessible from control room stations, as well as remotely. Under the hood, the system uses Osprey, a framework developed at Berkeley Lab to apply agent-based AI safely in complex control systems. Each user is authenticated and the framework maintains personalized context and memory across sessions, and multiple sessions can be managed simultaneously. This allows users to organize distinct tasks or experiments into separate threads. These inputs are routed through the Accelerator Assistant, which makes connections to the database of more than 230,000 process variables, a historical database archive service and Jupyter Notebook-based execution environments. â€œWe try to engineer the context of every language model call with whatever prior knowledge we have from this execution up to this point,â€ said Hellert. Inference is done either locally â€” using Ollama, which is an open-source tool for running LLMs with a personal computer, on an H100 GPU node located within the control room network â€” or externally with the CBorg gateway, which is a lab-managed interface that routes requests to external tools such as ChatGPT, Claude or Gemini. The hybrid architecture balances secure, low-latency, on-premises inference with access to the latest foundation models. Integration with EPICS (Experimental Physics and Industrial Control System) enables operator-standard safety constraints for direct interaction with accelerator hardware. EPICS is a distributed control system used in large-scale scientific facilities such as particle accelerators. Engineers can write Python code in Jupyter Notebook that can communicate with it. Basically, conversational input is turned into a clear natural language task description for objectives without redundancy. External knowledge such as personalized memory tied to users, documentation and accelerator databases are integrated to assist with terminology and context. â€œItâ€™s a large facility with a lot of specialized expertise,â€ said Hellert. â€œMuch of that knowledge is scattered across teams, so even finding something simple â€” like the address of a temperature sensor in one part of the machine â€” can take time.â€ Tapping Accelerator Assistant to Aid Engineers, Fusion Energy Development Using the Accelerator Assistant, engineers can start with a simple prompt describing their goal. Behind the scenes, the system draws on carefully prepared examples and keywords from accelerator operations to guide the LLMâ€™s reasoning. â€œEach prompt is engineered with relevant context from our facility, so the model already knows what kind of task itâ€™s dealing with,â€ said Hellert. Each agent is an expert in that field, he said. Once the task is defined, the agent brings together its specialized capabilities â€” such as finding process variables or navigating the control system â€” and can automatically generate and run Python scripts to analyze data, visualize results or interact safely with the accelerator itself. â€œThis is something that can save you serious time â€” in the paper, we say two orders of magnitude for such a prompt,â€ said Hellert. Looking ahead, Hellert aims to have the ALS engineers put together a wiki that documents the many processes that go on to support the experiments. These documents could help the agents run the facilities autonomously â€” with a human in the loop to approve the course of action. â€œOn these high-stakes scientific experiments, even if itâ€™s just a TEM microscope or something that might cost $1 million, a human in the loop can be very important,â€ said Hellert. The work has already expanded beyond ALS as part of the DOEâ€™s Genesys mission, with the framework being deployed across U.S. particle accelerator facilities. Next up, Hellert just began collaborating with engineers at the ITER fusion reactor â€” the worldâ€™s largest â€” in France for implementing the framework for use in the fusion reactor facility. He also has a collaboration in the works with the Extremely Large Telescope ELT, in northern Chile. Benefiting Humanity: Scientific Impact of Experiments Supported by ALS Beyond optimizing the accelerator and other industrial operations, the work at the ALS directly enables scientific breakthroughs with global impact. The facilityâ€™s stable X-ray beams underpin research in health, climate resilience and planetary science. During the COVID-19 pandemic, ALS researchers helped characterize a rare antibody that could neutralize SARS-CoV-2. Structural biology experiments at Beamline 4.2.2 revealed how six molecular loops of the antibody latch onto and disable the viral spike protein. The findings supported the rapid development of a therapeutic that remained effective through multiple variants. ALS science also contributes to climate-focused research. Metal-organic frameworks (MOFs) â€” a class of porous materials capable of capturing water or carbon dioxide from air â€” were extensively studied across several ALS beamlines. These experiments supported foundational work that ultimately led to the 2025 Nobel Prize in Chemistry, recognizing the transformative potential of MOFs for sustainable water harvesting and carbon management. In planetary science, ALS measurements of samples returned from NASAâ€™s OSIRIS-REx mission helped trace the chemical history of asteroid Bennu. X-ray analyses provided evidence that such asteroids carried water and molecular precursors of life to early Earth, deepening our understanding of the origins of the planetâ€™s habitable conditions. Categories: Generative AI Tags: GPU Computing]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 08 Jan 2026 17:00:41 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-copilot-berkeley-x-ray-particle-accelerator/</guid>
    </item>
    <item>
      <title>Japan Science and Technology Agency Develops NVIDIA-Powered Moonshot Robot for Elderly Care</title>
      <link>https://blogs.nvidia.com/blog/japan-science-technology-agency-develops-moonshot-robot/</link>
      <description><![CDATA[The next universal technology since the smartphone is on the horizon â€” and it may be a little less pocket friendly. The Moonshot research program, funded by the Japan Science and Technology Agency and accelerated by NVIDIA AI and robotics technologies, is working to create a world by 2050 where AI-powered, autonomously learning robots are integrated into Japanese citizensâ€™ everyday lives. Thatâ€™s just goal No. 3 of the broader Moonshot initiative, which includes researchers from across Japanâ€™s universities and comprises 10 ambitious technology goals â€” from ultra-early disease prediction to sustainable resource circulation. In light of Japanâ€™s rising elderly population, many of the research projects underway center on how robots can aid in senior care. This includes designing a robot thatâ€™s capable of caregiving tasks like cooking, cleaning and hygiene care. NVIDIA Architecture Powers On Moonshot Robots NVIDIA technologies are integrated into every level of the Moonshot projectâ€™s senior care robots known as AI-Driven Robot for Embrace and Care, or AIREC. Dry-AIREC robot, the larger and more mobile member of the Moonshot family, has two NVIDIA GPUs onboard. For AIREC-Basic, primarily used for data collection for the motion foundation model, three NVIDIA Jetson Orin NX modules power AI processing at the edge. Pictured is AIREC-Basic (left) and AIREC-Basic (right). Plus, NVIDIA Isaac Sim , an open-source robotic simulation framework, was used to train the AIREC robots to perform specific tasks, such as estimating the forces between objects. The integration of NVIDIA technologies and AI into the robot development process has allowed this project to go from a far-fetched dream to reality faster than imagined. â€œFive years ago, before generative AI, few people believed that this application was possible,â€ said Tetsuya Ogata, professor and director of the Institute for AI and Robotics at Waseda University. â€œNow, the atmosphere surrounding this technology has changed, so we can seriously think about this kind of application.â€ Building a Full Set of Caregiving Capabilities Additional research projects are underway to develop the Moonshot robotâ€™s elderly-care capabilities. â€œWeâ€™re focusing on things like changing diapers, helping patients take baths and providing meal assistance, so those actions can be supported by the robots, and caregivers can focus on improving the patientsâ€™ lives,â€ said Misa Matsumura, a bioengineering masterâ€™s student at the University of Tokyo. A recent paper by Matsumura â€” presented at the IEEE/RSJ International Conference on Intelligent Robots and Systems â€” focused on repositioning, an essential action in elderly care to prevent bed sores and enable diaper changing. Automating repositioning with a humanoid robot â€” while considering the elderly care patientsâ€™ personal states and bodily needs â€” is no easy feat. To train the Dry-AIREC robots for this research endeavor, Matsumuraâ€™s team used laptops powered by NVIDIA RTX GPUs. Matsumura used 3D posture estimation, trajectory calculations and force estimation to further develop the robotsâ€™ capabilities. Dry-AIRECâ€™s fisheye and depth cameras helped assess the movements required to reposition patients. The exact repositioning method needed for a patient is found through trajectory calculations based on movement data from skilled caregivers. The robot must also use the right amount of force in repositioning to complete the action without causing the patient pain. By predicting the pressure required to press the shoulders and knees, it determines the appropriate timing for movement â€” enabling actions with the ideal applied force. Preliminary experiments were done using mannequins, and Matsumuraâ€™s research has now advanced to incorporate humans testing the robots. Matsumura is conducting ongoing research to further improve this action for Dry-AIREC. Milestone images for goal No. 3 for the Japan Science and Technology Agency. Among the many projects within the Moonshot program, developing robots for elderly care has particular significance for some of the researchers due to the projectâ€™s social and personal implications. â€œAlthough my study focus is on medical robotics, I decided to join this project because my mother is growing older, and that experience has given me an appreciation for the importance of personal care,â€ said Etsuko Kobayashi, professor of bioengineering at the University of Tokyo and Matsumuraâ€™s graduate advisor. â€œI found that my experience in medical robotics can be meaningfully extended to care robotics, contributing to the development of safe and reliable robotic systems for human-centered applications.â€ The Moonshot team for goal No. 3 will showcase their progress at the 2026 International Symposium on System Integration in January. Learn more about NVIDIA Isaac Sim . Categories: Research | Robotics Tags: GPU | NVIDIA Isaac Sim | NVIDIA Jetson | NVIDIA RTX | Robotics]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 08 Jan 2026 16:00:29 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/japan-science-technology-agency-develops-moonshot-robot/</guid>
    </item>
    <item>
      <title>More Ways to Play, More Games to Love â€” GeForce NOW Wraps CES With Linux Support, Fire TV App, Flight Stick Controls</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-ces-2026-recap/</link>
      <description><![CDATA[NVIDIA is wrapping up a big week at the CES trade show with a set of GeForce NOW announcements that are bringing more ways to play and more games to the cloud. From new native apps for Linux and Amazon Fire TV streaming sticks to hands-on throttle-and-stick (HOTAS) support for simulation fans and a new single sign-on option, GeForce NOW is expanding across devices and making it easier to jump straight into gameplay. Topping it off, a new wave of AAA titles â€” including IO Interactiveâ€™s 007 First Light , Capcomâ€™s Resident Evil Requiem and Pearl Abyssâ€™ Crimson Desert â€” is gearing up to join the GeForce NOW library for high-fidelity, low-latency streaming across the globe. Get ready with the six new games available to stream this week. Leveling Up Where Gamers Play The family of GeForce NOW supported devices is growing. A new native GeForce NOW app is launching in beta for Linux, starting with Ubuntu 24.04 and later. This answers a long-standing request from the gaming community, turning compatible systems into RTX-powered gaming rigs that can stream at up to 5K resolution and 120 frames per second (fps) or 1080p 360 fps. With rendering handled in the cloud, Linux gamers can tap into ray tracing, NVIDIA DLSS and other RTX features without needing a local, high-end GPU.â€‹ Things are heating up: GeForce NOW on Fire TV streaming sticks. A new native app for Amazon Fire TV streaming sticks transforms the tiny device into a big-screen cloud gaming endpoint, giving members another way to stream their PC libraries straight to the TV with a gamepad in hand. It builds on existing device support, making it easier for gamers to pick up and play from the couch without a console or gaming PC attached.â€‹ Clearer Skies Ahead Cleared for takeoff in the cloud. Flight control support has landed in the cloud, letting virtual pilots connect their favorite gear â€” including industry-leading Thrustmaster and Logitech setups â€” to custom HOTAS configurations. Whether locking in a full desktop throttle-and-stick rig or fine-tuning a split cockpit arrangement, everythingâ€™s ready for takeoff. Paired with the incredible RTX 5080-powered performance available to Ultimate members, plus ultralow-latency streaming and NVIDIA Reflex in supported titles, pilots can now enjoy ultra-precise, hyper-immersive flight experiences straight from the cloud. Soaring over detailed landscapes in Microsoft Flight Simulator 2024 , dogfighting in War Thunder or exploring the stars in Elite Dangerous has never felt smoother. Easier Sign-Ins, Faster Game Time GeForce NOW now supports new games â€” and more ways to get into them faster. Because remembering passwords is so last season. Battle.net single sign-on recently joined the service, letting members jump straight into supported titles without juggling extra credentials each time. And coming soon early this year, Gaijin.net integration will make takeoff even smoother â€” one simple sign-on connects more game libraries across devices. Fewer passwords and pop-ups mean more time playing â€” whether on a desktop, laptop, handheld device or TV. Next Up in the Cloud A new wave of AAA titles is headed to GeForce NOW, bringing spies, survivors and freeswords to the cloud for members to stream across their favorite devices. The games each bring a distinct flavor â€” from stealthy espionage to nerve-shredding horror and gritty fantasy combat â€” and theyâ€™re all gearing up to join an already-stacked GeForce NOW library when they launch on PC. Earn the number. IO Interactiveâ€™s 007 First Light brings a modern James Bond origin story to the cloud when the PC title launches May 27, inviting members to step into the shadows as Bond begins his journey. Experience a balanced mix of stealth and action built around a â€œbreathingâ€ gameplay loop with IOIâ€™s cinematic, set-piece-driven storytelling. Approach encounters through stealth, direct action or creative improvisation, with multiple viable paths to each objective that let players define Bondâ€™s style. Welcome to the Wrenwood Hotel â€” late checkout is not recommended. Capcomâ€™s Resident Evil Requiem continues the legendary survival-horror saga with a new chapter that doubles down on tense exploration, eerie environments and resource-tight combat. Expect dimly lit corridors, unsettling encounters and that classic feeling of weighing every bullet and healing item before moving forward when playing as Grace, and exhilarating, death-defying action when playing as Leon. Everything matters in the cloud. Gaijin Entertainmentâ€™s Active Matter, coming this year , is a realistic military shooter where players join dangerous raids for loot or intense player vs. player battles set in a fractured multiverse. Active matter can be harvested from active matter-transformed creatures or seized from other players. Join this never-ending hunt to improve the chances to find the way out of the time loop. Open world, closed casket. Pearl Abyssâ€™ Crimson Desert is a stunning open-world action adventure set in a war-torn fantasy land, pairing large-scale exploration with cinematic storytelling and hard-hitting, combo-focused combat. One moment is a quiet ride across windswept plains, the next is a chaotic clash against towering foes with mystical power effects lighting up the battlefield. Playing these titles on GeForce NOW means jumping into big-budget experiences with RTX 5080-class performance, high frame rates and advanced graphics features on a wide range of devices, without worrying about downloads, patches or local hardware requirements. Members can pick up the same adventure on a desktop, laptop, handheld device or TV, letting each of these worlds shine wherever and however they choose to play. Hello New Games Welcome back to the plague. The eerie world of Pathologic 3 , from Ice-Pick Lodge and tinyBuild, invites players to step once again into a haunting town teetering on the edge of disaster. Time is fleeting, trust is scarce and every conversation feels like a test of loyalty â€” or survival. With its signature blend of tension, philosophy and dark humor, Pathologic 3 doesnâ€™t just tell a story; it dares players to live through it. In addition, members can look for the following: StarRupture (New release on Steam , Jan. 6) Ancient Farm (New release on Steam , Jan, 87) Pathologic 3 (New release on Steam , Jan. 9) Blood West ( Epic Games Store ) Paradise Killer ( Epic Games Store ) Supermarket Simulator ( Xbox , available on Game Pass) GeForce RTX 5080-ready game : HITMAN â€“ World of Assassination (Steam, Epic Games Store and Xbox, available on Game Pass) What are you planning to play this weekend? Let us know on X or in the comments below. NOW that's what I call gaming! ðŸŽ® More games âš¡ More platforms ðŸ’»ðŸ“±ðŸ–¥ï¸ðŸ•¹ï¸ More devices Experience GeForce RTX-powered PC gaming across more screens than ever. https://t.co/AhcjD6DSEK pic.twitter.com/cuu1Xu3Xe5 â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) January 6, 2026 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 08 Jan 2026 14:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-ces-2026-recap/</guid>
    </item>
    <item>
      <title>Steel, Sensors and Silicon: How Caterpillar Is Bringing Edge AI to the Jobsite</title>
      <link>https://blogs.nvidia.com/blog/caterpillar-ces-2026/</link>
      <description><![CDATA[At CES, Caterpillar reveals how itâ€™s integrating NVIDIA technologies, from NVIDIA Jetson Thor to speech models, to transform the worldâ€™s heavy industries.]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 07 Jan 2026 17:15:15 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/caterpillar-ces-2026/</guid>
    </item>
    <item>
      <title>From Warehouse to Wallet: New State of AI in Retail and CPG Survey Uncovers How AI Is Rewiring Supply Chains and Customer Experiences</title>
      <link>https://blogs.nvidia.com/blog/ai-in-retail-cpg-survey-2026/</link>
      <description><![CDATA[AI has transformed retail and consumer packaged goods (CPG) operations, enhancing customer analysis and segmentation to enable greater personalization for marketing and advertising, and boosting the speed and accuracy of demand forecasting for supply chains and logistics. Companies are also raising the bar for customer engagement through intelligent digital shopping assistants and catalog enrichment by dynamically enhancing and localizing product information. AI agents are increasing the speed and efficiency of operations, while physical AI systems are helping streamline and automate warehouse and supply chain operations. NVIDIAâ€™s third annual State of AI in Retail and Consumer Packaged Goods survey report, which garnered hundreds of responses, showed maturation of AI within the industry as companies move AI projects from pilot to production in all areas. Highlights of this yearâ€™s report include: 91% of respondents said their companies are either actively using or assessing AI. 90% said theyâ€™d build on the success of current projects by increasing their AI budgets in 2026. 89% reported AI is helping to increase annual revenue, while 95% said it is helping decrease annual costs. 79% said open-source models and software were moderately to extremely important to their AI strategy. 47% said their companies are either using or assessing agentic AI in their operations. Read more below on some of the reportâ€™s key findings. Open Source Opens Opportunities Open source has quickly become the foundation of many retail AI systems, giving teams the flexibility to adapt models to their data and use cases while maintaining strong governance. Open, interoperable ecosystems also make it easier to plug AI into existing tools and workflows, helping retailers rapidly scale innovation. â€œMost retailers first started experimenting with AI using proprietary AI vendors,â€ said Jason Goldberg, chief commerce strategy officer of Publicis Groupe. â€œThey had the models, but they didnâ€™t own the keys to their own kingdom. Open source flips that script, allowing retailers to leverage their proprietary data, avoid vendor lock-in and benefit from open-source community innovation.â€ AI Unlocks Significant Business Impact With 91% of respondents saying their companies are either actively using or assessing AI, the competitive question in retail and CPG has shifted from whether or not to invest in AI, to how to most effectively deploy and scale AI. Across the industry, the business impact of AI has been tangible and significant. When asked how AI has improved their business, 54% cited improved employee productivity; 52% said AI has helped to create operational efficiencies; and 41% reported improved customer service. As stated above, 89% of respondents said AI has helped to increase revenue. For many companies, that increase has been significant, with 30% stating revenue has increased by more than 10%. The story is the same for AIâ€™s role in helping to decrease annual costs, with 95% agreeing AI has reduced costs and 37% saying costs have been reduced by more than 10%. â€œWhat executives should be focused on is not green-lighting vanity projects at the expense of high-ROI wins,â€ said Chris Walton, co-CEO of Omni Talk. â€œThe retailers who will succeed will start with boring use cases that solve specific P&L problems, prove the value, then scale.â€ AI investment, including infrastructure, hiring AI experts and software, will increase next year, according to nine out of 10 survey respondents. And half of respondents said the increase could be significant, with budgets increasing 10% or more year over year. Agentic AI Makes Big Debut in Retail The retail and CPG industry is piloting AI agents across lines of business. Overall, 47% of survey respondents said theyâ€™re using or assessing agentic AI â€” with 20% saying AI agents are already active in their organizations and another 21% reporting agents are coming within the next year. â€œThe truly disruptive impact of agentic AI will hit retail supply chains and operations first, such as autonomous agents handling real-time inventory rebalancing, dynamic pricing and vendor negotiations at scale, because thatâ€™s where the ROI is measurable,â€ said Walton. Survey respondents identified three clear goals for agentic AI in retail and CPG: Increased process speed and efficiency, per 57% of respondents. Enhanced customer experience and personalization, per 40%. Improved decision-making with real-time data, per 40%. Broadly speaking, agentic AI will be spread across three operational lines: internal operations, employee and customer support, and customer engagement. For instance, in customer engagement, agents go beyond analytics and act on insights in real time, adjust messages, recommend products and guide purchase decisions based on individual customer contexts. AI Providing Resilience to the Supply Chain Retail and CPG have faced intense supply chain challenges this decade, and those challenges are only growing more complex. Sixty-four percent of respondents in this yearâ€™s survey reported increased challenges in the supply chain year over year, such as geopolitical instability, labor constraints, evolving consumer expectations for speed and transparency, and regulatory complexity across global operations. â€œAI lets retailers optimize inventory at the store and customer level rather than at a regional level,â€ said Goldberg. â€œAI allows retailers to incorporate many more factors in their demand forecasts, and much more accurately predict and avoid out of stocks, by much more accurately matching supply to demand.â€ The industry is turning to AI to streamline operations and solve complexity. The top pressure valve is using AI for supply chain operational efficiency and throughput, according to 51% of respondents. Meeting customer expectations was next on the list at 45%, and solving for traceability and transparency was third, per 38%. Physical AI is gaining ground in the industry, with 17% of respondents using or evaluating the technology. â€œThe real transformation will come from AI that makes existing physical infrastructure smarter,â€ said Walton. â€œMy favorite example is in-store robotics. Through them, you get better pricing, better inventory, management and better presentation quality.â€ The early movers demonstrate that, when integrated thoughtfully, physical AI systems deliver more than task automation, enhancing flexibility and throughput in response to workforce pressures and rising logistical complexity. Download the â€œ State of AI in Retail and CPG: 2026 Trends â€ report for in-depth results and insights. Explore NVIDIAâ€™s AI solutions and enterprise-level AI platforms for retail . Categories: Generative AI | Robotics Tags: Agentic AI | Artificial Intelligence | Open Source | Physical AI | Retail]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 07 Jan 2026 14:00:06 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-in-retail-cpg-survey-2026/</guid>
    </item>
    <item>
      <title>NVIDIA Brings GeForce RTX Gaming to More Devices With New GeForce NOW Apps for Linux PC and Amazon Fire TV</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-ces-2026/</link>
      <description><![CDATA[Announced at the CES trade show running this week in Las Vegas, NVIDIA is bringing more devices, more games and more ways to play to its GeForce NOW cloud gaming service. Powered by GeForce RTX 5080-class performance on the NVIDIA Blackwell RTX platform, GeForce NOW Ultimate continues to raise the bar for PC gamers streaming from the cloud. GeForce RTX 5080-powered servers are live globally for Ultimate members, delivering up to 5K resolution 120 frames-per-second (fps) streaming and up to 360 fps at 1080p with NVIDIA Reflex technology support for ultralow-latency, competitive play. Cinematic-Quality Streaming mode enhances image clarity and text sharpness for visually rich single-player adventures on nearly any screen. New this year, GeForce NOW is expanding that performance to more platforms than ever, headlined by a native Linux PC app and a new app for Amazon Fire TV sticks. Flight-simulation fans are also getting flight controls support, and members everywhere gain faster access to more games thanks to new single sign-on integrations and upcoming AAA titles joining the cloud. Here Come the Platforms Linux PCs and Amazon Fire TV sticks are joining the GeForce NOW native app family, unlocking new ways to play in the cloud across desktops and living rooms. These new apps build on GeForce NOWâ€™s existing support for Windows PCs, macOS, Chromebooks, mobile devices, smart TVs, virtual-reality devices and handhelds, all tapping into the same GeForce RTX 5080-class performance wherever members log in. Turn your Linux PC into an RTX gaming rig. A new native GeForce NOW app for Linux PCs, supported with Ubuntu 24.04 and later distributions, answers one of the top requests from the PC gaming community. Linux users can transform their compatible systems into GeForce RTX-powered gaming rigs, streaming supported PC titles from the cloud at up to 5K and 120 fps or 1080p 360 fps. With rendering handled in the cloud, high-end PC gaming is possible on Linux operating systems, breathing new life into older devices. Members can enjoy ray tracing, NVIDIA DLSS 4 and other RTX technologies without needing a local high-performance GPU. The app is designed to bring a seamless, native experience that fits naturally into Linux desktop workflows while giving access to the expansive GeForce NOW library, turning everyday Linux devices into RTX gaming powerhouses. The app is expected to enter a beta release early this year. Game on in the living room. A new native GeForce NOW app for select Amazon Fire TV sticks â€” starting with the Fire TV Stick 4K Plus (2nd Gen) and Fire TV Stick 4K Max (2nd Gen) â€” can bring RTX-powered PC gaming to another big screen in the home. Members can stream their compatible PC game libraries directly to Fire TV-connected displays to turn a compact streaming stick into a powerful cloud gaming rig. With support for gamepads and GeForce NOWâ€™s familiar interface, Fire TV users can jump into their favorite supported games without a console or gaming PC attached to the TV. This builds on existing TV support and helps make GeForce NOW the easiest way to bring high-performance PC gaming into the living room. The app is expected to be available in countries where compatible Amazon Fire TV sticks and GeForce NOW are offered and will be launching early this year. Take Flight No fight, just flight in the cloud. GeForce NOW turns more devices into powerful cloud gaming rigs, and CES this year brings another of the communityâ€™s most-requested additions. Simulation fans are getting a major upgrade with flight controls support on GeForce NOW. Popular flight sticks and throttle systems from leading brands such as Thrustmaster and Logitech can be used as affixed hands-on throttle-and-stick desktop units or as separately mounted stick-and-throttle setups for custom cockpits. Combined with RTX 5080 performance, ultralow-latency streaming and NVIDIA Reflex in supported titles, flight controls let virtual pilots experience greater precision and deeper immersion in their favorite flight- and space-simulation games â€” including Microsoft Flight Simulator 2024, Elite Dangerous and War Thunder. Members can build out detailed simulation setups at home while streaming the heavy lifting from the cloud when it launches early this year.. Blockbusters in the Cloud The GeForce NOW catalog includes thousands of supported games from top PC stores like Steam, Epic Games Store, Xbox and others, with more joining every week. Backed by RTX 5080-class performance, members can stream everything from competitive shooters to expansive role-playing games with high frame rates, advanced graphics features and low latency.â€‹ New AAA titles such as IO Interactiveâ€™s 007 First Light , Capcomâ€™s Resident Evil Requiem, Pearl Abyssâ€™ Crimson Desert, and Gaijin Entertainmentâ€™s Active Matter are coming to GeForce NOW when they launch on PC, adding to an already robust lineup of new releases and fan favorites. License to stream. 007 First Light drops players into a modern James Bond origin story filled with stealth, espionage and cinematic action. Resident Evil Requiem continues the iconic survival-horror series with a new protagonist facing terrifying threats in a chilling new setting. Crimson Desert blends open-world exploration, cinematic storytelling and intense combat in a richly detailed fantasy world. Active Matter from Gaijin is a realistic military shooter where players join dangerous raids for loot or intense player vs. player battles set in a fractured multiverse. Members can look forward to seeing these and other upcoming hits arrive on the service, with updates shared regularly on GFN Thursdays. One Login, Many Worlds Just sign in once. The rest is game history. GeForce NOW is also making it faster to jump into gaming with new account and platform integrations. Recent updates introduced Battle.net automatic sign-in, letting members connect their accounts and access supported titles more quickly. That seamless experience is expanding to additional game stores, with Gaijin.net set to soon support automatic sign-in on GeForce NOW early this year. Members will be able to authenticate once and jump into War Thunder and other titles with fewer steps. Learn more about the latest NVIDIA-powered innovations at CES , running through Friday, Jan. 9. See notice regarding software product information. Categories: Gaming Tags: CES 2026 | Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:51 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-ces-2026/</guid>
    </item>
    <item>
      <title>NVIDIA RTX Accelerates 4K AI Video Generation on PC With LTX-2 and ComfyUI Upgrades</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</link>
      <description><![CDATA[2025 marked a breakout year for AI development on PC. PC-class small language models (SLMs) improved accuracy by nearly 2x over 2024, dramatically closing the gap with frontier cloud-based large language models (LLMs). AI PC developer tools including Ollama, ComfyUI, llama.cpp and Unsloth have matured, their popularity has doubled year over year and the number of users downloading PC-class models grew tenfold from 2024. These developments are paving the way for generative AI to gain widespread adoption among everyday PC creators, gamers and productivity users this year. At CES this week, NVIDIA is announcing announcing a wave of AI upgrades for GeForce RTX, NVIDIA RTX PRO and NVIDIA DGX Spark devices that unlock the performance and memory needed for developers to deploy generative AI on PC, including: Up to 3x performance and 60% reduction in VRAM for video and image generative AI via PyTorch-CUDA optimizations and native NVFP4/FP8 precision support in ComfyUI . RTX Video Super Resolution integration in ComfyUI, accelerating 4K video generation. NVIDIA NVFP8 optimizations for the open weights release of Lightricksâ€™ state-of-the-art LTX-2 audio-video generation model . A new video generation pipeline for generating 4K AI video using a 3D scene in Blender to precisely control outputs. Up to 35% faster inference performance for SLMs via Ollama and llama.cpp. RTX acceleration for Nexa.ai â€™s Hyperlink new video search capability. These advancements will allow users to seamlessly run advanced video, image and language AI workflows with the privacy, security and low latency offered by local RTX AI PCs. Generate Videos 3x Faster and in 4K on RTX PCs Generative AI can make amazing videos, but online tools can be difficult to control with just prompts. And trying to generate 4K videos is near impossible, as most models are too large to fit on PC VRAM. Today, NVIDIA is introducing an RTX-powered video generation pipeline that enables artists to gain accurate control over their generations while generating videos 3x faster and upscaling them to 4K â€” only using a fraction of the VRAM. This video pipeline allows emerging artists to create a storyboard, turn it into photorealistic keyframes and then turn these keyframes into a high-quality, 4K video. The pipeline is split into three blueprints that artists can mix and match or modify to their needs: A 3D object generator that creates assets for scenes. A 3D-guided image generator that allows users to set their scene in Blender and generate photorealistic keyframes from it. A video generator that follows a userâ€™s start and end key frames to animate their video, and uses NVIDIA RTX Video technology to upscale it to 4K This pipeline is possible by the groundbreaking release of the new LTX-2 model from Lightricks, available for download today. A major milestone for local AI video creation, LTX-2 delivers results that stand toe-to-toe with leading cloud-based models while generating up to 20 seconds of 4K video with impressive visual fidelity. The model features built-in audio, multi-keyframe support and advanced conditioning capabilities enhanced with controllability low-rank adaptations â€” giving creators cinematic-level quality and control without relying on cloud dependencies. Under the hood, the pipeline is powered by ComfyUI. Over the past few months, NVIDIA has worked closely with ComfyUI to optimize performance by 40% on NVIDIA GPUs, and the latest update adds support for the NVFP4 and NVFP8 data formats. All combined, performance is 3x faster and VRAM is reduced by 60% with RTX 50 Seriesâ€™ NVFP4 format, and performance is 2x faster and VRAM is reduced by 40% with NVFP8. NVFP4 and NVFP8 checkpoints are now available for some of the top models directly in ComfyUI. These models include LTX-2 from Lightricks, FLUX.1 and FLUX.2 from Black Forest Labs, and Qwen-Image and Z-Image from Alibaba. Download them directly in ComfyUI, with additional model support coming soon. Once a video clip is generated, videos are upscaled to 4K in just seconds using the new RTX Video node in ComfyUI. This upscaler works in real time, sharpens edges and cleans up compression artifacts for a clear final image. RTX Video will be available in ComfyUI next month. To help users push beyond the limits of GPU memory, NVIDIA has collaborated with ComfyUI to improve its memory offload feature, known as weight streaming. With weight streaming enabled, ComfyUI can use system RAM when it runs out of VRAM, enabling larger models and more complex multistage node graphs on mid-range RTX GPUs. The video generation workflow will be available for download next month, with the newly released open weights of the LTX-2 Video Model and ComfyUI RTX updates available now. A New Way to Search PC Files and Videos File searching on PCs has been the same for decades. It still mostly relies on file names and spotty metadata, which makes tracking down that one document from last year way harder than it should be. Hyperlink â€” Nexa.aiâ€™s local search agent â€” turns RTX PCs into a searchable knowledge base that can answer questions in natural language with inline citations. It can scan and index documents, slides, PDFs and images, so searches can be driven by ideas and content instead of file name guesswork. All data is processed locally and stays on the userâ€™s PC for privacy and security. Plus, itâ€™s RTX-accelerated, taking 30 seconds per gigabyte to index text and image files and three seconds for a response on a RTX 5090 GPU, compared with an hour per gigabyte to index files and 90 seconds for a response on CPUs. At CES, Nexa.ai is unveiling a new beta version of Hyperlink that adds support for video content, enabling users to search through their videos for objects, actions and speech. This is ideal for users ranging from video artists looking for B-roll to gamers who want to find that time they won a battle royale match to share with their friends. For those interested in trying the Hyperlink private beta, sign up for access on this webpage . Access will roll out starting this month. Small Language Models Get 35% Faster NVIDIA has collaborated with the openâ€‘source community to deliver major performance gains for SLMs on RTX GPUs and the NVIDIA DGX Spark desktop supercomputer using Llama.cpp and Ollama. The latest changes are especially beneficial for mixture-of-experts models, including the new NVIDIA Nemotron 3 family of open models . SLM inference performance has improved by 35% and 30% for llama.cpp and Ollama, respectively, over the past four months. These updates are available now, and a quality-of-life upgrade for llama.cpp also speeds up LLM loading times. These speedups will be available in the next update of LM Studio, and will be coming soon to agentic apps like the new MSI AI Robot app. The MSI AI Robot app, which also takes advantage of the Llama.cpp optimizations, lets users control their MSI device settings and will incorporate the latest updates in an upcoming release. NVIDIA Broadcast 2.1 Brings Virtual Key Light to More PC Users The NVIDIA Broadcast app improves the quality of a userâ€™s PC microphone and webcam with AI effects, ideal for livestreaming and video conferencing. Version 2.1 updates the Virtual Key Light effect to improve performance â€” making it available to RTX 3060 desktop GPUs and higher â€” handle more lighting conditions, offer broader color temperature control and use an updated HDRi base map for a twoâ€‘keyâ€‘light style often seen in professional streams. Download the NVIDIA Broadcast update today. Transform an At-Home Creative Studio Into an AI Powerhouse With DGX Spark As new and increasingly capable AI models arrive on PC each month, developer interest in more powerful and flexible local AI setups continues to grow. DGX Spark â€” a compact AI supercomputer that fits on usersâ€™ desks and pairs seamlessly with a primary desktop or laptop â€” enables experimenting, prototyping and running advanced AI workloads alongside an existing PC. Spark is ideal for those interested in testing out LLMs or prototyping agentic workflows, or for artists who want to generate assets in parallel to their workflow so that their main PC is still available for editing. At CES, NVIDIA is unveiling major AI performance updates to Spark, delivering up to 2.6x faster performance since it launched just under three months ago. New DGX Spark playbooks are also available, including one for speculative decoding and another to fine-tune models with two DGX Spark modules. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | CES 2026 | Creators | GeForce RTX | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:18 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</guid>
    </item>
    <item>
      <title>NVIDIA DLSS 4.5, Path Tracing and G-SYNC Pulsar Supercharge Gameplay With Enhanced Performance and Visuals</title>
      <link>https://blogs.nvidia.com/blog/dlss-path-tracing-g-sync-pulsar-ces-2026/</link>
      <description><![CDATA[At the CES trade show, NVIDIA today announced DLSS 4.5, which introduces Dynamic Multi Frame Generation, a new 6X Multi Frame Generation mode and a second-generation transformer model for DLSS Super Resolution, so gamers can experience the latest and greatest titles with enhanced performance and visuals. Over 250 games and apps now support NVIDIA DLSS 4 technology, with this yearâ€™s biggest titles adding support, including 007 First Light, Phantom Blade Zero, PRAGMATA and Resident Evil Requiem at launch. In addition, RTX Remix Logic debuted, expanding the capabilities of the Remix modding platform to enable modders to trigger dynamic graphics effects throughout a game based on real-time game events. Plus, NVIDIA ACE technology demonstrated in Total War: PHARAOH showcases how AI can assist players in navigating the complexities of the gameâ€™s many systems and mechanics. In PUBG: BATTLEGROUNDS , PUBG Ally powered by NVIDIA ACE adds long-term memory, evolving its intelligence and capabilities. And G-SYNC Pulsar monitors are available this week, delivering a tear-free experience together with a perceived 1,000Hz+ effective motion clarity and G-SYNC Ambient Adaptive Technology â€” all setting a new gold standard for gamers. NVIDIA DLSS 4.5 Will Power 4K 240Hz Path-Traced Gaming NVIDIA DLSS 4.5 introduces Dynamic Multi Frame Generation and a new 6X Multi Frame Generation mode. DLSS 4.5 can generate up to five additional frames per traditionally rendered frame, dynamically boosting performance and enabling 240+ frames-per-second gaming with path tracing using GeForce RTX 50 Series GPUs. This delivers the smoothest gameplay experiences yet. Dynamic Multi Frame Generation and 6X Multi-Frame Generation are expected to be available in spring of this year. A second-generation transformer model for DLSS Super Resolution also arrives with NVIDIA DLSS 4.5, bringing state-of-the-art image quality to over 400 games and apps for all GeForce RTX GPUs. The second-generation transformer is available to try now via the NVIDIA App for all GeForce RTX GPUs. Learn more . Over 250 DLSS 4 Games and Apps Available Now DLSS 4 with Multi Frame Generation launched at CES last year with 75 games and apps supported. Now, more than 250 games and apps are supported, including 2025â€™s most-played titles, such as ARC Raiders, Battlefield 6 , Clair Obscur: Expedition 33 and Where Winds Meet. New and upgraded titles announced today that will support the latest GeForce RTX technologies include 007 First Light , Active Matter , DEFECT , Phantom Blade Zero , PRAGMATA , Resident Evil Requiem and Screamer . Learn more . Next-Generation G-SYNC Pulsar Gaming Monitors Available Now The launch of G-SYNC in 2013 revolutionized displays and gaming, introducing Variable Refresh Rate (VRR) technology that all major display manufacturers now incorporate. This meant gamers no longer had to experience screen-tearing glitches when optimizing for responsive gameplay. G-SYNC Pulsar displays mark the latest evolution of NVIDIAâ€™s pioneering VRR technology. Through the invention of variable frequency backlight strobing, they deliver effective motion clarity of over 1,000Hz, significantly increasing the clarity and visibility of content in motion so gamers can track targets with increased precision and maintain consistent smoothness in gameplay. In addition, new G-SYNC Ambient Adaptive Technology uses a built-in light sensor, letting users automatically tune color temperature and brightness for optimal viewing at any hour, day or night. Learn more . RTX Remix Logic Brings Dynamic Graphics Effects to Classic Games Many iconic PC games remain beloved for their unforgettable stories, characters and gameplay. However, as technology advances, their visuals can become dated, making it harder for gamers to immerse themselves in the titles. NVIDIA RTX Remix , a modding platform for RTX AI PCs built to reimagine the graphics of these timeless classics with cutting-edge path tracing, lets longtime fans relive their favorite adventures in stunning visual detail, while opening opportunities for a new wave of players. A new RTX Remix update â€” RTX Remix Logic â€” will be available later this month via the NVIDIA App. Remix Logic is a logic system for making RTX Remix mods visually reactive to the moment-to-moment, in-game action, equipping modders with 900+ configurable settings to trigger dynamic graphics effects based on a wide variety of in-game events. Historically, modifying a gameâ€™s graphics in response to real-time game events was restricted to those with source code or engine access. RTX Remix eliminates this barrier so modders can customize visuals across 165+ classic games without touching the original engine code. Learn more . NVIDIA ACE Powers New AI Teammates and Advisors Non-playable characters (NPCs) traditionally follow strict rules designed to provide scripted interactions with players. NVIDIA is expanding the NVIDIA ACE suite of AI technologies to turn conversational NPCs into autonomous game characters that use AI to perceive, plan and act like human players. Creative Assembly, creator of the award-winning Total War franchise, is experimenting with NVIDIA ACE in Total War: PHARAOH to power a new, dynamic AI advisor that assists players in learning the gameâ€™s many systems and mechanics. By processing the playerâ€™s prompts, current game state and data retrieved from the gameâ€™s complex database, the advisor delivers real-time, context-aware guidance that adapts to what the player is doing, while staying in-character and faithful to the gameâ€™s lore and time period. KRAFTON is adding long-term memory for PUBG Ally â€” an NVIDIA ACE-powered AI teammate in PUBG: BATTLEGROUNDS that allows players to issue commands and communicate plans of attack or other tactical maneuvers with each other. With long-term memory, the Ally can remember previous performances and gameplay interactions, and inject commentary into their responses that refers to past events. PUBG Ally will initially be released as part of a limited-time user test event in the first half of this year via PUBG: BATTLEGROUNDS Arcade for players using English, Korean or Chinese. GeForce NOW Expands RTX Cloud Gaming Across More Devices NVIDIA continues to push the limits of PC gaming accessibility with new GeForce NOW updates unveiled today at CES, delivering GeForce RTX 5080-class performance to an even broader range of devices. Gamers can now experience ultrahigh-fidelity RTX visuals through new native apps for Linux systems and Amazon Fire TV Sticks. This answers top community requests. New support for hands-on throttle-and-stick flight control peripherals provide immersion in simulation titles, enabling precise input and smooth streaming responsiveness for flight and space combat games. GeForce NOW also introduces streamlined single sign-on for Gaijin titles, minimizing setup time and getting players into their favorite experiences faster. GeForce NOWâ€™s ever-expanding library continues to grow with day-and-date cloud launches of major upcoming titles â€” including 007 First Light , Active Matter , Resident Evil Requiem and Crimson Desert â€” ensuring that gamers can stream the latest blockbusters the same day they arrive on PC, all powered by NVIDIA RTX technology. Learn more . RTX AI PCs Accelerate AI Video, Image and Text Generation At CES, NVIDIA announced a wave of AI upgrades for GeForce RTX GPUs and laptops that unlock the performance and memory needed for developers to deploy generative AI on PC, including: Up to 3x performance and 60% reduction in VRAM for video and image generative AI via PyTorch-CUDA optimizations and native NVFP4/FP8 precision support in ComfyUI. RTX Video Super Resolution integration in ComfyUI, accelerating 4K video generations. NVIDIA NVFP8 optimizations for the open weights release of Lightricksâ€™ state-of-the-art LTX-2 audio-video generation model. A blueprint for generating 4K AI video using a 3D scene in Blender to precisely control outputs. Up to 35% faster inference performance for SLMs via Ollama and llama.cpp. RTX acceleration for Nexa.ai â€™s Hyperlink new video search capability. These advancements will allow users to seamlessly run advanced video, image and language AI workflows with the privacy, security and low latency offered by local RTX AI PCs. Learn more . See notice regarding software product information. Categories: Gaming Tags: Artificial Intelligence | CES 2026 | Cloud Gaming | G-SYNC | Game Development | Gaming | GeForce | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:09 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/dlss-path-tracing-g-sync-pulsar-ces-2026/</guid>
    </item>
    <item>
      <title>NVIDIA Unveils New Open Models, Data and Tools to Advance AI Across Every Industry</title>
      <link>https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</link>
      <description><![CDATA[Expanding the open model universe, NVIDIA today released new open models, data and tools to advance AI across every industry. These models â€” spanning the NVIDIA Nemotron family for agentic AI, the NVIDIA Cosmos platform for physical AI , the new NVIDIA Alpamayo family for autonomous vehicle development, NVIDIA Isaac GR00T for robotics and NVIDIA Clara for biomedical â€” will empower companies with the tools to develop real-world AI systems. NVIDIA contributes open-source training frameworks and one of the worldâ€™s largest collections of open multimodal data, including 10 trillion language training tokens, 500,000 robotics trajectories, 455,000 protein structures and 100 terabytes of vehicle sensor data. This is an unprecedented scale of diverse open resources to accelerate innovation in language, robots, scientific research and autonomous vehicles. Leading technology companies â€” including Bosch, CodeRabbit, CrowdStrike, Cohesity, Fortinet, Franka Robotics, Humanoid, Palantir, Salesforce, ServiceNow, Hitachi and Uber â€” are adopting and building on NVIDIAâ€™s open model technologies. NVIDIA Nemotron Brings Speech, Multimodal Intelligence and Safety to AI Agents Building on the recently released NVIDIA Nemotron 3 family of open models and data, NVIDIA is releasing Nemotron models for speech, multimodal retrieval-augmented generation (RAG) and safety. Nemotron Speech comprises leaderboard-topping open models, including a new ASR model , that deliver real-time, low-latency speech recognition for live captions and speech AI applications. Daily and Modal benchmarks show that the model delivers 10x faster performance than other models in its class. Nemotron RAG comprises new embed and rerank vision language models (VLMs) that provide highly accurate multilingual and multimodal data insights to enhance document search and information retrieval. Nemotron Safety models, which strengthen the safety and trustworthiness of AI applications, now include the Llama Nemotron Content Safety model, featuring expanded language support, and Nemotron PII , which detects sensitive data with high accuracy. Bosch is adopting Nemotron Speech to enable drivers to interact with their vehicles. ServiceNow trains its Apriel model family on open datasets, including Nemotron for cost-efficient multimodal performance. Cadence and IBM are piloting NVIDIA Nemotron RAG models to improve search and reasoning across complex technical documents. CrowdStrike, Cohesity and Fortinet are adopting NVIDIA Nemotron Safety models to strengthen the trustworthiness of their AI applications. Palantir is integrating Nemotron models into its Ontology framework to build a first-of-its-kind, integrated technology stack for specialized AI agents. CodeRabbit is using Nemotron models to power and scale its AI code reviews, improving speed and cost efficiency while maintaining high review accuracy. NVIDIA is also releasing open-source datasets, training resources and blueprints to developers, including the dataset and training code for the Llama Embed Nemotron 8B model, featured on the MMTEB leaderboard . This is in addition to the updated LLM Router that shows developers how to automatically direct AI requests to the best model for the job, and the dataset used to build the new Nemotron Speech ASR model. New Models for Every Type of Physical AI and Robot Developing physical AI for robots and autonomous systems requires large, diverse datasets and models that can perceive, reason and act in complex, real-world environments. On Hugging Face, robotics is the fastest-growing segment, with NVIDIAâ€™s open robotics models and datasets leading the platformâ€™s downloads . NVIDIA is releasing NVIDIA Cosmos open world foundation models that bring humanlike reasoning and world generation to accelerate physical AI development and validation. Cosmos Reason 2 is a new, leaderboard-topping reasoning VLM that helps robots and AI agents see, understand and interact with higher accuracy in the physical world. Cosmos Transfer 2.5 and Cosmos Predict 2.5 are leading models that generate large-scale synthetic videos across diverse environments and conditions. NVIDIA has also released open models and blueprints for each physical AI embodiment, built on Cosmos: Isaac GR00T N1.6 is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding. The NVIDIA Blueprint for video search and summarization , part of the NVIDIA Metropolis platform, is a reference workflow for building vision AI agents that can analyze large volumes of recorded and live video to improve operational efficiency and public safety. Salesforce , Milestone , Hitachi, Uber, VAST Data and Encord are using Cosmos Reason for traffic and workplace productivity AI agents. Franka Robotics, Humanoid and NEURA Robotics are using Isaac GR00T to simulate, train and validate new behaviors for robots before scaling to production. NVIDIA Alpamayo for Reasoning-Based Autonomous Vehicles Developing safe, scalable autonomous driving depends on AI that can perceive, reason and act in complex real-world environments and scenarios, with development workflows that support rapid training, testing and improvement at scale. NVIDIA is releasing NVIDIA Alpamayo, a new family of open models, simulation tools and large datasets to advance reasoning-based autonomous vehicle development. It includes: Alpamayo 1 , the first open, large-scale reasoning VLA model for autonomous vehicles (AVs) that enables vehicles to understand their surroundings, as well as explain their actions.â€‹ AlpaSim , an open-source simulation framework that enables closed-loop training and evaluation of reasoning-based AV models across diverse environments and edge cases. NVIDIA is also releasing Physical AI Open Datasets , including over 1,700 hours of driving data collected across the widest range of geographies and conditions, covering rare and complex real-world edge cases essential for advancing reasoning architectures. NVIDIA Clara for Healthcare and Life Sciences To lower costs and deliver treatments faster, NVIDIA is launching new Clara AI models that bridge the gap between digital discovery and real-world medicine. Helping researchers design treatments that are safer, more effective and easier to produce, these models include: La-Proteina enables the design of large, atom-level-precise proteins for research and drug candidate development, giving scientists new tools to study diseases previously considered untreatable. ReaSyn v2 ensures AI-designed drugs are practical to synthesize by incorporating a manufacturing blueprint into the discovery process. KERMT provides high-accuracy, computational safety testing early in development by predicting how a potential drug will interact with the human body. RNAPro unlocks the potential of personalized medicine by predicting the complex 3D shapes of RNA molecules. In addition, an NVIDIA dataset of 455,000 synthetic protein structures helps AI researchers build more accurate AI models. Get Started With NVIDIA Open Models and Technologies NVIDIA open models, data and frameworks are now available on GitHub and Hugging Face and from a range of cloud, inference and AI infrastructure platforms, as well as build.nvidia.com , giving developers flexible access to supporting resources. Many of these models are also available as NVIDIA NIM microservices for secure, scalable deployment on any NVIDIA-accelerated infrastructure, from the edge to the cloud. Learn more by watching NVIDIA Live at CES . Categories: Driving | Generative AI | Robotics Tags: Agentic AI | CES 2026 | Cosmos | Healthcare and Life Sciences | Nemotron | NVIDIA Clara | NVIDIA NIM | Open Source | Physical AI]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 21:50:50 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</guid>
    </item>
    <item>
      <title>Marine Biological Laboratory Explores Human Memory With AI and Virtual Reality</title>
      <link>https://blogs.nvidia.com/blog/mbl-human-memory-ai-vr-rtx/</link>
      <description><![CDATA[The works of Plato state that when humans have an experience, some level of change occurs in their brain, which is powered by memory â€” specifically long-term memory. This change is what Andre Fenton, professor of neural science at New York University, and Abhishek Kumar, assistant professor of cell and regenerative biology at the University of Wisconsinâ€“Madison, are studying at the Marine Biological Laboratory (MBL) in Woods Hole, Massachusetts. â€œMy lifeâ€™s work is to understand how minds operate, and especially to understand memory â€” not merely as a trace of the past in the brain but as an estimate of the future that the brain is afforded,â€ Fenton said. The researchers have upleveled their project by harnessing NVIDIA RTX GPUs and HP Z Workstations to visualize massive datasets and by integrating custom AI tools and syGlass , a virtual-reality (VR) platform for scientific exploration. This project is additionally supported by grants from the National Institute of Mental Health and the Chan Zuckerberg Initiative. A Neural Forest Uncovered Memory is the job of the brainâ€™s hippocampus. This C-shaped structure, resembling a seahorse, is the main focus of the MBL research group. Fenton describes the cells within the hippocampus as a forest, where billions of neurons look like tiny tree trunks and the lines coming off the trunks look like leaves. Projection image of neuronal cell nuclei (left) and dendrites (right) or branched extensions of a nerve cell. Images were acquired by Matthew Parent and Daryl Watkins. The team is studying a small portion of these â€œleavesâ€ â€” representing protein markers: an incredibly tedious task due to their length, at about a micrometer each. A researcher must search through the forest of brain cells to find the correct protein markers, which make up only about 1% of all protein markers in the hippocampus. The researchers were looking to ease the process of studying these proteins and what their varying structures may reveal about memory encoding. Collecting and analyzing enough 3D volumetric data on protein markers was a bottleneck within the project until NVIDIA and HP technologies were introduced into the workflow. â€œThis is a massive computational challenge, and the HP and NVIDIA technologies have enabled us to do the first step: capture, check and store the 3D image data,â€ Fenton said. Using these technologies, the MBL researchers captured 10 terabytes of volumetric data and then performed human visual-quality inspections. Understanding Memory Could Prevent Neurological Diseases The teamâ€™s ultimate goal of discovering the function of memory at a molecular level can boost research into the root causes of brain diseases tied to neurocognition, such as Alzheimerâ€™s and dementia. â€œPeople donâ€™t normally think of memory as part of their mental health, but almost all mental dysfunction depends on what your brain stores â€” the beliefs, the anticipations, the anxieties that you have and the things that you expect,â€ said Fenton. â€œThese are all different aspects of what happens when you have a memory, so almost all neuropsychiatric illnesses and manipulation depend on this understanding.â€ As a step toward solving these large-scale problems, the researchers are looking at how memory is affected when proteins go to incorrect locations in the hippocampus. The team is also examining the correlation between the structure and function of brain cells through high-resolution 3D images curated and stored using syGlass on the HP Z high performance workstation powered and supported with multiple NVIDIA RTX GPUs. â€œIf we can understand how something is built, then if thereâ€™s a problem, we can dissect that and get to the bottom of it,â€ said Kumar. â€œThatâ€™s what weâ€™re trying to do: understand how we retain memory, so if a problem arises, we know how to fix it.â€ Enabling Virtual Reality and Student Exploration The use of syGlass on the HP Z6 desktop workstation, running on NVIDIA RTX GPUs, turned the researchersâ€™ endeavor from a time-consuming operation into an interactive scientific exploration â€” ideal for high-school-student engagement. â€œThe HP-NVDIA-syGlass system lets us innovate by engaging three high-school interns,â€ said Kumar. â€œThey had an abstract interest in our science, and we recognized that the syGlass virtual experience might enthrall them. We were right.â€ The researchers brought these three curious students into their lab this summer to analyze the memory proteins using the VR headsets, which allowed for 3D visuals of the data. Their task was to find the specific proteins that were memory-related and label them as such. While this may sound like a simple task, the interns had to sift through a sea of billions of neurons to find only a few thousand protein markers that were relevant to the research. High school intern using the syGlass VR headset to identify protein markers. Image taken by Andre Fenton. Due to the success of this pilot program, the team is now looking to expand high-school research opportunities for the project. â€œWhy leave it at three students?â€ said Fenton. â€œNext year, it could be 10 at multiple locations helping us learn about brains while they learn about brains.â€ Learn more about NVIDIA-accelerated academic research . Categories: Generative AI | Research | Workstation Tags: Education | NVIDIA RTX | Science | Scientific Visualization | Virtual Reality]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 22 Dec 2025 16:00:10 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/mbl-human-memory-ai-vr-rtx/</guid>
    </item>
    <item>
      <title>Now Generally Available, NVIDIA RTX PRO 5000 72GB Blackwell GPU Expands Memory Options for Desktop Agentic AI</title>
      <link>https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</link>
      <description><![CDATA[Top-notch options for AI at the desktops of developers, engineers and designers are expanding. The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available, bringing robust agentic and generative AI capabilities powered by the NVIDIA Blackwell architecture to more desktops and professionals across the world. The new GPU configuration offers AI developers, data scientists and creative professionals the hardware for modern, memory-hungry workflows â€” and arrives at a time when demand for NVIDIA Blackwell-class compute is higher than ever. With the flexibility to choose between this 72GB variant and the existing NVIDIA RTX PRO 5000 48GB model, AI developers can right-size their systems for a wider range of budgets and project requirements. The NVIDIA RTX PRO 5000 Blackwell GPU. Fueling the Next Generation of AI Development As generative AI evolves into complex, multimodal agentic AI , more demand is placed on the hardware required to develop and deploy these technologies. One defining challenge of AI development is memory capacity. Running cutting-edge AI workflows â€” especially those involving large language models ( LLMs ) and AI agents â€” places significant stress on GPU memory, particularly as models, context windows and multimodal pipelines grow in size and complexity. Agentic AI systems involve chains of tools, retrieval-augmented generation ( RAG ), and multimodal understanding. These systems often need to keep multiple AI models, data sources and code formats active simultaneously within the GPUâ€™s memory. Built on NVIDIA Blackwell â€” which delivers high throughput for AI, neural rendering and simulation with multi-workload scheduling and other architectural innovations â€” the RTX PRO 5000 72GB helps solve this bottleneck, offering 2,142 TOPS of AI performance. Plus, with 72GB of ultrafast GDDR7 memory â€” a 50% increase over the 48GB model â€” developers can train, fine-tune and prototype larger models locally. This enables users to maintain data privacy, low latency and cost efficiency, allowing teams to serve models directly from their workstations rather than relying on data-center-scale infrastructure for every AI task. Performance Enhancements For local AI development, raw compute is only half the battle â€” memory capacity determines what users can run, and throughput determines how fast it runs. In industry-standard benchmarks for generative AI, the RTX PRO 5000 72GB offers 3.5x the performance of prior-generation NVIDIA hardware for image generation, and 2x the performance of prior-generation hardware for text generation. In creative workflows, time saved in rendering is time gained for iteration. Across path-tracing engines like Arnold, Chaos V-Ray and Blender, as well as real-time GPU renderers like D5 Render and Redshift, the RTX PRO 5000 72GB slashes render times by up to 4.7x. And for computer-aided engineering and product design, the RTX PRO 5000 72GB offers more than 2x graphics performance. InfinitForm Optimizes Generative AI-Powered Design With RTX PRO 5000 InfinitForm, a provider of generative AI software for engineering design and a member of the NVIDIA Inception program for startups, is an early adopter of the RTX PRO 5000 72GB Blackwell GPU. The company is using the GPU to optimize its software with enhanced performance and speed, enabling advanced simulations to streamline processes for computer-aided design and manufacturing. â€œInfinitForm is thrilled to evaluate its CUDA-accelerated generative AI design optimization software on NVIDIA RTX PRO 5000 72GB to help customers like Yamaha Motor and NASA accelerate innovation and optimize products for performance and manufacturability,â€ said Michael Bogomolny, founder and CEO of InfinitForm. Versatile Media Accelerates Virtual Production Workflows With RTX PRO 5000 For creative professionals, such as those at Versatile Media â€” a global media company specializing in virtual production â€” the RTX PRO 5000 can significantly enhance real-time rendering performance for large-scale scenes and complex assets, delivering a meaningful leap in efficiency. As 3D pipelines now often integrate AI denoisers, generative tools and real-time physics, the new GPUâ€™s 72GB memory capacity allows for the manipulation of massive 3D scenes and asset libraries without slowing down the creative flow. An early adopter of the new GPU, Versatile Media plans to use it to design a series of complex, high-resolution real-time rendering scenarios that take advantage of the GPUâ€™s memory capacity. â€œFor film-grade virtual production, memory capacity directly translates into creative freedom,â€ said Eddy Shen, general manager of the production center at Versatile Media. â€œWith 72GB of GPU memory, the RTX PRO 5000 enables us to iterate with higher-resolution scenes and more complex lighting in real time without compromising performance.â€ Availability and Ecosystem Support The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available from partners including Ingram Micro, Leadtek, Unisplendour and xFusion, providing manufacturers and systems integrators with a powerful new option to anchor AI-ready workstations. Broader availability through global system builders will start early next year. As industries race to integrate AI into every facet of operation â€” from generative design to coding copilots â€” RTX PRO 5000 72GB is equipped to meet the moment. Learn more about the NVIDIA RTX PRO 5000 GPU family and join NVIDIA at SIGGRAPH Asia to discover how graphics and simulation innovations come together to drive creativity, industrial digitalization, robotics, and physical and spatial AI. Categories: Data Center | Generative AI | Pro Graphics Tags: Agentic AI | Artificial Intelligence | GPU | Hardware | NVIDIA RTX | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 18 Dec 2025 16:00:28 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</guid>
    </item>
    <item>
      <title>UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System</title>
      <link>https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</link>
      <description><![CDATA[The Hao AI Lab research team at the University of California San Diego â€” at the forefront of pioneering AI model innovation â€” recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference . Many LLM inference platforms in production today, such as NVIDIA Dynamo , use research concepts that originated in the Hao AI Lab, including DistServe . How Is Hao AI Lab Using the DGX B200? Members of the Hao AI Lab standing with the NVIDIA DGX B200 system. With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciencesâ€™ San Diego Supercomputer Center , the research opportunities are boundless. â€œDGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,â€ said Hao Zhang, assistant professor in the HalÄ±cÄ±oÄŸlu Data Science Institute and department of computer science and engineering at UC San Diego. â€œIt enables us to prototype and experiment much faster than using previous-generation hardware.â€ Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark. FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt â€” in just five seconds. The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system. Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including Tetris and Super Mario Bros . Users can test one model at a time or put two models up against each other to measure their performance. The illustrated workflow of Hao AI Labâ€™s Lmgame-Bench project. Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness. â€œOur current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,â€ said Junda Chen, a doctoral candidate in computer science at UC San Diego. How DistServe Influenced Disaggregated Serving Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests. The benefit of disaggregated inference lies in optimizing what DistServe calls â€œgoodputâ€ instead of â€œthroughputâ€ in the LLM-serving engine. Hereâ€™s the difference: Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another. While throughput measures the aggregate performance of the system, it doesnâ€™t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput. This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, â€œgoodputâ€: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience. DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model. How Can Developers Achieve Optimal Goodput? When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each tokenâ€™s future behavior based on past requestsâ€™ outcomes. This process is known as decode. https://blogs.nvidia.com/wp-content/uploads/2025/12/distserve.mp4 Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput. â€œPreviously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,â€ Chen said. â€œNow, if I split the jobs onto two different sets of GPUs â€” one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive â€” we can fundamentally eliminate the interference between the two jobs, making both jobs run faster. This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput. Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses. NVIDIA Dynamo â€” an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost â€” enables scaling disaggregated inference. In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation. Learn more about the NVIDIA DGX B200 system. Categories: Data Center | Generative AI | Research | Supercomputing Tags: Artificial Intelligence | Education | Inference | NVIDIA DGX | Open Source]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 17 Dec 2025 16:00:15 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</guid>
    </item>
    <item>
      <title>How to Fine-Tune an LLM on NVIDIA GPUs With Unsloth</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</link>
      <description><![CDATA[Modern workflows showcase the endless possibilities of generative and agentic AI on PCs. Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing oneâ€™s schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks. Thatâ€™s where fine-tuning comes in. Unsloth , one of the worldâ€™s most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. Itâ€™s optimized for efficient, low-memory training on NVIDIA GPUs â€” from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark , the worldâ€™s smallest AI supercomputer. Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning. Teaching AI New Tricks Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand. Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods: Parameter-efficient fine-tuning (such as LoRA or QLoRA) : How it works: Updates only a small portion of the model for faster, lower-cost training. Itâ€™s a smarter and efficient way to enhance a model without altering it drastically. Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied â€” including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior. Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs). Full fine-tuning : How it works: Updates all of the modelâ€™s parameters â€” useful for teaching the model to follow specific formats or styles. Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner. Requirements: Large dataset (1,000+ prompt-sample pairs). Reinforcement learning : How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference â€” and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unslothâ€™s Reinforcement Learning Guide for details. Target use case: Improving the accuracy of a model in a particular domain â€” such as law or medicine â€” or building autonomous agents that can orchestrate actions on a userâ€™s behalf. Requirements: A process that contains an action model, a reward model and an environment for the model to learn from. Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth. Fine-tuning requirements on Unsloth. Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently. Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training. Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unslothâ€™s ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers. The framework is built and optimized for NVIDIA hardware â€” from GeForce RTX laptops to RTX PRO workstations and DGX Spark â€” providing peak performance while reducing VRAM consumption. Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows. Check out some of these Unsloth guides: Fine-Tuning LLMs With NVIDIA RTX 50 Series GPUs and Unsloth Fine-Tuning LLMs With NVIDIA DGX Spark and Unsloth Learn how to install Unsloth on NVIDIA DGX Spark . Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform. For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below. Available Now: NVIDIA Nemotron 3 Family of Open Models The new Nemotron 3 family of open models â€” in Nano, Super, and Ultra sizes â€” built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications. Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. Itâ€™s optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers: Up to 60% fewer reasoning tokens, significantly reducing inference cost. A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks. Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026. NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth. Download Nemotron 3 Nano now from Hugging Face , or experiment with it through Llama.cpp and LM Studio. DGX Spark: A Compact AI Powerhouse DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC. Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally. For fine-tuning, DGX Spark enables: Larger model sizes. Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Sparkâ€™s unified memory. More advanced techniques. Full fine-tuning and reinforcement-learning-based workflows â€” which demand more memory and higher throughput â€” run significantly faster on DGX Spark. Local control without cloud queues. Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments. DGX Sparkâ€™s strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines. The table below shows performance for fine-tuning the Llama family of models on DGX Spark. Performance for fine-tuning Llama family of models on DGX Spark. As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark. Learn more about how DGX Spark enables intensive AI tasks . #ICYMI â€” The Latest Advancements in NVIDIA RTX AI PCs ðŸš€ FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%. âœ¨ Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs. ðŸ¤Mistral AI Unveils New Model Family Optimized for NVIDIA GPUs The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp. ðŸŽ¨Blender 5.0 Lands With HDR Color and Major Performance Gains The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | GeForce | NVIDIA RTX | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 15 Dec 2025 14:00:11 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</guid>
    </item>
    <item>
      <title>NVIDIA Awards up to $60,000 Research Fellowships to PhD Students</title>
      <link>https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</link>
      <description><![CDATA[For 25 years, the NVIDIA Graduate Fellowship Program has supported graduate students doing outstanding work relevant to NVIDIA technologies. Today, the program announced the latest awards of up to $60,000 each to 10 Ph.D. students involved in research that spans all areas of computing innovation. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding the fellowship year. Their work puts them at the forefront of accelerated computing â€” tackling projects in autonomous systems, computer architecture, computer graphics, deep learning, programming systems, robotics and security. The NVIDIA Graduate Fellowship Program is open to applicants worldwide. The 2026-2027 fellowship recipients are: Jiageng Mao , University of Southern California â€” Solving complex physical AI problems by using diverse priors from internet-scale data to enable robust, generalizable intelligence for embodied agents in the real world. Liwen Wu , University of California San Diego â€” Enriching realism and efficiency in physically based rendering with neural materials and neural rendering. Sizhe Chen , University of California, Berkeley â€” Securing AI in real-world applications, currently securing AI agents against prompt injection attacks with general and practical defenses that preserve the agentâ€™s utility. Yunfan Jiang , Stanford University â€” Developing scalable approaches to build generalist robots for everyday tasks through hybrid data sources spanning real-world whole-body manipulation, large-scale simulation and internet-scale multimodal supervision. Yijia Shao , Stanford University â€” Researching human-agent collaboration by developing AI agents that can communicate and coordinate with humans during task execution, and designing new human-agent interaction interfaces. Shangbin Feng , University of Washington â€” Advancing model collaboration: multiple machine learning models, trained on different data and by different people, collaborate, compose and complement each other for an open, decentralized and collaborative AI future. Shvetank Prakash , Harvard University â€” Advancing hardware architecture and systems design with AI agents built on new algorithms, curated datasets and agent-first infrastructure. Irene Wang , Georgia Institute of Technology â€” Developing a holistic codesign framework that integrates accelerator architecture, network topology and runtime scheduling to enable energy-efficient and sustainable AI training at scale. Chen Geng , Stanford University â€” Modeling 4D physical worlds with scalable data-driven algorithms and physics-inspired principles, advancing physically grounded 3D and 4D world models for robotics and scientific applications. Alexander Root , Stanford University â€” Designing programming systems for productive exploration of work-efficiency, compute and memory tradeoffs in irregular applications such as sparse array programming and path tracing. We also acknowledge the 2026-2027 fellowship finalists: Zizheng Guo , Peking University Peter Holderrieth , Massachusetts Institute of Technology Xianghui Xie , Max Planck Institute for Informatics Daniel Palenicek , Technical University of Darmstadt Categories: Generative AI | Research Tags: Artificial Intelligence | Education]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 17:00:44 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</guid>
    </item>
  </channel>
</rss>
