<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Sat, 06 Dec 2025 22:04:26 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>NVIDIA Awards up to $60,000 Research Fellowships to PhD Students</title>
      <link>https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</link>
      <description><![CDATA[For 25 years, the NVIDIA Graduate Fellowship Program has supported graduate students doing outstanding work relevant to NVIDIA technologies. Today, the program announced the latest awards of up to $60,000 each to 10 Ph.D. students involved in research that spans all areas of computing innovation. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding the fellowship year. Their work puts them at the forefront of accelerated computing â€” tackling projects in autonomous systems, computer architecture, computer graphics, deep learning, programming systems, robotics and security. The NVIDIA Graduate Fellowship Program is open to applicants worldwide. The 2026-2027 fellowship recipients are: Jiageng Mao , University of Southern California â€” Solving complex physical AI problems by using diverse priors from internet-scale data to enable robust, generalizable intelligence for embodied agents in the real world. Liwen Wu , University of California San Diego â€” Enriching realism and efficiency in physically based rendering with neural materials and neural rendering. Manya Bansal , Massachusetts Institute of Technology â€” Designing programming languages for modern accelerators that enable developers to write modular, reusable code without sacrificing the low-level control required for peak performance. Sizhe Chen , University of California, Berkeley â€” Securing AI in real-world applications, currently securing AI agents against prompt injection attacks with general and practical defenses that preserve the agentâ€™s utility. Yunfan Jiang , Stanford University â€” Developing scalable approaches to build generalist robots for everyday tasks through hybrid data sources spanning real-world whole-body manipulation, large-scale simulation and internet-scale multimodal supervision. Yijia Shao , Stanford University â€” Researching human-agent collaboration by developing AI agents that can communicate and coordinate with humans during task execution, and designing new human-agent interaction interfaces. Shangbin Feng , University of Washington â€” Advancing model collaboration: multiple machine learning models, trained on different data and by different people, collaborate, compose and complement each other for an open, decentralized and collaborative AI future. Shvetank Prakash , Harvard University â€” Advancing hardware architecture and systems design with AI agents built on new algorithms, curated datasets and agent-first infrastructure. Irene Wang , Georgia Institute of Technology â€” Developing a holistic codesign framework that integrates accelerator architecture, network topology and runtime scheduling to enable energy-efficient and sustainable AI training at scale. Chen Geng , Stanford University â€” Modeling 4D physical worlds with scalable data-driven algorithms and physics-inspired principles, advancing physically grounded 3D and 4D world models for robotics and scientific applications. We also acknowledge the 2026-2027 fellowship finalists: Zizheng Guo , Peking University Peter Holderrieth , Massachusetts Institute of Technology Xianghui Xie , Max Planck Institute for Informatics Alexander Root , Stanford University Daniel Palenicek , Technical University of Darmstadt Categories: Generative AI | Research Tags: Artificial Intelligence | Education]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 17:00:44 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</guid>
    </item>
    <item>
      <title>Robotsâ€™ Holiday Wishes Come True: NVIDIA Jetson Platform Offers High-Performance Edge AI at Festive Prices</title>
      <link>https://blogs.nvidia.com/blog/jetson-edge-ai-holiday-2025/</link>
      <description><![CDATA[Developers, researchers, hobbyists and students can take a byte out of holiday shopping this season as NVIDIA has unwrapped special discounts on the NVIDIA Jetson family of developer kits for edge AI and robotics â€” available through Sunday, Jan. 11. Whether tapping into the breakthrough capabilities of Jetson AGX Thor, the versatility of Jetson AGX Orin or the palm-sized power of the NVIDIA Jetson Orin Nano Super Developer Kit, anyone can deck out their bots with greater physical AI performance for lower costs: The Jetson AGX Thor Developer Kit â€” now 20% off â€” is designed for building humanoid robots, fleets of autonomous machines and multimodal physical AI agents. It delivers server-class compute and generative AI capabilities for the most challenging workloads in labs, factories and the field. The Jetson AGX Orin Developer Kit â€” now 50% off â€” powers advanced robots, autonomous machines and generative AI at the edge with 275 trillion operations per second (TOPS) of AI performance, ideal for delivery bots, smart vision capabilities and industrial automation. The Jetson Orin Nano Super Developer Kit â€” the worldâ€™s most affordable generative AI supercomputer â€” offers desktop-class AI in a palm-sized kit for exploring, building and deploying cutting-edge generative AI, vision and robotics. Read more below on how the NVIDIA Jetson platform presents the future of robotics. NVIDIA Jetson Orin Nano Super Serves as Brain of Self-Paddling Canoe Robotics enthusiast Dave Niewinski has built a selfâ€‘paddling canoe using the Jetson Orin Nano Super Developer Kit, letting boaters relax and glide on their rides. The robotic boat integrates two six-axis robotic arms mounted on a lightweight canoe frame, with paddle motion controlled through ROS software and AI algorithms. The Jetson Orin Nano Super delivers up to 67 INT8 TOPS of AI performance through its NVIDIA Ampere architecture GPU, 32 Tensor Cores and 1,024 CUDA cores, alongside a six-core Arm Cortexâ€‘A78AE CPU and 8GB LPDDR5 memory with 102 GB/s bandwidth. Its low power envelope of up to 25 watts enables sustained real-time inference and control in mobile, battery-powered applications. NVIDIA AGX Orin Dives Into Open Seas on Underwater AI System OptoScale, an AI aquaculture company based in Norway, has integrated the Jetson AGX Orin-powered MX13/23 platform from edge AI solutions provider Aetina to build an underwater AI sensing system that monitors fish health in massive open-sea pens housing thousands of fish. Mounted inside a submerged camera module, the Jetson AGX Orin processes high-resolution video streams directly at the edge and runs real-time vision models on the device. This allows the system to estimate biomass with exceptional accuracy and deliver continuous inference-based insights even in remote environments with limited connectivity. The Jetson AGX Orin delivers powerful GPU acceleration and high TOPS AI performance in a compact, energy-efficient module ideal for deployment anywhere. NVIDIA Jetson AGX Thor Powers Mobile Humanoid â€˜Dexâ€™ Las Vegas-based Richtech Robotics is developing Dex, a mobile humanoid robot for factory and warehouse environments designed to handle light-to medium-weight industrial tasks like machine operation, parts sorting, material handling and packaging. https://blogs.nvidia.com/wp-content/uploads/2025/12/richtech-robot-arm-20MB.mp4 Running on NVIDIA Jetson AGX Thor, Dex combines the mobility of an autonomous wheeled platform with the precision of dual-arm dexterity, allowing it to efficiently navigate environments and pick and place objects. It was trained with a mix of real-world and synthetic data generated from NVIDIA Isaac Sim . NVIDIA Jetson Thor modules enable real-time reasoning for physical AI, delivering up to 2,070 FP4 teraflops of AI compute and 128GB of memory with power configurable between 40-130 watts. With discounted pricing for a limited time and a full spectrum of performance options, the NVIDIA Jetson family gives anyone the tools to design, build and deploy the next generation of intelligent machines. Itâ€™s the ideal gift for robot lovers â€” and the robots in their lives. Shop NVIDIA Jetson now . Categories: Robotics Tags: Artificial Intelligence | Creators | Embedded Computing | Hardware | Industrial and Manufacturing | Inference | Isaac | Jetson | Physical AI | Robotics | Simulation and Design | Synthetic Data Generation]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 16:00:07 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/jetson-edge-ai-holiday-2025/</guid>
    </item>
    <item>
      <title>Game the Halls: GeForce NOW Brings Holiday Cheer With 30 New Games in the Cloud</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-dec-2025/</link>
      <description><![CDATA[Editorâ€™s note: The Game Pass edition of â€˜Hogwarts Legacyâ€™ will also be supported on GeForce NOW when the Steam and Epic Games Store versions launch on the service later this month. GeForce NOW is decking the digital halls with 30 new games to keep spirits high all month long. Join the fun with Hogwarts Legacy , the LEGO Harry Potter Collection and a sleighful of new adventures streaming straight from the cloud. The â€œHalf-Price Holidayâ€ sale keeps the savings rolling after Black Friday, with premium GeForce NOW memberships available at 50% off for the first month for a limited time. And GeForce NOW is bringing members a new way to jump into the worlds of some of the most iconic games. Stream select Activision titles through Ubisoft+ Premium in the cloud for GeForce RTX power â€” including Call of Duty: Modern Warfare II , Call of Duty: Modern Warfare III , Crash Bandicoot N. Sane Trilogy and the Spyro Reignited Trilogy , with more to come. Thatâ€™s not all: Battle.net single sign-on is now live, making it easier than ever for members to leap into Overwatch 2, Diablo IV and other titles without multiple logins. Link once and play instantly, no matter the device. Plus, the GeForce NOW Community Video Contest is rolling on â€” creators are decking the halls with epic clips and their merriest cloud gaming moments from all over the world. Thereâ€™s still time to jump in with a clip of a favorite game or a memorable match. Submit a clip and score two Ultimate day passes â€” one to share with a friend and one to keep â€” plus a chance to win a one-year Ultimate membership. Half-Price Holiday Wrap up the year with half off. The holiday season continues with more ways to play. Even after Black Friday, GeForce NOW is keeping the celebration going with its â€œHalf-Price Holidayâ€ offer. For a limited time, premium memberships are half off for the first month. Performance and Ultimate tiers unlock shorter queue times, longer gaming sessions and access to more than 2,000 additional Install-to-Play titles powered by GeForce RTX in the cloud. The â€œHalf-Price Holidayâ€ sale is the perfect opportunity to experience premium gaming at a fraction of the cost and keep the winter gaming streak alive. The sale wraps up on Tuesday, Dec. 30, while supplies last, so nowâ€™s the time to lock in that first month at half off and head into the new year with the GeForce NOW premium experience. Auto Sign-In, Auto Win One login, endless worlds. GeForce NOW is making it easier than ever for members to jump into games from Blizzard Entertainment, Activision and more. Starting today, members can link their Battle.net accounts directly to GeForce NOW for automatic single sign-on across all supported devices. After a quick one-time setup, members are automatically logged in for future cloud gaming sessions â€” no extra steps, no password juggling, just instant access to Battle.net favorites like Overwatch 2 and Diablo IV . The update expands on existing automatic login support for Xbox, Epic Games and Ubisoft, further streamlining the GeForce NOW cloud gaming experience. Whether at home or on the go, members can enjoy faster, simpler, smoother access to the games they love. A Very GeForce NOW December Eight travelers, one very GeForce NOW cloud. Square Enixâ€™s Octopath Traveler 0 brings the seriesâ€™ signature wanderlust and quiet drama back to Orsterra, this time with a fresh cast and a few extra tricks up each travelerâ€™s sleeve. The prequel leans into sharp character banter, devious Path Actions and choices that can charm, swindle or strong-arm just about anyone in the way. Expect rich pixel art, big feelings and plenty of scheming as players embark on an adventure of their own creation, navigating revenge and restoration. MARVEL Cosmic Invasion (New release on Steam and Xbox , available on Game Pass, Dec. 1) Call of Duty: Modern Warfare II (New release on Ubisoft , Dec. 2) Call of Duty: Modern Warfare III (New release on Ubisoft , Dec. 2) Crash Bandicoot N. Sane Trilogy (New release on Ubisoft , Dec. 2) XOCIETY (New release on Epic Games Store , Dec. 2) Spyro Reignited Trilogy (New release on Ubisoft , Dec. 2) Lost Records: Bloom & Rage (New release on Xbox , available on Game Pass, Dec. 2) OCTOPATH TRAVELER 0 (New release on Steam , Dec. 4) ROUTINE (New release on Steam and Xbox , available on Game Pass, Dec. 4) MIMESIS ( Steam ) GeForce RTX 5080-ready games: Enshrouded ( Steam ) Fallout 76 ( Steam and Xbox , available on Game Pass) Catch the full list of games coming to the cloud in December: Dome Keeper (New release on Xbox , available on Game Pass, Dec. 9) Death Howl (New release on Steam and Xbox , available on Game Pass, Dec. 9) Everdream Village (New release on Steam , Dec. 12) ARC Raiders ( Epic Games Store ) Dying Light: The Beast ( Epic Games Store ) Citizen Sleeper ( Steam ) For the King II ( Steam ) Jurassic World Evolution 3 ( Epic Games Store ) Hogwarts Legacy ( Steam, Epic Games Store , and Xbox , available on Game Pass) LEGO Harry Potter Collection ( Steam ) Lara Croft and the Temple of Osiris ( Xbox , available on Game Pass) Pigeon Simulator ( Xbox , available on Game Pass) Pacific Drive ( Xbox , available on Game Pass) Powerwash Simulator 2 (Steam) Shape of Dreams ( Steam ) Storage Hunter Simulator ( Steam ) Sword of the Sea ( Steam ) Underground Garage ( Steam ) Warhammer 40,000: SPACE MARINE 2 ( Epic Games Store ) Witchfire ( Epic Games Store ) â€˜Tis the Season for Extra Games In addition to the 23 games announced in November, an extra 10 joined over the month: Apollo Justice: Ace Attorney Trilogy ( Steam ) The Crew Motorfest ( Xbox , available on Game Pass) Cricket 26 ( Steam ) Kill It With Fire ( Xbox , available on PC Game Pass) Moonlighter 2: The Endless Vault ( Steam and Xbox , available on Game Pass) Of Ash and Steel ( Steam , GeForce RTX 5080-ready) Prologue: Go Wayback! ( Steam ) Sacred 2 Remaster ( Steam ) Songs of Silence ( Epic Games Store ) Zero Hour ( Epic Games Store ) To improve the overall quality of service for the most played games on GeForce NOW, members will see movement of games in the catalog. Some titles with little-to-no playtime that are currently available in the Ready-to-Play catalog will start moving to Install-to-Play on December 12. Premium members can continue to play these games as part of their Install-to-Play benefits. See this article for details. Some of the most popular Install-to-Play games â€” including Megabonk, R.E.P.O and RV There Yet? â€” have moved to Ready-to-Play, so theyâ€™ll always be kept up to date for instant streaming. What are you planning to play this weekend? Let us know on X or in the comments below. Health pots, extra weapons, alternate armor â€” what's an item you never remove from your inventory?ðŸŽ’ â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) December 3, 2025 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 14:00:37 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-dec-2025/</guid>
    </item>
    <item>
      <title>Mixture of Experts Powers the Most Intelligent Frontier AI Models, Runs 10x Faster on NVIDIA Blackwell NVL72</title>
      <link>https://blogs.nvidia.com/blog/mixture-of-experts-frontier-models/</link>
      <description><![CDATA[The top 10 most intelligent open-source models all use a mixture-of-experts architecture. Kimi K2 Thinking, DeepSeek-R1, Mistral Large 3 and others run 10x faster on NVIDIA GB200 NVL72. A look under the hood of virtually any frontier model today will reveal a mixture-of-experts (MoE) model architecture that mimics the efficiency of the human brain. Just as the brain activates specific regions based on the task, MoE models divide work among specialized â€œexperts,â€ activating only the relevant ones for every AI token . This results in faster, more efficient token generation without a proportional increase in compute. The industry has already recognized this advantage. On the independent Artificial Analysis (AA) leaderboard , the top 10 most intelligent open-source models use an MoE architecture, including DeepSeek AIâ€™s DeepSeek-R1, Moonshot AIâ€™s Kimi K2 Thinking, OpenAIâ€™s gpt-oss-120B and Mistral AIâ€™s Mistral Large 3. However, scaling MoE models in production while delivering high performance is notoriously difficult. The extreme codesign of NVIDIA GB200 NVL72 systems combines hardware and software optimizations for maximum performance and efficiency, making it practical and straightforward to scale MoE models. The Kimi K2 Thinking MoE model â€” ranked as the most intelligent open-source model on the AA leaderboard â€” sees a 10x performance leap on the NVIDIA GB200 NVL72 rack-scale system compared with NVIDIA HGX H200. Building on the performance delivered for the DeepSeek-R1 and Mistral Large 3 MoE models, this breakthrough underscores how MoE is becoming the architecture of choice for frontier models â€” and why NVIDIAâ€™s full-stack inference platform is the key to unlocking its full potential. What Is MoE, and Why Has It Become the Standard for Frontier Models? Until recently, the industry standard for building smarter AI was simply building bigger, dense models that use all of their model parameters â€” often hundreds of billions for todayâ€™s most capable models â€” to generate every token. While powerful, this approach requires immense computing power and energy, making it challenging to scale. Much like the human brain relies on specific regions to handle different cognitive tasks â€” whether processing language, recognizing objects or solving a math problem â€” MoE models comprise several specialized â€œexperts.â€ For any given token, only the most relevant ones are activated by a router. This design means that even though the overall model may contain hundreds of billions of parameters, generating a token involves using only a small subset â€” often just tens of billions. Like the human brain uses specific regions for different tasks, mixture-of-experts models use a router to select only the most relevant experts to generate every token. By selectively engaging only the experts that matter most, MoE models achieve higher intelligence and adaptability without a matching rise in computational cost. This makes them the foundation for efficient AI systems optimized for performance per dollar and per watt â€” generating significantly more intelligence for every unit of energy and capital invested. Given these advantages, it is no surprise that MoE has rapidly become the architecture of choice for frontier models, adopted by over 60% of open-source AI model releases this year. Since early 2023, itâ€™s enabled a nearly 70x increase in model intelligence â€” pushing the limits of AI capability. Since early 2025, nearly all leading frontier models use MoE designs. â€œOur pioneering work with OSS mixture-of-experts architecture, starting with Mixtral 8x7B two years ago, ensures advanced intelligence is both accessible and sustainable for a broad range of applications,â€ said Guillaume Lample, cofounder and chief scientist at Mistral AI. â€œMistral Large 3â€™s MoE architecture enables us to scale AI systems to greater performance and efficiency while dramatically lowering energy and compute demands.â€ Overcoming MoE Scaling Bottlenecks With Extreme Codesign Frontier MoE models are simply too large and complex to be deployed on a single GPU. To run them, experts must be distributed across multiple GPUs, a technique called expert parallelism. Even on powerful platforms such as the NVIDIA H200, deploying MoE models involves bottlenecks such as: Memory limitations : For each token, GPUs must dynamically load the selected expertsâ€™ parameters from high-bandwidth memory, causing frequent heavy pressure on memory bandwidth. Latency : Experts must execute a near-instantaneous all-to-all communication pattern to exchange information and form a final, complete answer. However, on H200, spreading experts across more than eight GPUs requires them to communicate over higher-latency scale-out networking, limiting the benefits of expert parallelism. The solution: extreme codesign. NVIDIA GB200 NVL72 is a rack-scale system with 72 NVIDIA Blackwell GPUs working together as if they were one, delivering 1.4 exaflops of AI performance and 30TB of fast shared memory. The 72 GPUs are connected using NVLink Switch into a single, massive NVLink interconnect fabric, which allows every GPU to communicate with each other with 130 TB/s of NVLink connectivity. MoE models can tap into this design to scale expert parallelism far beyond previous limits â€” distributing the experts across a much larger set of up to 72 GPUs. This architectural approach directly resolves MoE scaling bottlenecks by: Reducing the number of experts per GPU : Distributing experts across up to 72 GPUs reduces the number of experts per GPU, minimizing parameter-loading pressure on each GPUâ€™s high-bandwidth memory. Fewer experts per GPU also frees up memory space, allowing each GPU to serve more concurrent users and support longer input lengths. Accelerating expert communication : Experts spread across GPUs can communicate with each other instantly using NVLink. The NVLink Switch also has the compute power needed to perform some of the calculations required to combine information from various experts, speeding up delivery of the final answer. Other full-stack optimizations also play a key role in unlocking high inference performance for MoE models. The NVIDIA Dynamo framework orchestrates disaggregated serving by assigning prefill and decode tasks to different GPUs, allowing decode to run with large expert parallelism, while prefill uses parallelism techniques better suited to its workload. The NVFP4 format helps maintain accuracy while further boosting performance and efficiency. Open-source inference frameworks such as NVIDIA TensorRT-LLM, SGLang and vLLM support these optimizations for MoE models. SGLang, in particular, has played a significant role in advancing large-scale MoE on GB200 NVL72 , helping validate and mature many of the techniques used today. To bring this performance to enterprises worldwide, the GB200 NVL72 is being deployed by major cloud service providers and NVIDIA Cloud Partners including Amazon Web Services, Core42, CoreWeave, Crusoe, Google Cloud, Lambda, Microsoft Azure, Nebius, Nscale, Oracle Cloud Infrastructure, Together AI and others. â€œAt CoreWeave, our customers are leveraging our platform to put mixture-of-experts models into production as they build agentic workflows,â€ said Peter Salanki, cofounder and chief technology officer at CoreWeave. â€œBy working closely with NVIDIA, we are able to deliver a tightly integrated platform that brings MoE performance, scalability and reliability together in one place. You can only do that on a cloud purpose-built for AI.â€ Customers such as DeepL are using Blackwell NVL72 rack-scale design to build and deploy their next-generation AI models. â€œDeepL is leveraging NVIDIA GB200 hardware to train mixture-of-experts models, advancing its model architecture to improve efficiency during training and inference, setting new benchmarks for performance in AI,â€ said Paul Busch, research team lead at DeepL. The Proof Is in the Performance Per Watt NVIDIA GB200 NVL72 efficiently scales complex MoE models and delivers a 10x leap in performance per watt. This performance leap isnâ€™t just a benchmark; it enables 10x the token revenue, transforming the economics of AI at scale in power- and cost-constrained data centers. At NVIDIA GTC Washington, D.C., NVIDIA founder and CEO Jensen Huang highlighted how GB200 NVL72 delivers 10x the performance of NVIDIA Hopper for DeepSeek-R1, and this performance extends to other DeepSeek variants as well. â€œWith GB200 NVL72 and Together AIâ€™s custom optimizations, we are exceeding customer expectations for large-scale inference workloads for MoE models like DeepSeek-V3,â€ said Vipul Ved Prakash, cofounder and CEO of Together AI. â€œThe performance gains come from NVIDIAâ€™s full-stack optimizations coupled with Together AI Inference breakthroughs across kernels, runtime engine and speculative decoding.â€ This performance advantage is evident across other frontier models. Kimi K2 Thinking, the most intelligent open-source model, serves as another proof point, achieving 10x better generational performance when deployed on GB200 NVL72. Fireworks AI has currently deployed Kimi K2 on the NVIDIA B200 platform to achieve the highest performance on the Artificial Analysis leaderboard . â€œNVIDIA GB200 NVL72 rack-scale design makes MoE model serving dramatically more efficient,â€ said Lin Qiao, cofounder and CEO of Fireworks AI. â€œLooking ahead, NVL72 has the potential to transform how we serve massive MoE models, delivering major performance improvements over the Hopper platform and setting a new bar for frontier model speed and efficiency.â€ Mistral Large 3 also achieved a 10x performance gain on the GB200 NVL72 compared with the prior-generation H200. This generational gain translates into better user experience, lower per-token cost and higher energy efficiency for this new MoE model. Powering Intelligence at Scale The NVIDIA GB200 NVL72 rack-scale system is designed to deliver strong performance beyond MoE models. The reason becomes clear when taking a look at where AI is heading: the newest generation of multimodal AI models have specialized components for language, vision, audio and other modalities, activating only the ones relevant to the task at hand. In agentic systems, different â€œagentsâ€ specialize in planning, perception, reasoning, tool use or search, and an orchestrator coordinates them to deliver a single outcome. In both cases, the core pattern mirrors MoE: route each part of the problem to the most relevant experts, then coordinate their outputs to produce the final outcome. Extending this principle to production environments where multiple applications and agents serve multiple users unlocks new levels of efficiency. Instead of duplicating massive AI models for every agent or application, this approach can enable a shared pool of experts accessible to all, with each request routed to the right expert. Mixture of experts is a powerful architecture moving the industry toward a future where massive capability, efficiency and scale coexist. The GB200 NVL72 unlocks this potential today, and NVIDIAâ€™s roadmap with the NVIDIA Vera Rubin architecture will continue to expand the horizons of frontier models. Learn more about how GB200 NVL72 scales complex MoE models in this technical deep dive . This post is part of Think SMART , a series focused on how leading AI service providers, developers and enterprises can boost their inference performance and return on investment with the latest advancements from NVIDIAâ€™s full-stack inference platform . Categories: Data Center Tags: Artificial Intelligence | Dynamo | Inference | NVIDIA Blackwell | NVLink | Open Source | Think SMART]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 03 Dec 2025 16:00:32 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/mixture-of-experts-frontier-models/</guid>
    </item>
    <item>
      <title>NVIDIA Partners With Mistral AI to Accelerate New Family of Open Models</title>
      <link>https://blogs.nvidia.com/blog/mistral-frontier-open-models/</link>
      <description><![CDATA[Today, Mistral AI announced the Mistral 3 family of open-source multilingual, multimodal models, optimized across NVIDIA supercomputing and edge platforms. Mistral Large 3 is a mixture-of-experts (MoE) model â€” i nstead of firing up every neuron for every token, it only activates the parts of the model with the most impact. The result is efficiency that delivers scale without waste, accuracy without compromise and makes enterprise AI not just possible, but practical. Mistral AIâ€™s new models deliver industry-leading accuracy and efficiency for enterprise AI. It will be available everywhere, from the cloud to the data center to the edge, starting Tuesday, Dec. 2. With 41B active parameters, 675B total parameters and a large 256K context window, Mistral Large 3 delivers scalability, efficiency and adaptability for enterprise AI workloads. By combining NVIDIA GB200 NVL72 systems and Mistral AIâ€™s MoE architecture , enterprises can efficiently deploy and scale massive AI models, benefiting from advanced parallelism and hardware optimizations. This combination makes the announcement a step toward the era of â€” what Mistral AI calls â€˜distributed intelligence,â€™ bridging the gap between research breakthroughs and real-world applications. The modelâ€™s granular MoE architecture unlocks the full performance benefits of large-scale expert parallelism by tapping into NVIDIA NVLinkâ€™s coherent memory domain and using wide expert parallelism optimizations. These benefits stack with accuracy-preserving, low-precision NVFP4 and NVIDIA Dynamo disaggregated inference optimizations, ensuring peak performance for large-scale training and inference. On the GB200 NVL72, Mistral Large 3 achieved 10x performance gain compared with the prior â€“ generation NVIDIA H200. This generational gain translates into a better user experience, lower per-toke n co st and higher energy efficiency. Mistral AI isnâ€™t just driving state of the art for frontier large language models; it also released nine small language models that help developers run AI anywhere. The compact Ministral 3 suite is optimized to run across NVIDIAâ€™s edge platforms, including NVIDIA Spark, RTX PCs and laptops and NVIDIA Jetson devices. To deliver peak performance, NVIDIA collaborates on top AI frameworks such as Llama.cpp and Ollama to deliver peak performance across NVIDIA GPUs on the edge. Today, developers and enthusiasts can try out the Ministral 3 suite via Llama.cpp and Ollama for fast and efficient AI on the edge. The Mistral 3 family of models is openly available, empowering researchers and developers everywhere to experiment, customize and accelerate AI innovation while democratizing access to frontier-class technologies. By linking Mistral AIâ€™s models to open-source NVIDIA NeMo tools for AI agent lifecycle development â€” Data Designer, Customizer, Guardrails and NeMo Agent Toolkit â€” enterprises can customize these models further for their own use cases, making it faster to move from prototype to production. And to achieve efficiency from cloud to edge, NVIDIA has optimized inference frameworks including NVIDIA TensorRT-LLM, SGLang and v LLM for the Mistral 3 model family. Mistral 3 is available today on leading open-source platforms and cloud service providers . In addition, the models are expected to be deployable soon as NVIDIA NIM microservices. Wherever AI needs to go, these models are ready. See notice regarding software product information. Categories: Data Center Tags: Inference | NVL72 | NVLink | TensorRT]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 02 Dec 2025 18:00:30 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/mistral-frontier-open-models/</guid>
    </item>
    <item>
      <title>NVIDIA and AWS Expand Full-Stack Partnership, Providing the Secure, High-Performance Compute Platform Vital for Future Innovation</title>
      <link>https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</link>
      <description><![CDATA[At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI. As part of this expansion, AWS will support NVIDIA NVLink Fusion â€” a platform for custom AI infrastructure â€” for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure. Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities. AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion. AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms. AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems. By supporting AWSâ€™s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWSâ€™s cloud infrastructure and accelerating new AI service rollout. â€œGPU compute demand is skyrocketing â€” more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,â€ said Jensen Huang, founder and CEO of NVIDIA. â€œWith NVIDIA NVLink Fusion coming to AWS Trainium4, weâ€™re unifying our scale-up architecture with AWSâ€™s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution â€” bringing advanced AI to every company, in every country, and accelerating the worldâ€™s path to intelligence.â€ â€œAWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,â€ said Matt Garman, CEO of AWS. â€œWith NVIDIA, weâ€™re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.â€ Convergence of Scale and Sovereignty AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industryâ€™s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks. These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations. NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements. For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWSâ€™s industry-leading cloud infrastructure and services â€” known for its reliability, security and scalability â€” with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches. The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks. NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations Beyond hardware, the partnership expands integration of NVIDIAâ€™s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock , enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy. The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrockâ€™s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents. NVIDIA Software on AWS Simplifies Developer Experience NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS , an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost. These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when itâ€™s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs. Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production. This expanded support builds on AWSâ€™s existing integrations with NVIDIA technologies â€” including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo , as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock â€” that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever. Accelerating Physical AI With AWS Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment. NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS , enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation , Cosmos WFMs are also available on AWS Batch as containers. Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab . Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development. Sustained Collaboration Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats. Learn more about NVIDIA and AWSâ€™s collaboration and join sessions at AWS re:Invent , running through Friday, Dec. 5, in Las Vegas. Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Cosmos | Events | Isaac | Nemotron | NVIDIA Blackwell | NVIDIA NIM | NVIDIA Spectrum-X | NVLink | Open Source | Physical AI | Riva]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 02 Dec 2025 16:00:27 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</guid>
    </item>
    <item>
      <title>At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI</title>
      <link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link>
      <description><![CDATA[Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools â€” with potential applications in virtually every research field. At NeurIPS , one of the worldâ€™s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the worldâ€™s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety. NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more. These initiatives deepen NVIDIAâ€™s commitment to open source â€” an effort recognized by a new Openness Index from Artificial Analysis , an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details. NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving NVIDIA DRIVE Alpamayo-R1 (AR1) , the worldâ€™s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning â€” a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy . While previous iterations of self-driving models struggled with nuanced situations â€” a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane â€” reasoning gives autonomous vehicles the common sense to drive more like humans do. AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route. For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces â€” explanations on why it took certain actions â€” and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers. https://blogs.nvidia.com/wp-content/uploads/2025/12/construction_worker.mp4 AR1â€™s open foundation, based on NVIDIA Cosmos Reason , lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications. For post-training AR1, reinforcement learning has proven especially effective â€” researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model. NVIDIA DRIVE Alpamayo-R1 is now available on GitHub and Hugging Face , and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets . NVIDIA has also released the open-source AlpaSim framework to evaluate AR1. Learn more about reasoning VLA models for autonomous driving . Customizing NVIDIA Cosmos for Any Physical AI Use Case Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook . Itâ€™s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation. There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include: LidarGen , the first world model that can generate lidar data for AV simulation. Omniverse NuRec Fixer , a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data. Cosmos Policy , a framework for turning large pretrained video models into robust robot policies â€” a set of rules that dictate a robotâ€™s behavior. ProtoMotions3 , an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs) . Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen. Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics. Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM. NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs. AV developer Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X , Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation. NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include: MultiTalker Parakeet : An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations. Sortformer : A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream â€” a process called diarization â€” in real time. Nemotron Content Safety Reasoning : A reasoning-based AI safety model that dynamically enforces custom policies across domains. Nemotron Content Safety Audio Dataset : A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities. NeMo Gym : an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR). NeMo Data Designer Library : Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation. NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow. NeurIPS attendees can explore these innovations at the Nemotron Summit , taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. NVIDIA Research Furthers Language AI Innovation Of the dozens of NVIDIA-authored research papers at NeurIPS , here are a few highlights advancing language models: Audio Flamingo 3: Advancing Audio Intelligence With Fully Open Large Audio Language Models : This large audio language model is capable of reasoning across speech, sound and music. It can understand and reason audio segments up to 10 minutes in length, achieving state-of-the-art results on over 20 benchmarks. Minitron-SSM: Efficient Hybrid Language Model Compression Through Group-Aware SSM Pruning : This poster introduces a pruning method capable of compressing hybrid models, demonstrated by pruning and distilling Nemotron-H 8B from 8 billion to 4 billion parameters. The resulting model surpasses the accuracy of similarly sized models while achieving 2x faster inference throughput. Jet-Nemotron: Efficient Language Model With Post Neural Architecture Search : This work presents a cost-efficient post-training pipeline for developing new efficient language model architectures, and introduces a hybrid-architecture model family produced with the pipeline. These models match or surpass the accuracy of leading full-attention baselines while delivering substantially higher generation throughput. Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models : This project introduces a new small language model (SLM) architecture that redesigns SLMs around real-world latency rather than parameter count â€” achieving state-of-the-art speed and accuracy. ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models : Prolonged reinforcement learning, or ProRL, is a technique that extends model training over longer periods. In this NeurIPS poster, NVIDIA researchers describe how this methodology results in models that consistently outperform base models for reasoning. View the full list of events at NeurIPS , running through Sunday, Dec. 7, in San Diego. See notice regarding software product information. Categories: Corporate | Driving | Generative AI | Research | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | NVIDIA Research | Open Source | Physical AI | Synthetic Data Generation | Transportation]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 01 Dec 2025 17:00:48 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid>
    </item>
    <item>
      <title>The Ultimate Black Friday Deal Is Here</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-black-friday-2025/</link>
      <description><![CDATA[Black Friday is leveling up. Get ready to score one of the biggest deals of the season â€” 50% off the first three months of a new GeForce NOW Ultimate membership. Thatâ€™s GeForce RTX 5080-class power in the cloud for half the price. The completed Ultimate upgrade. The NVIDIA Blackwell RTX upgrade is now fully live across all servers â€” including in Stockholm, the final region to receive the upgrade â€” bringing even faster performance and lower latency to more members. To celebrate this milestone, Ultimate members can claim a Battlefield 6 reward to show off in style. Kick off a gaming-filled weekend with seven new titles joining the GeForce NOW library . And the GeForce NOW Community Video Contest continues. Submit epic gameplay moments or share what makes streaming with GeForce NOW special for a chance to win Ultimate day passes or even a full year of Ultimate access. Keep those clips rolling â€” thereâ€™s still time to enter and level up chances to win. Black Friday, GFN Thursday Green is the new black. The Ultimate Black Friday Sale is the perfect time to upgrade to peak cloud performance, with Blackwell RTX now live worldwide. Through Sunday, Nov. 30, the first three months of an Ultimate membership are 50% off. An Ultimate membership delivers GeForce RTX 5080-class power from the cloud, powering the fastest frame rates, ultrasmooth gameplay and breathtaking visuals with NVIDIA DLSS 4 technology. With cinematic-quality streaming up to 5K 120 frames per second, every session plays out with premium precision. Top titles like Battlefield 6 , Borderlands 4 and Outer Worlds 2 shine with GeForce RTX 50 Series performance. From explosive action to expansive worlds, Ultimate transforms every adventure into a showpiece. The Ultimate Black Friday Sale â€” available in the U.S., Mexico and Japan â€” ends at the end of this week. Take advantage now for epic savings and next-level performance. Built for Victors Dominate the battlefield. Ultimate members will turn heads in Battlefield 6 with a new free reward: an in-game weapon skin for the Marksman SVK-8.6 DMR, forged for those who thrive where chaos reigns. The fight takes on new style with a skin that means business, featuring battle-scarred alloy, digital accents and just enough swagger to stir envy among any squad. This weapon skin stands ready for gamers who lock in on victory, even as the odds explode around them. The limited-time reward is available through Friday, Dec. 26. Ultimate members can sign in to GeForce NOW and claim an edge in the midst of chaos. Zoom to New Games Born to push the limits. Project Motor Racing is an ambitious racing simulation game that captures the intensity and challenge of professional motorsport, featuring a wide selection of meticulously recreated cars and global circuits. Designed for both seasoned sim racers and newcomers, it offers deep single-player career modes and robust online competition, all built around immersive physics, dynamic weather and a dedication to authenticity. Every race is a fresh test of skill, with relentless attention to detail and a vibrant, competitive racing community. Check out this weekâ€™s new games here: Of Ash and Steel (New release on Steam , Nov. 24, GeForce RTX 5080-ready) Kill It With Fire ( Xbox , available on PC Game Pass, Nov. 25) Project Motor Racing (New release on Steam , Nov. 25) Brotato ( Steam ) Cricket 26 ( Steam ) GODBREAKERS ( Steam ) Zero Hour ( Epic Games Store ) And another GeForce RTX 5080-ready game atop this weekâ€™s addition Of Ash and Steel : Phasmaphobia (Steam) Launch dates shared on GFN Thursdays reflect release dates for new titles, which will arrive on GeForce NOW within the following week. Before digging into a gaming weekend, check out the question below. Post answers on X or in the comments below. So, here's the deal. Literally. ðŸ’¸ðŸ’¸ The Ultimate Black Friday deal is live! GO ðŸƒâ€â™‚ï¸â€âž¡ï¸ https://t.co/mMt5aYmSsH #GeForceSeason pic.twitter.com/idnvq6VBlO â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) November 25, 2025 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 27 Nov 2025 14:00:37 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-black-friday-2025/</guid>
    </item>
    <item>
      <title>From Government to Gaming, AI Is â€˜Strengthening Koreaâ€™s Digital Foundation,â€™ NVIDIA Leader Says at AI Day Seoul</title>
      <link>https://blogs.nvidia.com/blog/ai-day-seoul/</link>
      <description><![CDATA[Last week, more than 1,000 attendees joined NVIDIA AI Day Seoul to learn about sovereign AI â€” including breakout sessions on agentic and physical AI, hands-on workshops and a startupâ€¦Read Article]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 26 Nov 2025 17:00:32 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-day-seoul/</guid>
    </item>
    <item>
      <title>FLUX.2 Image Generation Models Now Released, Optimized for NVIDIA RTX GPUs</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</link>
      <description><![CDATA[Black Forest Labs â€” the frontier AI research lab developing visual generative AI models â€” today released the FLUX.2 family of state-of-the-art image generation models. FLUX.2 is packed with new tools and capabilities, including a multi-reference feature that can generate dozens of similar image variations, in photorealistic detail and with cleaner fonts â€” even at scale. NVIDIA has worked with Black Forest Labs and ComfyUI to make the models available with FP8 quantizations and RTX GPU performance optimizations at launch, decreasing the VRAM required to run them by 40% and improving performance by 40%. Requiring no special software package to run, the models are available directly in ComfyUI . State-of-the-Art Visual Intelligence Images generated by FLUX.2 are photorealistic, even at scale, featuring up to 4 megapixel resolution with real-world lighting and physics to eliminate that â€œAI lookâ€ that undermines visual fidelity. The models add direct pose control to explicitly specify the pose of a subject or character in an image, as well as deliver clean, readable text across infographics, user interface screens and even multilingual content. Plus, the new multi-reference feature enables artists to select up to six reference images where the style or subject stays consistent â€” eliminating the need for extensive model fine-tuning. Stunning, photorealistic details. Image courtesy of Black Forest Labs. For a complete overview of new FLUX.2 features, read Black Forest Labsâ€™ blog . Optimized for RTX The new FLUX.2 models are impressive, but also quite demanding. They run a staggering 32-billion-parameter model requiring 90GB VRAM to load completely. Even using lowVRAM mode â€” a popular setting that allows artists to only load the active model at a time â€” the VRAM requirement is still 64GB, which puts the model virtually out of reach for any consumer card to use effectively. To broaden FLUX.2 model accessibility, NVIDIA and Black Forest Labs collaborated to quantize the model to FP8 â€” reducing the VRAM requirements by 40% at comparable quality. FLUX.2 is here. And to make this model accessible on GeForce RTX GPUs, NVIDIA has partnered with ComfyUI â€” a popular application to run visual generative AI models on PC â€” to improve the appâ€™s RAM offload feature, known as weight streaming. Using the upgraded feature, users can offload parts of the model to system memory, extending the available memory on their GPUs â€” albeit with some performance loss, as system memory is slower than GPU memory. NVIDIA has also been collaborating with ComfyUI to optimize model performance on NVIDIA and GeForce RTX GPUs, including optimizations for FP8 checkpoints. Get started with FLUX.2 today. Update ComfyUI and check out the FLUX.2 templates, or visit Black Forest Labsâ€™ Hugging Face page to download the model weights. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . Categories: Generative AI Tags: Artificial Intelligence | Conversational AI | Creators | GeForce | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 25 Nov 2025 15:53:21 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</guid>
    </item>
    <item>
      <title>AI On: 3 Ways Specialized AI Agents Are Reshaping Businesses</title>
      <link>https://blogs.nvidia.com/blog/specialized-ai-agents/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of the AI On blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries. As agentic AI adoption continues to grow, with open-source models and tools maturing, companies across industries are increasingly asking: what AI agents should we build to solve our unique business challenges? Although faster outcomes are a core benefit of using AI, organizations are finding that specialization is the key to business impact and long-term AI adoption. Rather than relying on one-size-fits-all models and services, leading companies are developing specialized AI agents designed to understand and act within the needs of a specific use case. CrowdStrike, PayPal and Synopsys are examples of companies combining NVIDIA Nemotron open foundation models with their proprietary data and institutional knowledge to create specialized applications. The results are intelligent agents that have the level of expertise required to work alongside human colleagues and boost business operations. 1. CrowdStrike Defends Against Modern Cyber Threats In cybersecurity, speed and precision are essential, especially as cyber threats become more advanced and grow to larger scales. To meet these rapidly evolving digital threats, CrowdStrike is building specialized AI agents that can work alongside security teams through Charlotte AI AgentWorks. These agents, powered by NVIDIA Nemotron open models and NVIDIA NIM microservices, automate high-volume tasks such as alert triage and remediation, allowing human analysts to focus on higher-order decision-making. Built on open models and continuously trained by incident responders, CrowdStrikeâ€™s Agentic Security Platform increases accuracy of alert triage from 80% to 98.5% , reducing security analyst teamsâ€™ manual effort tenfold. The platform can adapt to new risks and collaborates across the security operations center. 2. PayPalâ€™s AI Agents Power Frictionless Commerce at Scale PayPal, a leader in payments and e-commerce, is building agent-driven infrastructure to accelerate intelligent commerce . The companyâ€™s specialized AI agents, developed on Nemotron open models, will enable the first wave of conversational commerce experiences, where agents can shop, buy and pay on a userâ€™s behalf. With this approach, PayPal built a fine-tuning pipeline in two weeks and reduced latency by nearly 50% while maintaining the high accuracy required to serve its 430 million customers and 30 million merchants. PayPalâ€™s agents rely on open, modular models that are fine-tuned specifically for payments and commerce, giving the company the control to balance performance, accuracy and cost at a massive scale. 3. Synopsys Advances Agentic AI for Chip Design Workflows The complexity of modern semiconductor design and manufacturing calls for expertise, precision and speed. Synopsys is pioneering an agentic AI framework that can be deployed throughout the chip development workflow. Synopsysâ€™ vision for agentic AI includes Synopsys AgentEngineer technology that can significantly boost productivity in research and development, identifying critical design bugs and helping reduce costly delays that traditional techniques can miss. In early trials of a formal verification workflow, Synopsys AI agents running on NVIDIA accelerated infrastructure achieved a 72% boost in productivity. Using open models fine-tuned for each engineering task, as well as software like the NVIDIA NeMo Agent Toolkit and Blueprints, Synopsys is enabling a new frontier of AI-enabled chip design. Building Specialized AI Agents With NVIDIA Technologies Companies across industries are taking the following steps to transform their proprietary knowledge into specialized AI agents: Evaluate open models, like NVIDIA Nemotron , that provide a powerful building block to create specialized models for any domain. Curate, generate and secure domain data using NVIDIA NeMo for agent lifecycle management. Create specialized agents using customized models that have access to proprietary data. Continue to fine-tune agents over time with a data flywheel . Learn how NVIDIA Nemotron can help businesses build specialized AI agents for maximum productivity and return on investment. Categories: Data Center | Generative AI Tags: Agentic AI | AI On | Artificial Intelligence | Cybersecurity | Financial Services | Industrial and Manufacturing | Nemotron | NVIDIA Blueprints | NVIDIA NeMo | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 24 Nov 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/specialized-ai-agents/</guid>
    </item>
    <item>
      <title>Into the Omniverse: How Smart City AI Agents Transform Urban Operations</title>
      <link>https://blogs.nvidia.com/blog/smart-city-ai-agents-urban-operations/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of Into the Omniverse , a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in OpenUSD and NVIDIA Omniverse . Cities worldwide face unprecedented challenges as urban populations surge and infrastructure strains to keep pace. Operational challenges like traffic congestion and coordinating emergency services are compounded by fragmented data pipelines, siloed local government processes and disparate systems. Technical barriers prevent cities from accessing the comprehensive, real-time insights needed for effective decision-making and city management. Leading cities and technology partners are deploying the NVIDIA Blueprint for smart city AI , a reference application that provides the complete software stack to build, test and operate AI agents in simulation-ready ( SimReady ) digital twins. OpenUSD is an open and extensible framework that connects to each stage of this physical AI workflow. OpenUSD-enabled digital twins serve as SimReady environments where cities can simulate â€œwhat ifâ€ scenarios and generate physically accurate sensor data. The blueprint powers a three-stage workflow: 1) simulate with the NVIDIA Cosmos platform and NVIDIA Omniverse libraries to generate synthetic data, 2) train and fine-tune vision AI models, and 3) deploy real-time video analytics AI agents with the NVIDIA Metropolis platform and the NVIDIA Blueprint for video search and summarization (VSS). This enables cities to move from reactive to proactive operations.â€‹ Based on these simulations, cities can deploy operational platforms where weather data, traffic sensors and emergency response systems converge, supporting rapid testing of rare scenarios, real-time monitoring, city infrastructure planning and optimization of urban systems. From Kaohsiung City, Taiwan, cutting incident response times by 80% with street-level AI to Raleigh, North Carolina, achieving 95% vehicle detection accuracy and French rail networks optimizing energy consumption by 20%, cities across the globe are using digital twins and AI agents to transform urban operations at scale. Smart Cities in Action Akila, With SNCF Gares&Connexions, Uses Digital Twins to Improve Rail Operations Akilaâ€™s digital twin application helps French rail operator SNCF Gares&Connexions optimize its network of nearly 14,000 daily trains with live scenario planning for solar heating, air flow and crowd movement. The OpenUSD-enabled digital twins deliver a 20% reduction in energy consumption, 100% on-time preventive maintenance and a 50% reduction in downtime and response times. Linker Vision Taps Physical AI for Street-Level Intelligence Linker Visionâ€™s physical AI system recognizes infrastructure events in Kaohsiung City, including damaged streetlights and fallen trees, eliminating manual city inspections and enabling faster emergency response. To scale its street-level intelligence to more cities, Linker Vision uses Omniverse libraries for simulation, Cosmos Reason for world understanding and the VSS blueprint for deployment powered by OpenUSD. Esri and Microsoft Enable Comprehensive Urban Intelligence in the City of Raleigh The City of Raleigh achieved 95% vehicle detection accuracy using the NVIDIA DeepStream software development kit, boosting traffic analysis workflows for engineers. This data enhances Raleighâ€™s digital twin, enabled by Esriâ€™s ArcGIS geospatial platform to support visualization and analysis for critical infrastructure planning and management. Integrating this computer vision pipeline with a vision AI agent powered by the NVIDIA VSS blueprint provides comprehensive real-time visibility and insights in ArcGIS on Azure Cloud. Milestone Systemsâ€™ VLM Automates Video Review Milestone Systems is soon launching its Hafnia VLM, which will include a VLM plug-in for its video management software XProtect as well as a VLM-as-a-service. Fine-tuned on more than 75,000 hours of video data, the Hafnia VLM can reduce operator alarm fatigue by up to 30% by automating video review and filtering out false alarms. It was developed with NVIDIA Cosmos Reason VLMs and Metropolis. The Hafnia VLM plug-in for XProtect will make generative AI more easily accessible for XProtect operators and users. K2K Analyzes Italy Video Streams K2Kâ€™s platform uses NVIDIA Cosmos Reason and the VSS blueprint to analyze over 1,000 video streams in Palermo, Italy, processing 7 billion events annually and automatically notifying city officials through natural language queries and video events when critical conditions are extracted and analyzed. Learn more about how cities are transforming with simulation, vision AI and digital twins by watching this on-demand NVIDIA GTC session, â€œ Leadership Strategies to Transform Public Services .â€ Get Started With Smart City AI Learn more about OpenUSD and computer vision workflows through these resources: Watch this video on bringing physical AI to cities with the NVIDIA Blueprint for smart city AI. Read this technical blog on how to integrate computer vision pipelines with generative AI and reasoning with the VSS blueprint. Access step-by-step intelligent traffic system workflows and technical recipes in new cookbooks for NVIDIA Cosmos Predict , NVIDIA Cosmos Transfer and NVIDIA Cosmos Reason . Stay up to date by subscribing to NVIDIA Omniverse news , joining the Omniverse community and following Omniverse on Discord , Instagram , LinkedIn , Threads , X and YouTube . Categories: Generative AI | Pro Graphics Tags: Agentic AI | Artificial Intelligence | Computer Vision | Cosmos | DeepStream | Digital Twin | Into the Omniverse | Metropolis | Mobility | NVIDIA Blueprints | Omniverse | Physical AI | Simulation and Design | Smart Spaces | Synthetic Data Generation | Universal Scene Description | Visual Computing]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 20 Nov 2025 16:00:39 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/smart-city-ai-agents-urban-operations/</guid>
    </item>
    <item>
      <title>The Largest Digital Zoo: Biology Model Trained on NVIDIA GPUs Identifies Over a Million Species</title>
      <link>https://blogs.nvidia.com/blog/bioclip2-foundation-ai-model/</link>
      <description><![CDATA[Tanya Berger-Wolfâ€™s first computational biology project started as a bet with a colleague: that she could build an AI model capable of identifying individual zebras faster than a zoologist. She won. Now, the director of the Translational Data Analytics Institute and a professor at The Ohio State University, Berger-Wolf is taking on the whole animal kingdom with BioCLIP 2 , a biology-based foundation model trained on the biggest, most diverse dataset of organisms to date. The model will be showcased at this yearâ€™s NeurIPS AI research conference. BioCLIP 2 goes beyond extracting information from images. It can distinguish speciesâ€™ traits and determine inter-and intraspecies relationships. For example, the model arranged Darwinâ€™s finches by beak size, without teaching the concept of size, shown in the image below. Scatter plot shows how BioCLIP 2 arranges Darwinâ€™s finches by beak size from left to right. These capabilities will allow researchers to use the model as both a biological encyclopedia, a powerful scientific platform and an interactive research tool with inference capabilities to help address an ongoing issue in conservation biology: data deficiency for certain species. â€œFor iconic species like killer whales, we lack enough data to determine population size and for polar bears, the population is unknown,â€ said Berger-Wolf. â€œIf we donâ€™t have data for those species, what hope do the beetles and fungi have?â€ AI models can enhance existing conservation efforts for threatened species and their habitats by filling this data-deficiency gap. BioCLIP 2 is available under an open-source license on Hugging Face , where it was downloaded over 45,000 times last month. This paper builds on the first BioCLIP model, released over a year ago, which was also trained on NVIDIA GPUs and received the Best Student Paper award at the Computer Vision and Pattern Recognition (CVPR) conference. The BioCLIP 2 paper will be presented at NeurIPS, taking place Nov. 30-Dec. 5 in Mexico City, and Dec. 2-7 in San Diego. Building the Worldâ€™s Biggest Biological Flash Card Deck The project began with the compilation of a massive dataset, TREEOFLIFE-200M , which comprises 214 million images of organisms that span over 925,000 taxonomic classes â€” from monkeys to mealworms and magnolias. To curate this vast amount of data, Berger-Wolfâ€™s team at the Imageomics Institute collaborated with the Smithsonian Institution , experts from various universities and other field-related organizations. These researchers set out to discover what would happen if they trained a biology model on more data than ever. The team wanted to see if it was possible to move â€œbeyond the science of individual organisms to the science of ecosystems,â€ said Berger-Wolf. After 10 days of training on 32 NVIDIA H100 GPUs, BioCLIP 2 displayed novel abilities, such as distinguishing between adult and juvenile as well as male and female animals within species â€” without being explicitly taught these concepts. It also made associations between related species â€” like understanding how zebras relate to other equids. â€œThis model learns that at every level of taxonomy, all of these images of zebras have a particular genus label, and of these images of equids â€” including zebras, horses and donkeys â€” they have a particular family trait and so on,â€ she said. â€œIt learns the hierarchy without ever being told it, just through these associations.â€ The model can even determine the health of an organism based on training data. For example, it separated healthy apple or blueberry leaves from diseased leaves, as well as could recognize differing types of diseases, when generating the scatter plot below. The scatter plots show plant species better separated as the model is trained. The intra-species variations also form clusters, making them easier to separate. Berger-Wolfâ€™s team used a cluster of 64 NVIDIA Tensor Core GPUs to accelerate model training, plus individual Tensor Core GPUs for inference . â€œFoundation models like BioCLIP would not be possible without NVIDIA accelerated computing,â€ said Berger-Wolf. Wildlife Digital Twins: The Future of Studying Ecosystem Relationships The researchersâ€™ next endeavor is to develop a wildlife-based interactive digital twin that can be used to visualize and simulate ecological interactions between species as well as their ways of engaging with the environment. The goal is to provide a safe, easy way to study organismal relationships that naturally occur in the wild, while minimizing impact and disturbance on ecosystems. â€œThe digital twin allows us to visualize species interactions and put them in context, as well as to play the what-if scenarios and test our models without destroying the actual environment â€” creating as light a footprint as possible,â€ said Berger-Wolf. The digital twin will give scientists the opportunity to explore the points of view of the species theyâ€™re studying within the simulated environment, opening endless possibilities for more complex and accurate ecological research. Eventually, versions of this technology could even be deployed for public use â€” such as through interactive platforms at zoos. People could explore, visualize and learn about the natural environment and its many species from entirely new vantage points. â€œIâ€™m getting goosebumps just imagining that scenario of a kid coming into the zoo and being like, wow â€” this is what you would see if you were another zebra part of that herd, or if you were the little spider sitting on that scratching post,â€ Berger-Wolf said. Learn more about BioCLIP 2 . Categories: Generative AI | Research Tags: Artificial Intelligence | Education | Science]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 20 Nov 2025 14:00:17 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/bioclip2-foundation-ai-model/</guid>
    </item>
    <item>
      <title>Powering AI Superfactories, NVIDIA and Microsoft Integrate Latest Technologies for Inference, Cybersecurity, Physical AI</title>
      <link>https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</link>
      <description><![CDATA[Timed with the Microsoft Ignite conference running this week, NVIDIA is expanding its collaboration with Microsoft, including through the adoption of next-generation NVIDIA Spectrum-X Ethernet switches for the new Microsoft Fairwater AI superfactory, powered by the NVIDIA Blackwell platform. The collaboration brings new integrations across Microsoft 365 Copilot, as well as the public preview of next-generation Azure NC Series VMs powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs , NVIDIA Nemotron integrations to accelerate AI for Microsoft SQL Server 2025, capabilities for onboarding AI agents in Microsoft 365 and optimizations for high-performance inference, cybersecurity and physical AI. Microsoftâ€™s AI Superfactory connects the landmark Fairwater data center in Wisconsin with a new, state-of-the-art facility in Atlanta , Georgia. This massive-scale infrastructure will integrate hundreds of thousands of NVIDIA Blackwell GPUs for large-scale training. In addition, Microsoft is deploying more than 100,000 Blackwell Ultra GPUs in NVIDIA GB300 NVL72 systems being deployed globally for inference. â€œOur collaboration with NVIDIA is built on driving innovation across the entire system and full stack, from silicon to services,â€ said Nidhi Chappell, corporate vice president of product management at Microsoft. â€œBy coupling Microsoft Azureâ€™s unmatched data center scale with NVIDIAâ€™s accelerated computing, we are maximizing AI data center performance and efficiency, which is of paramount importance for our customers leading the new AI era.â€ The most demanding workloads for OpenAI, the Microsoft AI Superintelligence Team, Microsoft 365 Copilot and Microsoft Foundry services will be powered by this infrastructure. Customers like Black Forest Labs are also using NVIDIA GB200 NVL72 systems to train next-generation multimodal FLUX models that power visual intelligence. To connect this massive infrastructure, Microsoft is deploying next-generation NVIDIA Spectrum-X Ethernet switches in its Fairwater AI data center â€” the largest and most sophisticated AI factories ever built â€” delivering the performance, scale and efficiency required for OpenAI to run large-scale AI models and applications. New Azure NCv6 Series VMs with NVIDIA RTX PRO 6000 Blackwell GPUs are now in public preview on Azure, expanding the Blackwell platform to provide right-sized acceleration for multiple workloads including multimodal agentic AI, industrial digitalization with NVIDIA Omniverse libraries, scientific simulation and visual computing. This flexibility extends from the cloud to the edge with Azure Local , enabling powerful sovereign AI solutions while bringing low-latency, real-time AI to wherever data needs to reside. This allows enterprises to seamlessly develop, deploy and manage AI-powered digital twins and generative AI applications with NVIDIA RTX PRO 6000 Blackwell GPUs from the Azure cloud directly to their factory floors, on-premises data centers or secure edge locations. Software Optimizations Deliver a Fungible AI Fleet The NVIDIA platform on Azure, spanning NVIDIA Blackwell and Hopper GPUs, accelerates the latest models from the Microsoft AI Superintelligence Team, including text (MAI-1-preview), real-time voice (MAI-Voice-1) and high-fidelity image generation (MAI-Image-1) â€” bringing new multimodal experiences across Bing Image Creator and Microsoft Copilot. Central to NVIDIAâ€™s collaboration with Microsoft is building a fungible fleet â€” a flexible, continuously modernized infrastructure that can accelerate any workload with maximum efficiency. This is achieved through continuous, full-stack software optimizations that deliver compounding performance gains and maximize throughput across the entire AI lifecycle and across multiple NVIDIA architectures on Azure. The gains also extend to workloads beyond generative AI, including data processing, vector search, databases, digital twins, scientific computing and 3D design. This co-engineering saves significant costs for customers, making AI projects that were once theoretical now economically viable. For example, the continuous full-stack optimization work has directly contributed to an over 90% drop in the price of popular GPT models for end users on Azure in two years. Ongoing optimization work now extends to Microsoft Foundry , where the NVIDIA TensorRT-LLM library helps boost throughput, reduce latency and lower costs for a wide range of popular open models. NVIDIA and Microsoft have also partnered to optimize their fleet for AI workload performance through the NVIDIA DGX Cloud Benchmarking suite. Engineering teams from both companies worked closely together to identify bottlenecks and implement infrastructure tuning, driving performance gains. By achieving 95% of the performance possible using the NVIDIA reference architecture, Microsoft was named an Exemplar Cloud for H100 training. From Intelligent Data to AI Agents NVIDIA and Microsoft are integrating AI into the core of the enterprise, unlocking decades of proprietary data stored in one of the worldâ€™s most trusted databases. NVIDIA is accelerating AI in the new Microsoft SQL Server 2025 by integrating it with NVIDIA Nemotron open models and NVIDIA NIM microservices. This solution delivers GPU-optimized, secure and scalable retrieval-augmented generation directly where enterprise data lives, in the cloud or on premises. Plus, the collaboration extends to the new frontier of agentic AI in the workplace. The NVIDIA NeMo Agent Toolkit now connects with Microsoft Agent 365 , enabling developers to build, deploy and onboard compliant, enterprise-ready AI agents directly into the Microsoft 365 app ecosystem, including Outlook, Teams, Word and SharePoint. To power these new enterprise agents, Microsoft Foundry now offers NVIDIA Nemotron models for digital AI and NVIDIA Cosmos models for physical AI as secure NIM microservices. Developers can use them to build enterprise-grade agentic AI for a vast range of applications that benefit from multimodal intelligence, multilingual reasoning, math, coding and physical AI capabilities. The collaboration is also tackling cyber threats for enterprises. Microsoft and NVIDIA are collaborating on research for new adversarial learning models , built on the NVIDIA Dynamo-Triton framework and the NVIDIA TensorRT suite of tools, that can help enterprises defend against real-time cybersecurity threats with a 160x performance speedup compared with CPU methods. Physical AI and Industrial Digitalization NVIDIA and Microsoft are building the future of physical AI . With NVIDIA Omniverse libraries available on Microsoft Azure, NVIDIA is unlocking end-to-end reindustrialization in the cloud through its developer ecosystem. Developers are transforming industrial workflows, from computer-aided engineering with Synopsys to factory operations with Sight Machine and SymphonyAI . Robotics developers can tap into the NVIDIA Isaac Sim open-source robotics simulation framework to unlock critical workflows, from synthetic data generation to software-in-the-loop testing for all types of robot embodiments. Hexagon is building its AEON humanoid robot primarily using NVIDIAâ€™s full robotics stack on Azure. Similarly, the robotics platform, Wandelbots NOVA , running on Azure integrates Isaac Sim and Isaac Lab to simplify and speed up simulation to real-world deployment. In addition, NVIDIA and Microsoft are using a standardized approach for digital engineering to enable seamless OpenUSD interoperability across 3D workflows, making simulation and digital content creation accessible in the cloud. This expanded collaboration comes on the heels of a partnership announced with Anthropic and Microsoft earlier today. NVIDIA and Anthropic will collaborate on design and engineering to optimize Anthropic models for performance, efficiency and total cost of ownership, as well as optimize future NVIDIA architectures for Anthropic workloads. Learn more about NVIDIA and Microsoftâ€™s collaboration and sessions at Microsoft Ignite . Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | DGX Cloud | Digital Twin | Industrial and Manufacturing | Nemotron | NVIDIA Blackwell | NVIDIA Isaac Sim | NVIDIA NeMo | NVIDIA NIM | NVIDIA Omniverse | NVIDIA RTX | NVIDIA Spectrum-X]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 18 Nov 2025 20:00:22 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</guid>
    </item>
  </channel>
</rss>
