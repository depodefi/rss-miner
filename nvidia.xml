<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Thu, 11 Dec 2025 10:04:41 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Opt-In NVIDIA Software Enables Data Center Fleet Management</title>
      <link>https://blogs.nvidia.com/blog/optional-data-center-fleet-management-software/</link>
      <description><![CDATA[As the scale and complexity of AI infrastructure grows, data center operators need continuous visibility into factors including performance, temperature and power usage. These insights enable data center operators to actively monitor and adjust data center configurations across large-scale, distributed systems â€” validating that these systems are operating at their highest efficiency and reliability. NVIDIA is developing a software solution for visualizing and monitoring fleets of NVIDIA GPUs â€” giving cloud partners and enterprises an insights dashboard that can help them boost GPU uptime across computing infrastructures. The offering is an opt-in, customer-installed service that monitors GPU usage, configuration and errors. It will include an open-source client software agent â€” part of NVIDIAâ€™s ongoing support of open, transparent software that helps customers get the most from their GPU-powered systems. With the service, data center operators will be able to: Track spikes in power usage to keep within energy budgets while maximizing performance per watt. Monitor utilization, memory bandwidth and interconnect health across the fleet. Detect hotspots and airflow issues early to avoid thermal throttling and premature component aging. Confirm consistent software configurations and settings to ensure reproducible results and reliable operation. Spot errors and anomalies to identify failing parts early. These capabilities can help enterprises and cloud providers visualize their GPU fleet, address system bottlenecks and optimize productivity for higher return on investment. This optional service provides real-time monitoring by each GPU system communicating and sharing GPU metrics with the external cloud service. NVIDIA GPUs do not have hardware tracking technology, kill switches and backdoors . Open-Source Agent Offers Insights for Data Center Owners The service will feature a client software agent that the customer can install to stream node-level GPU telemetry data to a portal hosted on NVIDIA NGC . Customers will be able to visualize their GPU fleet utilization in a dashboard, globally or by compute zones â€” groups of nodes enrolled in the same physical or cloud locations. The dashboard provides insight into GPU status across a customerâ€™s global fleet. The client tooling agent is also slated to be open sourced, providing transparency and auditability. Itâ€™ll offer a working example for how customers can incorporate NVIDIA tools into their own solutions for monitoring GPU infrastructure â€” whether for critical compute clusters or entire fleets. The software provides insight into a companyâ€™s GPU inventory but cannot modify GPU configurations or underlying operations. It provides read-only telemetry data thatâ€™s customer managed and customizable. The service will also enable customers to generate reports that detail GPU fleet information. As AI applications grow in number and complexity, modern AI infrastructure management is evolving to keep pace. Making sure that AI data centers are running at peak health is vital as AI revolutionizes every industry and application. This software service is here to help. Register for NVIDIA GTC , taking place March 16-19 in San Jose, California, to learn more. See notice regarding software product information. Categories: Corporate | Software Tags: Cybersecurity]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 10 Dec 2025 23:49:26 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/optional-data-center-fleet-management-software/</guid>
    </item>
    <item>
      <title>How NVIDIA H100 GPUs on CoreWeaveâ€™s AI Cloud Platform Delivered a Record-Breaking Graph500 Run</title>
      <link>https://blogs.nvidia.com/blog/h100-coreweave-graph500/</link>
      <description><![CDATA[The worldâ€™s top-performing system for graph processing at scale was built on a commercially available cluster. NVIDIA last month announced a record-breaking benchmark result of 410 trillion traversed edges per second (TEPS), ranking No. 1 on the 31st Graph500 breadth-first search (BFS) list. Performed on an accelerated computing cluster hosted in a CoreWeave data center in Dallas, the winning run used 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. This result is more than double the performance of comparable solutions on the list, including those hosted in national labs. To put this performance in perspective, say every person on Earth has 150 friends. This would represent 1.2 trillion edges in a graph of social relationships. The level of performance recently achieved by NVIDIA and CoreWeave enables searching through every friend relationship on Earth in just about three milliseconds. Speed at that scale is half the story â€” the real breakthrough is efficiency. A comparable entry in the top 10 runs of the Graph500 list used about 9,000 nodes, while the winning run from NVIDIA used just over 1,000 nodes, delivering 3x better performance per dollar. NVIDIA tapped into the combined power of its full-stack compute, networking and software technologies â€” including the NVIDIA CUDA platform, Spectrum-X networking, H100 GPUs and a new active messaging library â€” to push the boundaries of performance while minimizing hardware footprint. By saving significant time and costs at this scale in a commercially available system, the win demonstrates how the NVIDIA computing platform is ready to democratize access to acceleration of the worldâ€™s largest sparse, irregular workloads â€” involving data and work items that come in varying and unpredictable sizes â€” in addition to dense workloads like AI training. How Graphs at Scale Work Graphs are the underlying information structure for modern technology. People interact with them on social networks and banking apps, among other use cases, every day. Graphs capture relationships between pieces of information in massive webs of information. For example, consider LinkedIn. A userâ€™s profile is a vertex. Connections or relationships to other users are edges â€” with other users represented as vertices. Some users have five connections, others have 50,000. This creates variable density across the graph, making it sparse and irregular. Unlike an image or language model, which is structured and dense, a graph is unpredictable. Graph500 BFS has a long history as the industry-standard benchmark because it measures a systemâ€™s ability to navigate this irregularity at scale. BFS measures the speed of traversing the graph through every vertex and edge. A high TEPS score for BFS â€” measuring how fast the system can process these edges â€” proves the system has superior interconnects, such as cables or switches between compute nodes, as well as more memory bandwidth and software able to take advantage of the systemâ€™s capabilities. It validates the engineering of the entire system, not just the speed of the CPU or GPU. Effectively, itâ€™s a measure of how fast a system can â€œthinkâ€ and associate disparate pieces of information. Current Techniques for Processing Graphs GPUs are known for accelerating dense workloads like AI training. Until recently, the largest sparse linear algebra and graph workloads have remained the domain of traditional CPU architectures. To process graphs, CPUs move graph data across compute nodes. As the graph scales to trillions of edges, this constant movement creates bottlenecks and jams communications. Developers use a variety of software techniques to circumvent this issue. A common approach is to process the graph where it is with active messages, where developers send messages that can process graph data in place. The messages are smaller and can be grouped together to maximize network efficiency. While this software technique significantly accelerates processing, active messaging was designed to run on CPUs and is inherently limited by the throughput rate and compute capabilities of CPU systems. Reengineering Graph Processing for the GPU To speed up the BFS run, NVIDIA engineered a full-stack, GPU-only solution that reimagines how data moves across the network. A custom software framework developed using InfiniBand GPUDirect Async (IBGDA) and the NVSHMEM parallel programming interface enables GPU-to-GPU active messages. With IBGDA, the GPU can directly communicate with the InfiniBand network interface card. Message aggregation has been engineered from the ground up to support hundreds of thousands of GPU threads sending active messages simultaneously, compared with just hundreds of threads on a CPU. As such, in this redesigned system, active messaging runs completely on GPUs, bypassing the CPU. This enables taking full advantage of the massive parallelism and memory bandwidth of NVIDIA H100 GPUs to send messages, move them across the network and process them on the receiver. Running on the stable, high-performance infrastructure of NVIDIA partner CoreWeave, this orchestration enabled doubling the performance of comparable runs while using a fraction of the hardware â€” at a fraction of the cost. NVIDIA submission run on CoreWeave cluster with 8,192 H100 GPUs tops the leaderboard on the 31st Graph500 breadth-first search list. Accelerating New Workloads This breakthrough has massive implications for high-performance computing. HPC fields like fluid dynamics and weather forecasting rely on similar sparse data structures and communication patterns that power the graphs that underpin social networks and cybersecurity. For decades, these fields have been tethered to CPUs at the largest scales, even as data scales from billions to trillions of edges. NVIDIAâ€™s winning result on Graph500, alongside two other top 10 entries, validates a new approach for high-performance computing at scale. With the full-stack orchestration of NVIDIA computing, networking and software, developers can now use technologies like NVSHMEM and IBGDA to efficiently scale their largest HPC applications, bringing supercomputing performance to commercially available infrastructure. Stay up to date on the latest Graph500 benchmarks and learn more about NVIDIA networking technologies . Categories: Data Center | Networking Tags: Cloud Services | CUDA | Hardware | NVIDIA Hopper Architecture | Supercomputing]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 10 Dec 2025 20:56:53 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/h100-coreweave-graph500/</guid>
    </item>
    <item>
      <title>3 Ways NVIDIA Is Powering the Industrial Revolution</title>
      <link>https://blogs.nvidia.com/blog/gpu-cuda-scaling-laws-industrial-revolution/</link>
      <description><![CDATA[The NVIDIA accelerated computing platform is leading supercomputing benchmarks once dominated by CPUs, enabling AI, science, business and computing efficiency worldwide. Mooreâ€™s Law has run its course, and parallel processing is the way forward. With this evolution, NVIDIA GPU platforms are now uniquely positioned to deliver on the three scaling laws â€” pretraining, post-training and test-time compute â€” for everything from next-generation recommender systems and large language models (LLMs) to AI agents and beyond. How NVIDIA has transformed the foundation of computing AI pretraining, post-training and inference are driving the frontier How hyperscalers are using AI to transform search and recommender systems The CPU-to-GPU Transition: A Historic Shift in Computing ðŸ”— At SC25, NVIDIA founder and CEO Jensen Huang highlighted the shifting landscape. Within the TOP100, a subset of the TOP500 list of supercomputers, over 85% of systems use GPUs. This flip represents a historic transition from the serialâ€‘processing paradigm of CPUs to massively parallel accelerated architectures. Before 2012, machine learning was based on programmed logic. Statistical models were used and ran efficiently on CPUs as a corpus of hard-coded rules. But this all changed when AlexNet running on gaming GPUs demonstrated image classification could be learned by examples. Its implications were enormous for the future of AI, with parallel processing on increasing sums of data on GPUs driving a new wave of computing. This flip isnâ€™t just about hardware. Itâ€™s about platforms unlocking new science. GPUs deliver far more operations per watt, making exascale practical without untenable energy demands. Recent results from the Green500 , a ranking of the worldâ€™s most energy-efficient supercomputers, underscore the contrast between GPUs versus CPUs. The top five performers in this industry standard benchmark were all NVIDIA GPUs, delivering an average of 70.1 gigaflops per watt. Meanwhile, the top CPU-only systems provided 15.5 flops per watt on average. This 4.5x differential between GPUs versus CPUs on energy efficiency highlights the massive TCO (total cost of ownership) advantage of moving these systems to GPUs. Another measure of the CPU-versus-GPU energy-efficiency and performance differential arrived with NVIDIAâ€™s results on the Graph500. NVIDIA delivered a record-breaking result of 410 trillion traversed edges per second, placing first on the Graph500 breadth-first search list. The winning run more than doubled the next highest score and utilized 8,192 NVIDIA H100 GPUs to process a graph with 2.2 trillion vertices and 35 trillion edges. That compares with the next best result on the list, which required roughly 150,000 CPUs for this workload. Hardware footprint reductions of this scale save time, money and energy. Yet NVIDIA showcased at SC25 that its AI supercomputing platform is far more than GPUs. Networking, CUDA libraries, memory, storage and orchestration are co-designed to deliver a full-stack platform. Enabled by CUDA, NVIDIA is a full-stack platform. Open-source libraries and frameworks such as those in the CUDA-X ecosystem are where big speedups occur. Snowflake recently announced an integration of NVIDIA A10 GPUs to supercharge data science workflows. Snowflake ML now comes preinstalled with NVIDIA cuML and cuDF libraries to accelerate popular ML algorithms with these GPUs. With this native integration, Snowflakeâ€™s users can easily accelerate model development cycles with no code changes required. NVIDIAâ€™s benchmark runs show 5x less time required for Random Forest and up to 200x for HDBSCAN on NVIDIA A10 GPUs compared with CPUs. The flip was the turning point. The scaling laws are the trajectory forward. And at every stage, GPUs are the engine driving AI into its next chapter. But CUDA-X and many open-source software libraries and frameworks are where much of the magic happens. CUDA-X libraries accelerate workloads across every industry and application â€” engineering, finance, data analytics, genomics, biology, chemistry, telecommunications, robotics and much more. â€œThe world has a massive investment in non-AI software. From data processing to science and engineering simulations, representing hundreds of billions of dollars in compute cloud computing spend each year,â€ Huang said on NVIDIAâ€™s recent earning call. Many applications that once ran exclusively on CPUs are now rapidly shifting to CUDA GPUs. â€œAccelerated computing has reached a tipping point. AI has also reached a tipping point and is transforming existing applications while enabling entirely new ones,â€ he said. What began as an energyâ€‘efficiency imperative has matured into a scientific platform: simulation and AI fused at scale. The leadership of NVIDIA GPUs in the TOP100 is both proof of this trajectory and a signal of what comes next â€” breakthroughs across every discipline. As a result, researchers can now train trillionâ€‘parameter models, simulate fusion reactors and accelerate drug discovery at scales CPUs alone could never reach. The Three Scaling Laws Driving AIâ€™s Next Frontier ðŸ”— The change from CPUs to GPUs is not just a milestone in supercomputing. Itâ€™s the foundation for the three scaling laws that represent the roadmap for AIâ€™s next workflow: pretraining, postâ€‘training and testâ€‘time scaling. Preâ€‘training scaling was the first law to assist the industry. Researchers discovered that as datasets, parameter counts and compute grew, model performance improved predictably. Doubling the data or parameters meant leaps in accuracy and versatility. On the latest MLPerf Training industry benchmarks, the NVIDIA platform delivered the highest performance on every test and was the only platform to submit on all tests. Without GPUs, the â€œbigger is betterâ€ era of AI research would have stalled under the weight of power budgets and time constraints. Postâ€‘training scaling extends the story. Once a foundation model is built, it must be refined â€” tuned for industries, languages or safety constraints. Techniques like reinforcement learning from human feedback, pruning and distillation require enormous additional compute. In some cases, the demands rival preâ€‘training itself. This is like a student improving after basic education. GPUs again provide the horsepower, enabling continual fineâ€‘tuning and adaptation across domains. Testâ€‘time scaling, the newest law, may prove the most transformative. Modern models powered by mixture-of-experts architectures can reason, plan and evaluate multiple solutions in real time. Chainâ€‘ofâ€‘thought reasoning, generative search and agentic AI demand dynamic, recursive compute â€” often exceeding pretraining requirements. This stage will drive exponential demand for inference infrastructure â€” from data centers to edge devices. Together, these three laws explain the demand for GPUs for new AI workloads. Pretraining scaling has made GPUs indispensable. Postâ€‘training scaling has reinforced their role in refinement. Testâ€‘time scaling is ensuring GPUs remain critical long after training ends. This is the next chapter in accelerated computing: a lifecycle where GPUs power every stage of AI â€” from learning to reasoning to deployment. Generative, Agentic, Physical AI and Beyond ðŸ”— The world of AI is expanding far beyond basic recommenders, chatbots and text generation. VLMs, or vision language models, are AI systems combining computer vision and natural language processing for understanding and interpreting images and text. And recommender systems â€” the engines behind personalized shopping, streaming and social feeds â€” are but one of many examples of how the massive transition from CPUs to GPUs is reshaping AI. Meanwhile, generative AI is transforming everything from robotics and autonomous vehicles to software-as-a-service companies and represents a massive investment in startups. NVIDIA platforms are the only to run on all of the leading generative AI models and handle 1.4 million open-source models. Once constrained by CPU architectures, recommender systems struggled to capture the complexity of user behavior at scale. With CUDA GPUs, pretraining scaling enables models to learn from massive datasets of clicks, purchases and preferences, uncovering richer patterns. Postâ€‘training scaling fineâ€‘tunes those models for specific domains, sharpening personalization for industries from retail to entertainment. On leading global online sites, even a 1% gain in relevance accuracy of recommendations can yield billions more in sales. Electronic commerce sales are expected to reach $6.4 trillion worldwide for 2025, according to Emarketer . The worldâ€™s hyperscalers, a trillion-dollar industry, are transforming search, recommendations and content understanding from classical machine learning to generative AI. NVIDIA CUDA excels at both and is the ideal platform for this transition driving infrastructure investment measured in hundreds of billions of dollars. Now, testâ€‘time scaling is transforming inference itself: recommender engines can reason dynamically, evaluating multiple options in real time to deliver contextâ€‘aware suggestions. The result is a leap in precision and relevance â€” recommendations that feel less like static lists and more like intelligent guidance. GPUs and scaling laws are turning recommendation from a background feature into a frontline capability of agentic AI, enabling billions of people to sort through trillions of things on the internet with an ease that would otherwise be unfeasible. What began as conversational interfaces powered by LLMs is now evolving into intelligent, autonomous systems poised to reshape nearly every sector of the global economy. We are experiencing a foundational shift â€” from AI as a virtual technology to AI entering the physical world. This transformation demands nothing less than explosive growth in computing infrastructure and new forms of collaboration between humans and machines. Generative AI has proven capable of not just creating new text and images, but code, designs and even scientific hypotheses. Now, agentic AI is arriving â€” systems that perceive, reason, plan and act autonomously. These agents behave less like tools and more like digital colleagues, carrying out complex, multistep tasks across industries. From legal research to logistics, agentic AI promises to accelerate productivity by serving as autonomous digital workers. Perhaps the most transformative leap is physical AI â€” the embodiment of intelligence in robots of every form. Three computers are required to build physical AI-embodied robots â€” NVIDIA DGX GB300 to train the reasoning vision-language action model, NVIDIA RTX PRO to simulate, test and validate the model in a virtual world built on Omniverse, and Jetson Thor to run the reasoning VLA at real-time speed. Whatâ€™s expected next is a breakthrough moment for robotics within years, with autonomous mobile robots, collaborative robots and humanoids disrupting manufacturing, logistics and healthcare. Morgan Stanley estimates there will be 1 billion humanoid robots with $5 trillion in revenue by 2050. Signaling how deeply AI will embed into the physical economy, thatâ€™s just a sip of whatâ€™s on tap. NVIDIA CEO Jensen Huang stands on stage with a lineup of nine advanced humanoid robots during his keynote address at the GTC DC 2025 conference. The robots, including models from Boston Dynamics, Figure, Agility Robotics, and Disney Research, were brought together to showcase NVIDIAâ€™s new Project GR00T, a general-purpose foundation model aimed at advancing the capabilities of humanoid robots and artificial intelligence. AI is no longer just a tool. It performs work and stands to transform every one of the worldâ€™s $100 trillion in markets. And a virtuous cycle of AI has arrived, fundamentally changing the entire computing stack, transitioning all computers into new supercomputing platforms for vastly larger opportunities.â€‹ Categories: Corporate Tags: CUDA | CUDA-X | NVIDIA GPU | Physical AI | SC25]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 10 Dec 2025 18:35:05 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/gpu-cuda-scaling-laws-industrial-revolution/</guid>
    </item>
    <item>
      <title>NVIDIA Awards up to $60,000 Research Fellowships to PhD Students</title>
      <link>https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</link>
      <description><![CDATA[For 25 years, the NVIDIA Graduate Fellowship Program has supported graduate students doing outstanding work relevant to NVIDIA technologies. Today, the program announced the latest awards of up to $60,000 each to 10 Ph.D. students involved in research that spans all areas of computing innovation. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding the fellowship year. Their work puts them at the forefront of accelerated computing â€” tackling projects in autonomous systems, computer architecture, computer graphics, deep learning, programming systems, robotics and security. The NVIDIA Graduate Fellowship Program is open to applicants worldwide. The 2026-2027 fellowship recipients are: Jiageng Mao , University of Southern California â€” Solving complex physical AI problems by using diverse priors from internet-scale data to enable robust, generalizable intelligence for embodied agents in the real world. Liwen Wu , University of California San Diego â€” Enriching realism and efficiency in physically based rendering with neural materials and neural rendering. Manya Bansal , Massachusetts Institute of Technology â€” Designing programming languages for modern accelerators that enable developers to write modular, reusable code without sacrificing the low-level control required for peak performance. Sizhe Chen , University of California, Berkeley â€” Securing AI in real-world applications, currently securing AI agents against prompt injection attacks with general and practical defenses that preserve the agentâ€™s utility. Yunfan Jiang , Stanford University â€” Developing scalable approaches to build generalist robots for everyday tasks through hybrid data sources spanning real-world whole-body manipulation, large-scale simulation and internet-scale multimodal supervision. Yijia Shao , Stanford University â€” Researching human-agent collaboration by developing AI agents that can communicate and coordinate with humans during task execution, and designing new human-agent interaction interfaces. Shangbin Feng , University of Washington â€” Advancing model collaboration: multiple machine learning models, trained on different data and by different people, collaborate, compose and complement each other for an open, decentralized and collaborative AI future. Shvetank Prakash , Harvard University â€” Advancing hardware architecture and systems design with AI agents built on new algorithms, curated datasets and agent-first infrastructure. Irene Wang , Georgia Institute of Technology â€” Developing a holistic codesign framework that integrates accelerator architecture, network topology and runtime scheduling to enable energy-efficient and sustainable AI training at scale. Chen Geng , Stanford University â€” Modeling 4D physical worlds with scalable data-driven algorithms and physics-inspired principles, advancing physically grounded 3D and 4D world models for robotics and scientific applications. We also acknowledge the 2026-2027 fellowship finalists: Zizheng Guo , Peking University Peter Holderrieth , Massachusetts Institute of Technology Xianghui Xie , Max Planck Institute for Informatics Alexander Root , Stanford University Daniel Palenicek , Technical University of Darmstadt Categories: Generative AI | Research Tags: Artificial Intelligence | Education]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 17:00:44 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</guid>
    </item>
    <item>
      <title>Robotsâ€™ Holiday Wishes Come True: NVIDIA Jetson Platform Offers High-Performance Edge AI at Festive Prices</title>
      <link>https://blogs.nvidia.com/blog/jetson-edge-ai-holiday-2025/</link>
      <description><![CDATA[Developers, researchers, hobbyists and students can take a byte out of holiday shopping this season as NVIDIA has unwrapped special discounts on the NVIDIA Jetson family of developer kits for edge AI and robotics â€” available through Sunday, Jan. 11. Whether tapping into the breakthrough capabilities of Jetson AGX Thor, the versatility of Jetson AGX Orin or the palm-sized power of the NVIDIA Jetson Orin Nano Super Developer Kit, anyone can deck out their bots with greater physical AI performance for lower costs: The Jetson AGX Thor Developer Kit â€” now 20% off â€” is designed for building humanoid robots, fleets of autonomous machines and multimodal physical AI agents. It delivers server-class compute and generative AI capabilities for the most challenging workloads in labs, factories and the field. The Jetson AGX Orin Developer Kit â€” now 50% off â€” powers advanced robots, autonomous machines and generative AI at the edge with 275 trillion operations per second (TOPS) of AI performance, ideal for delivery bots, smart vision capabilities and industrial automation. The Jetson Orin Nano Super Developer Kit â€” the worldâ€™s most affordable generative AI supercomputer â€” offers desktop-class AI in a palm-sized kit for exploring, building and deploying cutting-edge generative AI, vision and robotics. Read more below on how the NVIDIA Jetson platform presents the future of robotics. NVIDIA Jetson Orin Nano Super Serves as Brain of Self-Paddling Canoe Robotics enthusiast Dave Niewinski has built a selfâ€‘paddling canoe using the Jetson Orin Nano Super Developer Kit, letting boaters relax and glide on their rides. The robotic boat integrates two six-axis robotic arms mounted on a lightweight canoe frame, with paddle motion controlled through ROS software and AI algorithms. The Jetson Orin Nano Super delivers up to 67 INT8 TOPS of AI performance through its NVIDIA Ampere architecture GPU, 32 Tensor Cores and 1,024 CUDA cores, alongside a six-core Arm Cortexâ€‘A78AE CPU and 8GB LPDDR5 memory with 102 GB/s bandwidth. Its low power envelope of up to 25 watts enables sustained real-time inference and control in mobile, battery-powered applications. NVIDIA AGX Orin Dives Into Open Seas on Underwater AI System OptoScale, an AI aquaculture company based in Norway, has integrated the Jetson AGX Orin-powered MX13/23 platform from edge AI solutions provider Aetina to build an underwater AI sensing system that monitors fish health in massive open-sea pens housing thousands of fish. Mounted inside a submerged camera module, the Jetson AGX Orin processes high-resolution video streams directly at the edge and runs real-time vision models on the device. This allows the system to estimate biomass with exceptional accuracy and deliver continuous inference-based insights even in remote environments with limited connectivity. The Jetson AGX Orin delivers powerful GPU acceleration and high TOPS AI performance in a compact, energy-efficient module ideal for deployment anywhere. NVIDIA Jetson AGX Thor Powers Mobile Humanoid â€˜Dexâ€™ Las Vegas-based Richtech Robotics is developing Dex, a mobile humanoid robot for factory and warehouse environments designed to handle light-to medium-weight industrial tasks like machine operation, parts sorting, material handling and packaging. https://blogs.nvidia.com/wp-content/uploads/2025/12/richtech-robot-arm-20MB.mp4 Running on NVIDIA Jetson AGX Thor, Dex combines the mobility of an autonomous wheeled platform with the precision of dual-arm dexterity, allowing it to efficiently navigate environments and pick and place objects. It was trained with a mix of real-world and synthetic data generated from NVIDIA Isaac Sim . NVIDIA Jetson Thor modules enable real-time reasoning for physical AI, delivering up to 2,070 FP4 teraflops of AI compute and 128GB of memory with power configurable between 40-130 watts. With discounted pricing for a limited time and a full spectrum of performance options, the NVIDIA Jetson family gives anyone the tools to design, build and deploy the next generation of intelligent machines. Itâ€™s the ideal gift for robot lovers â€” and the robots in their lives. Shop NVIDIA Jetson now . Categories: Robotics Tags: Artificial Intelligence | Creators | Embedded Computing | Hardware | Industrial and Manufacturing | Inference | Isaac | Jetson | Physical AI | Robotics | Simulation and Design | Synthetic Data Generation]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 16:00:07 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/jetson-edge-ai-holiday-2025/</guid>
    </item>
    <item>
      <title>Game the Halls: GeForce NOW Brings Holiday Cheer With 30 New Games in the Cloud</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-dec-2025/</link>
      <description><![CDATA[Editorâ€™s note: The Game Pass edition of â€˜Hogwarts Legacyâ€™ will also be supported on GeForce NOW when the Steam and Epic Games Store versions launch on the service later this month. GeForce NOW is decking the digital halls with 30 new games to keep spirits high all month long. Join the fun with Hogwarts Legacy , the LEGO Harry Potter Collection and a sleighful of new adventures streaming straight from the cloud. The â€œHalf-Price Holidayâ€ sale keeps the savings rolling after Black Friday, with premium GeForce NOW memberships available at 50% off for the first month for a limited time. And GeForce NOW is bringing members a new way to jump into the worlds of some of the most iconic games. Stream select Activision titles through Ubisoft+ Premium in the cloud for GeForce RTX power â€” including Call of Duty: Modern Warfare II , Call of Duty: Modern Warfare III , Crash Bandicoot N. Sane Trilogy and the Spyro Reignited Trilogy , with more to come. Thatâ€™s not all: Battle.net single sign-on is now live, making it easier than ever for members to leap into Overwatch 2, Diablo IV and other titles without multiple logins. Link once and play instantly, no matter the device. Plus, the GeForce NOW Community Video Contest is rolling on â€” creators are decking the halls with epic clips and their merriest cloud gaming moments from all over the world. Thereâ€™s still time to jump in with a clip of a favorite game or a memorable match. Submit a clip and score two Ultimate day passes â€” one to share with a friend and one to keep â€” plus a chance to win a one-year Ultimate membership. Half-Price Holiday Wrap up the year with half off. The holiday season continues with more ways to play. Even after Black Friday, GeForce NOW is keeping the celebration going with its â€œHalf-Price Holidayâ€ offer. For a limited time, premium memberships are half off for the first month. Performance and Ultimate tiers unlock shorter queue times, longer gaming sessions and access to more than 2,000 additional Install-to-Play titles powered by GeForce RTX in the cloud. The â€œHalf-Price Holidayâ€ sale is the perfect opportunity to experience premium gaming at a fraction of the cost and keep the winter gaming streak alive. The sale wraps up on Tuesday, Dec. 30, while supplies last, so nowâ€™s the time to lock in that first month at half off and head into the new year with the GeForce NOW premium experience. Auto Sign-In, Auto Win One login, endless worlds. GeForce NOW is making it easier than ever for members to jump into games from Blizzard Entertainment, Activision and more. Starting today, members can link their Battle.net accounts directly to GeForce NOW for automatic single sign-on across all supported devices. After a quick one-time setup, members are automatically logged in for future cloud gaming sessions â€” no extra steps, no password juggling, just instant access to Battle.net favorites like Overwatch 2 and Diablo IV . The update expands on existing automatic login support for Xbox, Epic Games and Ubisoft, further streamlining the GeForce NOW cloud gaming experience. Whether at home or on the go, members can enjoy faster, simpler, smoother access to the games they love. A Very GeForce NOW December Eight travelers, one very GeForce NOW cloud. Square Enixâ€™s Octopath Traveler 0 brings the seriesâ€™ signature wanderlust and quiet drama back to Orsterra, this time with a fresh cast and a few extra tricks up each travelerâ€™s sleeve. The prequel leans into sharp character banter, devious Path Actions and choices that can charm, swindle or strong-arm just about anyone in the way. Expect rich pixel art, big feelings and plenty of scheming as players embark on an adventure of their own creation, navigating revenge and restoration. MARVEL Cosmic Invasion (New release on Steam and Xbox , available on Game Pass, Dec. 1) Call of Duty: Modern Warfare II (New release on Ubisoft , Dec. 2) Call of Duty: Modern Warfare III (New release on Ubisoft , Dec. 2) Crash Bandicoot N. Sane Trilogy (New release on Ubisoft , Dec. 2) XOCIETY (New release on Epic Games Store , Dec. 2) Spyro Reignited Trilogy (New release on Ubisoft , Dec. 2) Lost Records: Bloom & Rage (New release on Xbox , available on Game Pass, Dec. 2) OCTOPATH TRAVELER 0 (New release on Steam , Dec. 4) ROUTINE (New release on Steam and Xbox , available on Game Pass, Dec. 4) MIMESIS ( Steam ) GeForce RTX 5080-ready games: Enshrouded ( Steam ) Fallout 76 ( Steam and Xbox , available on Game Pass) Catch the full list of games coming to the cloud in December: Dome Keeper (New release on Xbox , available on Game Pass, Dec. 9) Death Howl (New release on Steam and Xbox , available on Game Pass, Dec. 9) Everdream Village (New release on Steam , Dec. 12) ARC Raiders ( Epic Games Store ) Dying Light: The Beast ( Epic Games Store ) Citizen Sleeper ( Steam ) For the King II ( Steam ) Jurassic World Evolution 3 ( Epic Games Store ) Hogwarts Legacy ( Steam, Epic Games Store , and Xbox , available on Game Pass) LEGO Harry Potter Collection ( Steam ) Lara Croft and the Temple of Osiris ( Xbox , available on Game Pass) Pigeon Simulator ( Xbox , available on Game Pass) Pacific Drive ( Xbox , available on Game Pass) Powerwash Simulator 2 (Steam) Shape of Dreams ( Steam ) Storage Hunter Simulator ( Steam ) Sword of the Sea ( Steam ) Underground Garage ( Steam ) Warhammer 40,000: SPACE MARINE 2 ( Epic Games Store ) Witchfire ( Epic Games Store ) â€˜Tis the Season for Extra Games In addition to the 23 games announced in November, an extra 10 joined over the month: Apollo Justice: Ace Attorney Trilogy ( Steam ) The Crew Motorfest ( Xbox , available on Game Pass) Cricket 26 ( Steam ) Kill It With Fire ( Xbox , available on PC Game Pass) Moonlighter 2: The Endless Vault ( Steam and Xbox , available on Game Pass) Of Ash and Steel ( Steam , GeForce RTX 5080-ready) Prologue: Go Wayback! ( Steam ) Sacred 2 Remaster ( Steam ) Songs of Silence ( Epic Games Store ) Zero Hour ( Epic Games Store ) To improve the overall quality of service for the most played games on GeForce NOW, members will see movement of games in the catalog. Some titles with little-to-no playtime that are currently available in the Ready-to-Play catalog will start moving to Install-to-Play on December 12. Premium members can continue to play these games as part of their Install-to-Play benefits. See this article for details. Some of the most popular Install-to-Play games â€” including Megabonk, R.E.P.O and RV There Yet? â€” have moved to Ready-to-Play, so theyâ€™ll always be kept up to date for instant streaming. What are you planning to play this weekend? Let us know on X or in the comments below. Health pots, extra weapons, alternate armor â€” what's an item you never remove from your inventory?ðŸŽ’ â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) December 3, 2025 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 14:00:37 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-dec-2025/</guid>
    </item>
    <item>
      <title>NVIDIA and AWS Expand Full-Stack Partnership, Providing the Secure, High-Performance Compute Platform Vital for Future Innovation</title>
      <link>https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</link>
      <description><![CDATA[At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI. As part of this expansion, AWS will support NVIDIA NVLink Fusion â€” a platform for custom AI infrastructure â€” for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure. Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities. AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion. AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms. AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems. By supporting AWSâ€™s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWSâ€™s cloud infrastructure and accelerating new AI service rollout. â€œGPU compute demand is skyrocketing â€” more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,â€ said Jensen Huang, founder and CEO of NVIDIA. â€œWith NVIDIA NVLink Fusion coming to AWS Trainium4, weâ€™re unifying our scale-up architecture with AWSâ€™s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution â€” bringing advanced AI to every company, in every country, and accelerating the worldâ€™s path to intelligence.â€ â€œAWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,â€ said Matt Garman, CEO of AWS. â€œWith NVIDIA, weâ€™re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.â€ Convergence of Scale and Sovereignty AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industryâ€™s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks. These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations. NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements. For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWSâ€™s industry-leading cloud infrastructure and services â€” known for its reliability, security and scalability â€” with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches. The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks. NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations Beyond hardware, the partnership expands integration of NVIDIAâ€™s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock , enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy. The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrockâ€™s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents. NVIDIA Software on AWS Simplifies Developer Experience NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS , an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost. These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when itâ€™s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs. Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production. This expanded support builds on AWSâ€™s existing integrations with NVIDIA technologies â€” including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo , as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock â€” that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever. Accelerating Physical AI With AWS Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment. NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS , enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation , Cosmos WFMs are also available on AWS Batch as containers. Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab . Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development. Sustained Collaboration Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats. Learn more about NVIDIA and AWSâ€™s collaboration and join sessions at AWS re:Invent , running through Friday, Dec. 5, in Las Vegas. Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Cosmos | Events | Isaac | Nemotron | NVIDIA Blackwell | NVIDIA NIM | NVIDIA Spectrum-X | NVLink | Open Source | Physical AI | Riva]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 02 Dec 2025 16:00:27 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</guid>
    </item>
    <item>
      <title>At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI</title>
      <link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link>
      <description><![CDATA[Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools â€” with potential applications in virtually every research field. At NeurIPS , one of the worldâ€™s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the worldâ€™s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety. NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more. These initiatives deepen NVIDIAâ€™s commitment to open source â€” an effort recognized by a new Openness Index from Artificial Analysis , an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details. NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving NVIDIA DRIVE Alpamayo-R1 (AR1) , the worldâ€™s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning â€” a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy . While previous iterations of self-driving models struggled with nuanced situations â€” a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane â€” reasoning gives autonomous vehicles the common sense to drive more like humans do. AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route. For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces â€” explanations on why it took certain actions â€” and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers. https://blogs.nvidia.com/wp-content/uploads/2025/12/construction_worker.mp4 AR1â€™s open foundation, based on NVIDIA Cosmos Reason , lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications. For post-training AR1, reinforcement learning has proven especially effective â€” researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model. NVIDIA DRIVE Alpamayo-R1 is now available on GitHub and Hugging Face , and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets . NVIDIA has also released the open-source AlpaSim framework to evaluate AR1. Learn more about reasoning VLA models for autonomous driving . Customizing NVIDIA Cosmos for Any Physical AI Use Case Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook . Itâ€™s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation. There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include: LidarGen , the first world model that can generate lidar data for AV simulation. Omniverse NuRec Fixer , a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data. Cosmos Policy , a framework for turning large pretrained video models into robust robot policies â€” a set of rules that dictate a robotâ€™s behavior. ProtoMotions3 , an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs) . Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen. Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics. Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM. NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs. AV developer Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X , Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation. NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include: MultiTalker Parakeet : An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations. Sortformer : A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream â€” a process called diarization â€” in real time. Nemotron Content Safety Reasoning : A reasoning-based AI safety model that dynamically enforces custom policies across domains. Nemotron Content Safety Audio Dataset : A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities. NeMo Gym : an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR). NeMo Data Designer Library : Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation. NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow. NeurIPS attendees can explore these innovations at the Nemotron Summit , taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. NVIDIA Research Furthers Language AI Innovation Of the dozens of NVIDIA-authored research papers at NeurIPS , here are a few highlights advancing language models: Audio Flamingo 3: Advancing Audio Intelligence With Fully Open Large Audio Language Models : This large audio language model is capable of reasoning across speech, sound and music. It can understand and reason audio segments up to 10 minutes in length, achieving state-of-the-art results on over 20 benchmarks. Minitron-SSM: Efficient Hybrid Language Model Compression Through Group-Aware SSM Pruning : This poster introduces a pruning method capable of compressing hybrid models, demonstrated by pruning and distilling Nemotron-H 8B from 8 billion to 4 billion parameters. The resulting model surpasses the accuracy of similarly sized models while achieving 2x faster inference throughput. Jet-Nemotron: Efficient Language Model With Post Neural Architecture Search : This work presents a cost-efficient post-training pipeline for developing new efficient language model architectures, and introduces a hybrid-architecture model family produced with the pipeline. These models match or surpass the accuracy of leading full-attention baselines while delivering substantially higher generation throughput. Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models : This project introduces a new small language model (SLM) architecture that redesigns SLMs around real-world latency rather than parameter count â€” achieving state-of-the-art speed and accuracy. ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models : Prolonged reinforcement learning, or ProRL, is a technique that extends model training over longer periods. In this NeurIPS poster, NVIDIA researchers describe how this methodology results in models that consistently outperform base models for reasoning. View the full list of events at NeurIPS , running through Sunday, Dec. 7, in San Diego. See notice regarding software product information. Categories: Corporate | Driving | Generative AI | Research | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | NVIDIA Research | Open Source | Physical AI | Synthetic Data Generation | Transportation]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 01 Dec 2025 17:00:48 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid>
    </item>
    <item>
      <title>From Government to Gaming, AI Is â€˜Strengthening Koreaâ€™s Digital Foundation,â€™ NVIDIA Leader Says at AI Day Seoul</title>
      <link>https://blogs.nvidia.com/blog/ai-day-seoul/</link>
      <description><![CDATA[Last week, more than 1,000 attendees joined NVIDIA AI Day Seoul to learn about sovereign AI â€” including breakout sessions on agentic and physical AI, hands-on workshops and a startupâ€¦Read Article]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 26 Nov 2025 17:00:32 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-day-seoul/</guid>
    </item>
    <item>
      <title>FLUX.2 Image Generation Models Now Released, Optimized for NVIDIA RTX GPUs</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</link>
      <description><![CDATA[Black Forest Labs â€” the frontier AI research lab developing visual generative AI models â€” today released the FLUX.2 family of state-of-the-art image generation models. FLUX.2 is packed with new tools and capabilities, including a multi-reference feature that can generate dozens of similar image variations, in photorealistic detail and with cleaner fonts â€” even at scale. NVIDIA has worked with Black Forest Labs and ComfyUI to make the models available with FP8 quantizations and RTX GPU performance optimizations at launch, decreasing the VRAM required to run them by 40% and improving performance by 40%. Requiring no special software package to run, the models are available directly in ComfyUI . State-of-the-Art Visual Intelligence Images generated by FLUX.2 are photorealistic, even at scale, featuring up to 4 megapixel resolution with real-world lighting and physics to eliminate that â€œAI lookâ€ that undermines visual fidelity. The models add direct pose control to explicitly specify the pose of a subject or character in an image, as well as deliver clean, readable text across infographics, user interface screens and even multilingual content. Plus, the new multi-reference feature enables artists to select up to six reference images where the style or subject stays consistent â€” eliminating the need for extensive model fine-tuning. Stunning, photorealistic details. Image courtesy of Black Forest Labs. For a complete overview of new FLUX.2 features, read Black Forest Labsâ€™ blog . Optimized for RTX The new FLUX.2 models are impressive, but also quite demanding. They run a staggering 32-billion-parameter model requiring 90GB VRAM to load completely. Even using lowVRAM mode â€” a popular setting that allows artists to only load the active model at a time â€” the VRAM requirement is still 64GB, which puts the model virtually out of reach for any consumer card to use effectively. To broaden FLUX.2 model accessibility, NVIDIA and Black Forest Labs collaborated to quantize the model to FP8 â€” reducing the VRAM requirements by 40% at comparable quality. FLUX.2 is here. And to make this model accessible on GeForce RTX GPUs, NVIDIA has partnered with ComfyUI â€” a popular application to run visual generative AI models on PC â€” to improve the appâ€™s RAM offload feature, known as weight streaming. Using the upgraded feature, users can offload parts of the model to system memory, extending the available memory on their GPUs â€” albeit with some performance loss, as system memory is slower than GPU memory. NVIDIA has also been collaborating with ComfyUI to optimize model performance on NVIDIA and GeForce RTX GPUs, including optimizations for FP8 checkpoints. Get started with FLUX.2 today. Update ComfyUI and check out the FLUX.2 templates, or visit Black Forest Labsâ€™ Hugging Face page to download the model weights. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . Categories: Generative AI Tags: Artificial Intelligence | Conversational AI | Creators | GeForce | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 25 Nov 2025 15:53:21 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-flux-2-comfyui/</guid>
    </item>
    <item>
      <title>AI On: 3 Ways Specialized AI Agents Are Reshaping Businesses</title>
      <link>https://blogs.nvidia.com/blog/specialized-ai-agents/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of the AI On blog series, which explores the latest techniques and real-world applications of agentic AI, chatbots and copilots. The series also highlights the NVIDIA software and hardware powering advanced AI agents, which form the foundation of AI query engines that gather insights and perform tasks to transform everyday experiences and reshape industries. As agentic AI adoption continues to grow, with open-source models and tools maturing, companies across industries are increasingly asking: what AI agents should we build to solve our unique business challenges? Although faster outcomes are a core benefit of using AI, organizations are finding that specialization is the key to business impact and long-term AI adoption. Rather than relying on one-size-fits-all models and services, leading companies are developing specialized AI agents designed to understand and act within the needs of a specific use case. CrowdStrike, PayPal and Synopsys are examples of companies combining NVIDIA Nemotron open foundation models with their proprietary data and institutional knowledge to create specialized applications. The results are intelligent agents that have the level of expertise required to work alongside human colleagues and boost business operations. 1. CrowdStrike Defends Against Modern Cyber Threats In cybersecurity, speed and precision are essential, especially as cyber threats become more advanced and grow to larger scales. To meet these rapidly evolving digital threats, CrowdStrike is building specialized AI agents that can work alongside security teams through Charlotte AI AgentWorks. These agents, powered by NVIDIA Nemotron open models and NVIDIA NIM microservices, automate high-volume tasks such as alert triage and remediation, allowing human analysts to focus on higher-order decision-making. Built on open models and continuously trained by incident responders, CrowdStrikeâ€™s Agentic Security Platform increases accuracy of alert triage from 80% to 98.5% , reducing security analyst teamsâ€™ manual effort tenfold. The platform can adapt to new risks and collaborates across the security operations center. 2. PayPalâ€™s AI Agents Power Frictionless Commerce at Scale PayPal, a leader in payments and e-commerce, is building agent-driven infrastructure to accelerate intelligent commerce . The companyâ€™s specialized AI agents, developed on Nemotron open models, will enable the first wave of conversational commerce experiences, where agents can shop, buy and pay on a userâ€™s behalf. With this approach, PayPal built a fine-tuning pipeline in two weeks and reduced latency by nearly 50% while maintaining the high accuracy required to serve its 430 million customers and 30 million merchants. PayPalâ€™s agents rely on open, modular models that are fine-tuned specifically for payments and commerce, giving the company the control to balance performance, accuracy and cost at a massive scale. 3. Synopsys Advances Agentic AI for Chip Design Workflows The complexity of modern semiconductor design and manufacturing calls for expertise, precision and speed. Synopsys is pioneering an agentic AI framework that can be deployed throughout the chip development workflow. Synopsysâ€™ vision for agentic AI includes Synopsys AgentEngineer technology that can significantly boost productivity in research and development, identifying critical design bugs and helping reduce costly delays that traditional techniques can miss. In early trials of a formal verification workflow, Synopsys AI agents running on NVIDIA accelerated infrastructure achieved a 72% boost in productivity. Using open models fine-tuned for each engineering task, as well as software like the NVIDIA NeMo Agent Toolkit and Blueprints, Synopsys is enabling a new frontier of AI-enabled chip design. Building Specialized AI Agents With NVIDIA Technologies Companies across industries are taking the following steps to transform their proprietary knowledge into specialized AI agents: Evaluate open models, like NVIDIA Nemotron , that provide a powerful building block to create specialized models for any domain. Curate, generate and secure domain data using NVIDIA NeMo for agent lifecycle management. Create specialized agents using customized models that have access to proprietary data. Continue to fine-tune agents over time with a data flywheel . Learn how NVIDIA Nemotron can help businesses build specialized AI agents for maximum productivity and return on investment. Categories: Data Center | Generative AI Tags: Agentic AI | AI On | Artificial Intelligence | Cybersecurity | Financial Services | Industrial and Manufacturing | Nemotron | NVIDIA Blueprints | NVIDIA NeMo | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 24 Nov 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/specialized-ai-agents/</guid>
    </item>
    <item>
      <title>Into the Omniverse: How Smart City AI Agents Transform Urban Operations</title>
      <link>https://blogs.nvidia.com/blog/smart-city-ai-agents-urban-operations/</link>
      <description><![CDATA[Editorâ€™s note: This post is part of Into the Omniverse , a series focused on how developers, 3D practitioners and enterprises can transform their workflows using the latest advancements in OpenUSD and NVIDIA Omniverse . Cities worldwide face unprecedented challenges as urban populations surge and infrastructure strains to keep pace. Operational challenges like traffic congestion and coordinating emergency services are compounded by fragmented data pipelines, siloed local government processes and disparate systems. Technical barriers prevent cities from accessing the comprehensive, real-time insights needed for effective decision-making and city management. Leading cities and technology partners are deploying the NVIDIA Blueprint for smart city AI , a reference application that provides the complete software stack to build, test and operate AI agents in simulation-ready ( SimReady ) digital twins. OpenUSD is an open and extensible framework that connects to each stage of this physical AI workflow. OpenUSD-enabled digital twins serve as SimReady environments where cities can simulate â€œwhat ifâ€ scenarios and generate physically accurate sensor data. The blueprint powers a three-stage workflow: 1) simulate with the NVIDIA Cosmos platform and NVIDIA Omniverse libraries to generate synthetic data, 2) train and fine-tune vision AI models, and 3) deploy real-time video analytics AI agents with the NVIDIA Metropolis platform and the NVIDIA Blueprint for video search and summarization (VSS). This enables cities to move from reactive to proactive operations.â€‹ Based on these simulations, cities can deploy operational platforms where weather data, traffic sensors and emergency response systems converge, supporting rapid testing of rare scenarios, real-time monitoring, city infrastructure planning and optimization of urban systems. From Kaohsiung City, Taiwan, cutting incident response times by 80% with street-level AI to Raleigh, North Carolina, achieving 95% vehicle detection accuracy and French rail networks optimizing energy consumption by 20%, cities across the globe are using digital twins and AI agents to transform urban operations at scale. Smart Cities in Action Akila, With SNCF Gares&Connexions, Uses Digital Twins to Improve Rail Operations Akilaâ€™s digital twin application helps French rail operator SNCF Gares&Connexions optimize its network of nearly 14,000 daily trains with live scenario planning for solar heating, air flow and crowd movement. The OpenUSD-enabled digital twins deliver a 20% reduction in energy consumption, 100% on-time preventive maintenance and a 50% reduction in downtime and response times. Linker Vision Taps Physical AI for Street-Level Intelligence Linker Visionâ€™s physical AI system recognizes infrastructure events in Kaohsiung City, including damaged streetlights and fallen trees, eliminating manual city inspections and enabling faster emergency response. To scale its street-level intelligence to more cities, Linker Vision uses Omniverse libraries for simulation, Cosmos Reason for world understanding and the VSS blueprint for deployment powered by OpenUSD. Esri and Microsoft Enable Comprehensive Urban Intelligence in the City of Raleigh The City of Raleigh achieved 95% vehicle detection accuracy using the NVIDIA DeepStream software development kit, boosting traffic analysis workflows for engineers. This data enhances Raleighâ€™s digital twin, enabled by Esriâ€™s ArcGIS geospatial platform to support visualization and analysis for critical infrastructure planning and management. Integrating this computer vision pipeline with a vision AI agent powered by the NVIDIA VSS blueprint provides comprehensive real-time visibility and insights in ArcGIS on Azure Cloud. Milestone Systemsâ€™ VLM Automates Video Review Milestone Systems is soon launching its Hafnia VLM, which will include a VLM plug-in for its video management software XProtect as well as a VLM-as-a-service. Fine-tuned on more than 75,000 hours of video data, the Hafnia VLM can reduce operator alarm fatigue by up to 30% by automating video review and filtering out false alarms. It was developed with NVIDIA Cosmos Reason VLMs and Metropolis. The Hafnia VLM plug-in for XProtect will make generative AI more easily accessible for XProtect operators and users. K2K Analyzes Italy Video Streams K2Kâ€™s platform uses NVIDIA Cosmos Reason and the VSS blueprint to analyze over 1,000 video streams in Palermo, Italy, processing 7 billion events annually and automatically notifying city officials through natural language queries and video events when critical conditions are extracted and analyzed. Learn more about how cities are transforming with simulation, vision AI and digital twins by watching this on-demand NVIDIA GTC session, â€œ Leadership Strategies to Transform Public Services .â€ Get Started With Smart City AI Learn more about OpenUSD and computer vision workflows through these resources: Watch this video on bringing physical AI to cities with the NVIDIA Blueprint for smart city AI. Read this technical blog on how to integrate computer vision pipelines with generative AI and reasoning with the VSS blueprint. Access step-by-step intelligent traffic system workflows and technical recipes in new cookbooks for NVIDIA Cosmos Predict , NVIDIA Cosmos Transfer and NVIDIA Cosmos Reason . Stay up to date by subscribing to NVIDIA Omniverse news , joining the Omniverse community and following Omniverse on Discord , Instagram , LinkedIn , Threads , X and YouTube . Categories: Generative AI | Pro Graphics Tags: Agentic AI | Artificial Intelligence | Computer Vision | Cosmos | DeepStream | Digital Twin | Into the Omniverse | Metropolis | Mobility | NVIDIA Blueprints | Omniverse | Physical AI | Simulation and Design | Smart Spaces | Synthetic Data Generation | Universal Scene Description | Visual Computing]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 20 Nov 2025 16:00:39 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/smart-city-ai-agents-urban-operations/</guid>
    </item>
    <item>
      <title>The Largest Digital Zoo: Biology Model Trained on NVIDIA GPUs Identifies Over a Million Species</title>
      <link>https://blogs.nvidia.com/blog/bioclip2-foundation-ai-model/</link>
      <description><![CDATA[Tanya Berger-Wolfâ€™s first computational biology project started as a bet with a colleague: that she could build an AI model capable of identifying individual zebras faster than a zoologist. She won. Now, the director of the Translational Data Analytics Institute and a professor at The Ohio State University, Berger-Wolf is taking on the whole animal kingdom with BioCLIP 2 , a biology-based foundation model trained on the biggest, most diverse dataset of organisms to date. The model will be showcased at this yearâ€™s NeurIPS AI research conference. BioCLIP 2 goes beyond extracting information from images. It can distinguish speciesâ€™ traits and determine inter-and intraspecies relationships. For example, the model arranged Darwinâ€™s finches by beak size, without teaching the concept of size, shown in the image below. Scatter plot shows how BioCLIP 2 arranges Darwinâ€™s finches by beak size from left to right. These capabilities will allow researchers to use the model as both a biological encyclopedia, a powerful scientific platform and an interactive research tool with inference capabilities to help address an ongoing issue in conservation biology: data deficiency for certain species. â€œFor iconic species like killer whales, we lack enough data to determine population size and for polar bears, the population is unknown,â€ said Berger-Wolf. â€œIf we donâ€™t have data for those species, what hope do the beetles and fungi have?â€ AI models can enhance existing conservation efforts for threatened species and their habitats by filling this data-deficiency gap. BioCLIP 2 is available under an open-source license on Hugging Face , where it was downloaded over 45,000 times last month. This paper builds on the first BioCLIP model, released over a year ago, which was also trained on NVIDIA GPUs and received the Best Student Paper award at the Computer Vision and Pattern Recognition (CVPR) conference. The BioCLIP 2 paper will be presented at NeurIPS, taking place Nov. 30-Dec. 5 in Mexico City, and Dec. 2-7 in San Diego. Building the Worldâ€™s Biggest Biological Flash Card Deck The project began with the compilation of a massive dataset, TREEOFLIFE-200M , which comprises 214 million images of organisms that span over 925,000 taxonomic classes â€” from monkeys to mealworms and magnolias. To curate this vast amount of data, Berger-Wolfâ€™s team at the Imageomics Institute collaborated with the Smithsonian Institution , experts from various universities and other field-related organizations. These researchers set out to discover what would happen if they trained a biology model on more data than ever. The team wanted to see if it was possible to move â€œbeyond the science of individual organisms to the science of ecosystems,â€ said Berger-Wolf. After 10 days of training on 32 NVIDIA H100 GPUs, BioCLIP 2 displayed novel abilities, such as distinguishing between adult and juvenile as well as male and female animals within species â€” without being explicitly taught these concepts. It also made associations between related species â€” like understanding how zebras relate to other equids. â€œThis model learns that at every level of taxonomy, all of these images of zebras have a particular genus label, and of these images of equids â€” including zebras, horses and donkeys â€” they have a particular family trait and so on,â€ she said. â€œIt learns the hierarchy without ever being told it, just through these associations.â€ The model can even determine the health of an organism based on training data. For example, it separated healthy apple or blueberry leaves from diseased leaves, as well as could recognize differing types of diseases, when generating the scatter plot below. The scatter plots show plant species better separated as the model is trained. The intra-species variations also form clusters, making them easier to separate. Berger-Wolfâ€™s team used a cluster of 64 NVIDIA Tensor Core GPUs to accelerate model training, plus individual Tensor Core GPUs for inference . â€œFoundation models like BioCLIP would not be possible without NVIDIA accelerated computing,â€ said Berger-Wolf. Wildlife Digital Twins: The Future of Studying Ecosystem Relationships The researchersâ€™ next endeavor is to develop a wildlife-based interactive digital twin that can be used to visualize and simulate ecological interactions between species as well as their ways of engaging with the environment. The goal is to provide a safe, easy way to study organismal relationships that naturally occur in the wild, while minimizing impact and disturbance on ecosystems. â€œThe digital twin allows us to visualize species interactions and put them in context, as well as to play the what-if scenarios and test our models without destroying the actual environment â€” creating as light a footprint as possible,â€ said Berger-Wolf. The digital twin will give scientists the opportunity to explore the points of view of the species theyâ€™re studying within the simulated environment, opening endless possibilities for more complex and accurate ecological research. Eventually, versions of this technology could even be deployed for public use â€” such as through interactive platforms at zoos. People could explore, visualize and learn about the natural environment and its many species from entirely new vantage points. â€œIâ€™m getting goosebumps just imagining that scenario of a kid coming into the zoo and being like, wow â€” this is what you would see if you were another zebra part of that herd, or if you were the little spider sitting on that scratching post,â€ Berger-Wolf said. Learn more about BioCLIP 2 . Categories: Generative AI | Research Tags: Artificial Intelligence | Education | Science]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 20 Nov 2025 14:00:17 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/bioclip2-foundation-ai-model/</guid>
    </item>
    <item>
      <title>Powering AI Superfactories, NVIDIA and Microsoft Integrate Latest Technologies for Inference, Cybersecurity, Physical AI</title>
      <link>https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</link>
      <description><![CDATA[Timed with the Microsoft Ignite conference running this week, NVIDIA is expanding its collaboration with Microsoft, including through the adoption of next-generation NVIDIA Spectrum-X Ethernet switches for the new Microsoft Fairwater AI superfactory, powered by the NVIDIA Blackwell platform. The collaboration brings new integrations across Microsoft 365 Copilot, as well as the public preview of next-generation Azure NC Series VMs powered by NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs , NVIDIA Nemotron integrations to accelerate AI for Microsoft SQL Server 2025, capabilities for onboarding AI agents in Microsoft 365 and optimizations for high-performance inference, cybersecurity and physical AI. Microsoftâ€™s AI Superfactory connects the landmark Fairwater data center in Wisconsin with a new, state-of-the-art facility in Atlanta , Georgia. This massive-scale infrastructure will integrate hundreds of thousands of NVIDIA Blackwell GPUs for large-scale training. In addition, Microsoft is deploying more than 100,000 Blackwell Ultra GPUs in NVIDIA GB300 NVL72 systems being deployed globally for inference. â€œOur collaboration with NVIDIA is built on driving innovation across the entire system and full stack, from silicon to services,â€ said Nidhi Chappell, corporate vice president of product management at Microsoft. â€œBy coupling Microsoft Azureâ€™s unmatched data center scale with NVIDIAâ€™s accelerated computing, we are maximizing AI data center performance and efficiency, which is of paramount importance for our customers leading the new AI era.â€ The most demanding workloads for OpenAI, the Microsoft AI Superintelligence Team, Microsoft 365 Copilot and Microsoft Foundry services will be powered by this infrastructure. Customers like Black Forest Labs are also using NVIDIA GB200 NVL72 systems to train next-generation multimodal FLUX models that power visual intelligence. To connect this massive infrastructure, Microsoft is deploying next-generation NVIDIA Spectrum-X Ethernet switches in its Fairwater AI data center â€” the largest and most sophisticated AI factories ever built â€” delivering the performance, scale and efficiency required for OpenAI to run large-scale AI models and applications. New Azure NCv6 Series VMs with NVIDIA RTX PRO 6000 Blackwell GPUs are now in public preview on Azure, expanding the Blackwell platform to provide right-sized acceleration for multiple workloads including multimodal agentic AI, industrial digitalization with NVIDIA Omniverse libraries, scientific simulation and visual computing. This flexibility extends from the cloud to the edge with Azure Local , enabling powerful sovereign AI solutions while bringing low-latency, real-time AI to wherever data needs to reside. This allows enterprises to seamlessly develop, deploy and manage AI-powered digital twins and generative AI applications with NVIDIA RTX PRO 6000 Blackwell GPUs from the Azure cloud directly to their factory floors, on-premises data centers or secure edge locations. Software Optimizations Deliver a Fungible AI Fleet The NVIDIA platform on Azure, spanning NVIDIA Blackwell and Hopper GPUs, accelerates the latest models from the Microsoft AI Superintelligence Team, including text (MAI-1-preview), real-time voice (MAI-Voice-1) and high-fidelity image generation (MAI-Image-1) â€” bringing new multimodal experiences across Bing Image Creator and Microsoft Copilot. Central to NVIDIAâ€™s collaboration with Microsoft is building a fungible fleet â€” a flexible, continuously modernized infrastructure that can accelerate any workload with maximum efficiency. This is achieved through continuous, full-stack software optimizations that deliver compounding performance gains and maximize throughput across the entire AI lifecycle and across multiple NVIDIA architectures on Azure. The gains also extend to workloads beyond generative AI, including data processing, vector search, databases, digital twins, scientific computing and 3D design. This co-engineering saves significant costs for customers, making AI projects that were once theoretical now economically viable. For example, the continuous full-stack optimization work has directly contributed to an over 90% drop in the price of popular GPT models for end users on Azure in two years. Ongoing optimization work now extends to Microsoft Foundry , where the NVIDIA TensorRT-LLM library helps boost throughput, reduce latency and lower costs for a wide range of popular open models. NVIDIA and Microsoft have also partnered to optimize their fleet for AI workload performance through the NVIDIA DGX Cloud Benchmarking suite. Engineering teams from both companies worked closely together to identify bottlenecks and implement infrastructure tuning, driving performance gains. By achieving 95% of the performance possible using the NVIDIA reference architecture, Microsoft was named an Exemplar Cloud for H100 training. From Intelligent Data to AI Agents NVIDIA and Microsoft are integrating AI into the core of the enterprise, unlocking decades of proprietary data stored in one of the worldâ€™s most trusted databases. NVIDIA is accelerating AI in the new Microsoft SQL Server 2025 by integrating it with NVIDIA Nemotron open models and NVIDIA NIM microservices. This solution delivers GPU-optimized, secure and scalable retrieval-augmented generation directly where enterprise data lives, in the cloud or on premises. Plus, the collaboration extends to the new frontier of agentic AI in the workplace. The NVIDIA NeMo Agent Toolkit now connects with Microsoft Agent 365 , enabling developers to build, deploy and onboard compliant, enterprise-ready AI agents directly into the Microsoft 365 app ecosystem, including Outlook, Teams, Word and SharePoint. To power these new enterprise agents, Microsoft Foundry now offers NVIDIA Nemotron models for digital AI and NVIDIA Cosmos models for physical AI as secure NIM microservices. Developers can use them to build enterprise-grade agentic AI for a vast range of applications that benefit from multimodal intelligence, multilingual reasoning, math, coding and physical AI capabilities. The collaboration is also tackling cyber threats for enterprises. Microsoft and NVIDIA are collaborating on research for new adversarial learning models , built on the NVIDIA Dynamo-Triton framework and the NVIDIA TensorRT suite of tools, that can help enterprises defend against real-time cybersecurity threats with a 160x performance speedup compared with CPU methods. Physical AI and Industrial Digitalization NVIDIA and Microsoft are building the future of physical AI . With NVIDIA Omniverse libraries available on Microsoft Azure, NVIDIA is unlocking end-to-end reindustrialization in the cloud through its developer ecosystem. Developers are transforming industrial workflows, from computer-aided engineering with Synopsys to factory operations with Sight Machine and SymphonyAI . Robotics developers can tap into the NVIDIA Isaac Sim open-source robotics simulation framework to unlock critical workflows, from synthetic data generation to software-in-the-loop testing for all types of robot embodiments. Hexagon is building its AEON humanoid robot primarily using NVIDIAâ€™s full robotics stack on Azure. Similarly, the robotics platform, Wandelbots NOVA , running on Azure integrates Isaac Sim and Isaac Lab to simplify and speed up simulation to real-world deployment. In addition, NVIDIA and Microsoft are using a standardized approach for digital engineering to enable seamless OpenUSD interoperability across 3D workflows, making simulation and digital content creation accessible in the cloud. This expanded collaboration comes on the heels of a partnership announced with Anthropic and Microsoft earlier today. NVIDIA and Anthropic will collaborate on design and engineering to optimize Anthropic models for performance, efficiency and total cost of ownership, as well as optimize future NVIDIA architectures for Anthropic workloads. Learn more about NVIDIA and Microsoftâ€™s collaboration and sessions at Microsoft Ignite . Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | DGX Cloud | Digital Twin | Industrial and Manufacturing | Nemotron | NVIDIA Blackwell | NVIDIA Isaac Sim | NVIDIA NeMo | NVIDIA NIM | NVIDIA Omniverse | NVIDIA RTX | NVIDIA Spectrum-X]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 18 Nov 2025 20:00:22 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-microsoft-ai-superfactories/</guid>
    </item>
  </channel>
</rss>
