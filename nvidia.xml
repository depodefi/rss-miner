<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 07 Jan 2026 10:04:58 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>NVIDIA Brings GeForce RTX Gaming to More Devices With New GeForce NOW Apps for Linux PC and Amazon Fire TV</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-ces-2026/</link>
      <description><![CDATA[Announced at the CES trade show running this week in Las Vegas, NVIDIA is bringing more devices, more games and more ways to play to its GeForce NOW cloud gaming service. Powered by GeForce RTX 5080-class performance on the NVIDIA Blackwell RTX platform, GeForce NOW Ultimate continues to raise the bar for PC gamers streaming from the cloud. GeForce RTX 5080-powered servers are live globally for Ultimate members, delivering up to 5K resolution 120 frames-per-second (fps) streaming and up to 360 fps at 1080p with NVIDIA Reflex technology support for ultralow-latency, competitive play. Cinematic-Quality Streaming mode enhances image clarity and text sharpness for visually rich single-player adventures on nearly any screen. New this year, GeForce NOW is expanding that performance to more platforms than ever, headlined by a native Linux PC app and a new app for Amazon Fire TV sticks. Flight-simulation fans are also getting flight controls support, and members everywhere gain faster access to more games thanks to new single sign-on integrations and upcoming AAA titles joining the cloud. Here Come the Platforms Linux PCs and Amazon Fire TV sticks are joining the GeForce NOW native app family, unlocking new ways to play in the cloud across desktops and living rooms. These new apps build on GeForce NOW’s existing support for Windows PCs, macOS, Chromebooks, mobile devices, smart TVs, virtual-reality devices and handhelds, all tapping into the same GeForce RTX 5080-class performance wherever members log in. Turn your Linux PC into an RTX gaming rig. A new native GeForce NOW app for Linux PCs, supported with Ubuntu 24.04 and later distributions, answers one of the top requests from the PC gaming community. Linux users can transform their compatible systems into GeForce RTX-powered gaming rigs, streaming supported PC titles from the cloud at up to 5K and 120 fps or 1080p 360 fps. With rendering handled in the cloud, high-end PC gaming is possible on Linux operating systems, breathing new life into older devices. Members can enjoy ray tracing, NVIDIA DLSS 4 and other RTX technologies without needing a local high-performance GPU. The app is designed to bring a seamless, native experience that fits naturally into Linux desktop workflows while giving access to the expansive GeForce NOW library, turning everyday Linux devices into RTX gaming powerhouses. The app is expected to enter a beta release early this year. Game on in the living room. A new native GeForce NOW app for select Amazon Fire TV sticks — starting with the Fire TV Stick 4K Plus (2nd Gen) and Fire TV Stick 4K Max (2nd Gen) — can bring RTX-powered PC gaming to another big screen in the home. Members can stream their compatible PC game libraries directly to Fire TV-connected displays to turn a compact streaming stick into a powerful cloud gaming rig. With support for gamepads and GeForce NOW’s familiar interface, Fire TV users can jump into their favorite supported games without a console or gaming PC attached to the TV. This builds on existing TV support and helps make GeForce NOW the easiest way to bring high-performance PC gaming into the living room. The app is expected to be available in countries where compatible Amazon Fire TV sticks and GeForce NOW are offered and will be launching early this year. Take Flight No fight, just flight in the cloud. GeForce NOW turns more devices into powerful cloud gaming rigs, and CES this year brings another of the community’s most-requested additions. Simulation fans are getting a major upgrade with flight controls support on GeForce NOW. Popular flight sticks and throttle systems from leading brands such as Thrustmaster and Logitech can be used as affixed hands-on throttle-and-stick desktop units or as separately mounted stick-and-throttle setups for custom cockpits. Combined with RTX 5080 performance, ultralow-latency streaming and NVIDIA Reflex in supported titles, flight controls let virtual pilots experience greater precision and deeper immersion in their favorite flight- and space-simulation games — including Microsoft Flight Simulator 2024, Elite Dangerous and War Thunder. Members can build out detailed simulation setups at home while streaming the heavy lifting from the cloud when it launches early this year.. Blockbusters in the Cloud The GeForce NOW catalog includes thousands of supported games from top PC stores like Steam, Epic Games Store, Xbox and others, with more joining every week. Backed by RTX 5080-class performance, members can stream everything from competitive shooters to expansive role-playing games with high frame rates, advanced graphics features and low latency.​ New AAA titles such as IO Interactive’s 007 First Light , Capcom’s Resident Evil Requiem, Pearl Abyss’ Crimson Desert, and Gaijin Entertainment’s Active Matter are coming to GeForce NOW when they launch on PC, adding to an already robust lineup of new releases and fan favorites. License to stream. 007 First Light drops players into a modern James Bond origin story filled with stealth, espionage and cinematic action. Resident Evil Requiem continues the iconic survival-horror series with a new protagonist facing terrifying threats in a chilling new setting. Crimson Desert blends open-world exploration, cinematic storytelling and intense combat in a richly detailed fantasy world. Active Matter from Gaijin is a realistic military shooter where players join dangerous raids for loot or intense player vs. player battles set in a fractured multiverse. Members can look forward to seeing these and other upcoming hits arrive on the service, with updates shared regularly on GFN Thursdays. One Login, Many Worlds Just sign in once. The rest is game history. GeForce NOW is also making it faster to jump into gaming with new account and platform integrations. Recent updates introduced Battle.net automatic sign-in, letting members connect their accounts and access supported titles more quickly. That seamless experience is expanding to additional game stores, with Gaijin.net set to soon support automatic sign-in on GeForce NOW early this year. Members will be able to authenticate once and jump into War Thunder and other titles with fewer steps. Learn more about the latest NVIDIA-powered innovations at CES , running through Friday, Jan. 9. See notice regarding software product information. Categories: Gaming Tags: CES 2026 | Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:51 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-ces-2026/</guid>
    </item>
    <item>
      <title>NVIDIA RTX Accelerates 4K AI Video Generation on PC With LTX-2 and ComfyUI Upgrades</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</link>
      <description><![CDATA[2025 marked a breakout year for AI development on PC. PC-class small language models (SLMs) improved accuracy by nearly 2x over 2024, dramatically closing the gap with frontier cloud-based large language models (LLMs). AI PC developer tools including Ollama, ComfyUI, llama.cpp and Unsloth have matured, their popularity has doubled year over year and the number of users downloading PC-class models grew tenfold from 2024. These developments are paving the way for generative AI to gain widespread adoption among everyday PC creators, gamers and productivity users this year. At CES this week, NVIDIA is announcing announcing a wave of AI upgrades for GeForce RTX, NVIDIA RTX PRO and NVIDIA DGX Spark devices that unlock the performance and memory needed for developers to deploy generative AI on PC, including: Up to 3x performance and 60% reduction in VRAM for video and image generative AI via PyTorch-CUDA optimizations and native NVFP4/FP8 precision support in ComfyUI . RTX Video Super Resolution integration in ComfyUI, accelerating 4K video generation. NVIDIA NVFP8 optimizations for the open weights release of Lightricks’ state-of-the-art LTX-2 audio-video generation model . A new video generation pipeline for generating 4K AI video using a 3D scene in Blender to precisely control outputs. Up to 35% faster inference performance for SLMs via Ollama and llama.cpp. RTX acceleration for Nexa.ai ’s Hyperlink new video search capability. These advancements will allow users to seamlessly run advanced video, image and language AI workflows with the privacy, security and low latency offered by local RTX AI PCs. Generate Videos 3x Faster and in 4K on RTX PCs Generative AI can make amazing videos, but online tools can be difficult to control with just prompts. And trying to generate 4K videos is near impossible, as most models are too large to fit on PC VRAM. Today, NVIDIA is introducing an RTX-powered video generation pipeline that enables artists to gain accurate control over their generations while generating videos 3x faster and upscaling them to 4K — only using a fraction of the VRAM. This video pipeline allows emerging artists to create a storyboard, turn it into photorealistic keyframes and then turn these keyframes into a high-quality, 4K video. The pipeline is split into three blueprints that artists can mix and match or modify to their needs: A 3D object generator that creates assets for scenes. A 3D-guided image generator that allows users to set their scene in Blender and generate photorealistic keyframes from it. A video generator that follows a user’s start and end key frames to animate their video, and uses NVIDIA RTX Video technology to upscale it to 4K This pipeline is possible by the groundbreaking release of the new LTX-2 model from Lightricks, available for download today. A major milestone for local AI video creation, LTX-2 delivers results that stand toe-to-toe with leading cloud-based models while generating up to 20 seconds of 4K video with impressive visual fidelity. The model features built-in audio, multi-keyframe support and advanced conditioning capabilities enhanced with controllability low-rank adaptations — giving creators cinematic-level quality and control without relying on cloud dependencies. Under the hood, the pipeline is powered by ComfyUI. Over the past few months, NVIDIA has worked closely with ComfyUI to optimize performance by 40% on NVIDIA GPUs, and the latest update adds support for the NVFP4 and NVFP8 data formats. All combined, performance is 3x faster and VRAM is reduced by 60% with RTX 50 Series’ NVFP4 format, and performance is 2x faster and VRAM is reduced by 40% with NVFP8. NVFP4 and NVFP8 checkpoints are now available for some of the top models directly in ComfyUI. These models include LTX-2 from Lightricks, FLUX.1 and FLUX.2 from Black Forest Labs, and Qwen-Image and Z-Image from Alibaba. Download them directly in ComfyUI, with additional model support coming soon. Once a video clip is generated, videos are upscaled to 4K in just seconds using the new RTX Video node in ComfyUI. This upscaler works in real time, sharpens edges and cleans up compression artifacts for a clear final image. RTX Video will be available in ComfyUI next month. To help users push beyond the limits of GPU memory, NVIDIA has collaborated with ComfyUI to improve its memory offload feature, known as weight streaming. With weight streaming enabled, ComfyUI can use system RAM when it runs out of VRAM, enabling larger models and more complex multistage node graphs on mid-range RTX GPUs. The video generation workflow will be available for download next month, with the newly released open weights of the LTX-2 Video Model and ComfyUI RTX updates available now. A New Way to Search PC Files and Videos File searching on PCs has been the same for decades. It still mostly relies on file names and spotty metadata, which makes tracking down that one document from last year way harder than it should be. Hyperlink — Nexa.ai’s local search agent — turns RTX PCs into a searchable knowledge base that can answer questions in natural language with inline citations. It can scan and index documents, slides, PDFs and images, so searches can be driven by ideas and content instead of file name guesswork. All data is processed locally and stays on the user’s PC for privacy and security. Plus, it’s RTX-accelerated, taking 30 seconds per gigabyte to index text and image files and three seconds for a response on a RTX 5090 GPU, compared with an hour per gigabyte to index files and 90 seconds for a response on CPUs. At CES, Nexa.ai is unveiling a new beta version of Hyperlink that adds support for video content, enabling users to search through their videos for objects, actions and speech. This is ideal for users ranging from video artists looking for B-roll to gamers who want to find that time they won a battle royale match to share with their friends. For those interested in trying the Hyperlink private beta, sign up for access on this webpage . Access will roll out starting this month. Small Language Models Get 35% Faster NVIDIA has collaborated with the open‑source community to deliver major performance gains for SLMs on RTX GPUs and the NVIDIA DGX Spark desktop supercomputer using Llama.cpp and Ollama. The latest changes are especially beneficial for mixture-of-experts models, including the new NVIDIA Nemotron 3 family of open models . SLM inference performance has improved by 35% and 30% for llama.cpp and Ollama, respectively, over the past four months. These updates are available now, and a quality-of-life upgrade for llama.cpp also speeds up LLM loading times. These speedups will be available in the next update of LM Studio, and will be coming soon to agentic apps like the new MSI AI Robot app. The MSI AI Robot app, which also takes advantage of the Llama.cpp optimizations, lets users control their MSI device settings and will incorporate the latest updates in an upcoming release. NVIDIA Broadcast 2.1 Brings Virtual Key Light to More PC Users The NVIDIA Broadcast app improves the quality of a user’s PC microphone and webcam with AI effects, ideal for livestreaming and video conferencing. Version 2.1 updates the Virtual Key Light effect to improve performance — making it available to RTX 3060 desktop GPUs and higher — handle more lighting conditions, offer broader color temperature control and use an updated HDRi base map for a two‑key‑light style often seen in professional streams. Download the NVIDIA Broadcast update today. Transform an At-Home Creative Studio Into an AI Powerhouse With DGX Spark As new and increasingly capable AI models arrive on PC each month, developer interest in more powerful and flexible local AI setups continues to grow. DGX Spark — a compact AI supercomputer that fits on users’ desks and pairs seamlessly with a primary desktop or laptop — enables experimenting, prototyping and running advanced AI workloads alongside an existing PC. Spark is ideal for those interested in testing out LLMs or prototyping agentic workflows, or for artists who want to generate assets in parallel to their workflow so that their main PC is still available for editing. At CES, NVIDIA is unveiling major AI performance updates to Spark, delivering up to 2.6x faster performance since it launched just under three months ago. New DGX Spark playbooks are also available, including one for speculative decoding and another to fine-tune models with two DGX Spark modules. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X — and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | CES 2026 | Creators | GeForce RTX | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:18 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</guid>
    </item>
    <item>
      <title>NVIDIA DLSS 4.5, Path Tracing and G-SYNC Pulsar Supercharge Gameplay With Enhanced Performance and Visuals</title>
      <link>https://blogs.nvidia.com/blog/dlss-path-tracing-g-sync-pulsar-ces-2026/</link>
      <description><![CDATA[At the CES trade show, NVIDIA today announced DLSS 4.5, which introduces Dynamic Multi Frame Generation, a new 6X Multi Frame Generation mode and a second-generation transformer model for DLSS Super Resolution, so gamers can experience the latest and greatest titles with enhanced performance and visuals. Over 250 games and apps now support NVIDIA DLSS 4 technology, with this year’s biggest titles adding support, including 007 First Light, Phantom Blade Zero, PRAGMATA and Resident Evil Requiem at launch. In addition, RTX Remix Logic debuted, expanding the capabilities of the Remix modding platform to enable modders to trigger dynamic graphics effects throughout a game based on real-time game events. Plus, NVIDIA ACE technology demonstrated in Total War: PHARAOH showcases how AI can assist players in navigating the complexities of the game’s many systems and mechanics. In PUBG: BATTLEGROUNDS , PUBG Ally powered by NVIDIA ACE adds long-term memory, evolving its intelligence and capabilities. And G-SYNC Pulsar monitors are available this week, delivering a tear-free experience together with a perceived 1,000Hz+ effective motion clarity and G-SYNC Ambient Adaptive Technology — all setting a new gold standard for gamers. NVIDIA DLSS 4.5 Will Power 4K 240Hz Path-Traced Gaming NVIDIA DLSS 4.5 introduces Dynamic Multi Frame Generation and a new 6X Multi Frame Generation mode. DLSS 4.5 can generate up to five additional frames per traditionally rendered frame, dynamically boosting performance and enabling 240+ frames-per-second gaming with path tracing using GeForce RTX 50 Series GPUs. This delivers the smoothest gameplay experiences yet. Dynamic Multi Frame Generation and 6X Multi-Frame Generation are expected to be available in spring of this year. A second-generation transformer model for DLSS Super Resolution also arrives with NVIDIA DLSS 4.5, bringing state-of-the-art image quality to over 400 games and apps for all GeForce RTX GPUs. The second-generation transformer is available to try now via the NVIDIA App for all GeForce RTX GPUs. Learn more . Over 250 DLSS 4 Games and Apps Available Now DLSS 4 with Multi Frame Generation launched at CES last year with 75 games and apps supported. Now, more than 250 games and apps are supported, including 2025’s most-played titles, such as ARC Raiders, Battlefield 6 , Clair Obscur: Expedition 33 and Where Winds Meet. New and upgraded titles announced today that will support the latest GeForce RTX technologies include 007 First Light , Active Matter , DEFECT , Phantom Blade Zero , PRAGMATA , Resident Evil Requiem and Screamer . Learn more . Next-Generation G-SYNC Pulsar Gaming Monitors Available Now The launch of G-SYNC in 2013 revolutionized displays and gaming, introducing Variable Refresh Rate (VRR) technology that all major display manufacturers now incorporate. This meant gamers no longer had to experience screen-tearing glitches when optimizing for responsive gameplay. G-SYNC Pulsar displays mark the latest evolution of NVIDIA’s pioneering VRR technology. Through the invention of variable frequency backlight strobing, they deliver effective motion clarity of over 1,000Hz, significantly increasing the clarity and visibility of content in motion so gamers can track targets with increased precision and maintain consistent smoothness in gameplay. In addition, new G-SYNC Ambient Adaptive Technology uses a built-in light sensor, letting users automatically tune color temperature and brightness for optimal viewing at any hour, day or night. Learn more . RTX Remix Logic Brings Dynamic Graphics Effects to Classic Games Many iconic PC games remain beloved for their unforgettable stories, characters and gameplay. However, as technology advances, their visuals can become dated, making it harder for gamers to immerse themselves in the titles. NVIDIA RTX Remix , a modding platform for RTX AI PCs built to reimagine the graphics of these timeless classics with cutting-edge path tracing, lets longtime fans relive their favorite adventures in stunning visual detail, while opening opportunities for a new wave of players. A new RTX Remix update — RTX Remix Logic — will be available later this month via the NVIDIA App. Remix Logic is a logic system for making RTX Remix mods visually reactive to the moment-to-moment, in-game action, equipping modders with 900+ configurable settings to trigger dynamic graphics effects based on a wide variety of in-game events. Historically, modifying a game’s graphics in response to real-time game events was restricted to those with source code or engine access. RTX Remix eliminates this barrier so modders can customize visuals across 165+ classic games without touching the original engine code. Learn more . NVIDIA ACE Powers New AI Teammates and Advisors Non-playable characters (NPCs) traditionally follow strict rules designed to provide scripted interactions with players. NVIDIA is expanding the NVIDIA ACE suite of AI technologies to turn conversational NPCs into autonomous game characters that use AI to perceive, plan and act like human players. Creative Assembly, creator of the award-winning Total War franchise, is experimenting with NVIDIA ACE in Total War: PHARAOH to power a new, dynamic AI advisor that assists players in learning the game’s many systems and mechanics. By processing the player’s prompts, current game state and data retrieved from the game’s complex database, the advisor delivers real-time, context-aware guidance that adapts to what the player is doing, while staying in-character and faithful to the game’s lore and time period. KRAFTON is adding long-term memory for PUBG Ally — an NVIDIA ACE-powered AI teammate in PUBG: BATTLEGROUNDS that allows players to issue commands and communicate plans of attack or other tactical maneuvers with each other. With long-term memory, the Ally can remember previous performances and gameplay interactions, and inject commentary into their responses that refers to past events. PUBG Ally will initially be released as part of a limited-time user test event in the first half of this year via PUBG: BATTLEGROUNDS Arcade for players using English, Korean or Chinese. GeForce NOW Expands RTX Cloud Gaming Across More Devices NVIDIA continues to push the limits of PC gaming accessibility with new GeForce NOW updates unveiled today at CES, delivering GeForce RTX 5080-class performance to an even broader range of devices. Gamers can now experience ultrahigh-fidelity RTX visuals through new native apps for Linux systems and Amazon Fire TV Sticks. This answers top community requests. New support for hands-on throttle-and-stick flight control peripherals provide immersion in simulation titles, enabling precise input and smooth streaming responsiveness for flight and space combat games. GeForce NOW also introduces streamlined single sign-on for Gaijin titles, minimizing setup time and getting players into their favorite experiences faster. GeForce NOW’s ever-expanding library continues to grow with day-and-date cloud launches of major upcoming titles — including 007 First Light , Active Matter , Resident Evil Requiem and Crimson Desert — ensuring that gamers can stream the latest blockbusters the same day they arrive on PC, all powered by NVIDIA RTX technology. Learn more . RTX AI PCs Accelerate AI Video, Image and Text Generation At CES, NVIDIA announced a wave of AI upgrades for GeForce RTX GPUs and laptops that unlock the performance and memory needed for developers to deploy generative AI on PC, including: Up to 3x performance and 60% reduction in VRAM for video and image generative AI via PyTorch-CUDA optimizations and native NVFP4/FP8 precision support in ComfyUI. RTX Video Super Resolution integration in ComfyUI, accelerating 4K video generations. NVIDIA NVFP8 optimizations for the open weights release of Lightricks’ state-of-the-art LTX-2 audio-video generation model. A blueprint for generating 4K AI video using a 3D scene in Blender to precisely control outputs. Up to 35% faster inference performance for SLMs via Ollama and llama.cpp. RTX acceleration for Nexa.ai ’s Hyperlink new video search capability. These advancements will allow users to seamlessly run advanced video, image and language AI workflows with the privacy, security and low latency offered by local RTX AI PCs. Learn more . See notice regarding software product information. Categories: Gaming Tags: Artificial Intelligence | CES 2026 | Cloud Gaming | G-SYNC | Game Development | Gaming | GeForce | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:09 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/dlss-path-tracing-g-sync-pulsar-ces-2026/</guid>
    </item>
    <item>
      <title>NVIDIA Rubin Platform, Open Models, Autonomous Driving: NVIDIA Presents Blueprint for the Future at CES</title>
      <link>https://blogs.nvidia.com/blog/2026-ces-special-presentation/</link>
      <description><![CDATA[NVIDIA founder and CEO Jensen Huang took the stage at the Fontainebleau Las Vegas to open CES 2026, declaring that AI is scaling into every domain and every device. “Computing has been fundamentally reshaped as a result of accelerated computing, as a result of artificial intelligence,” Huang said. “What that means is some $10 trillion or so of the last decade of computing is now being modernized to this new way of doing computing.” Huang unveiled Rubin , NVIDIA’s first extreme-codesigned, six-chip AI platform now in full production, and introduced Alpamayo , an open reasoning model family for autonomous vehicle development — part of a sweeping push to bring AI into every domain. With Rubin, NVIDIA aims to “push AI to the next frontier” while slashing the cost of generating tokens to roughly one-tenth that of the previous platform, Huang said, making large-scale AI far more economical to deploy. Huang also emphasized the role of NVIDIA open models across every domain, trained on NVIDIA supercomputers, forming a global ecosystem of intelligence that developers and enterprises can build on. “Every single six months, a new model is emerging, and these models are getting smarter and smarter,” Huang said. “Because of that, you could see the number of downloads has exploded.” Find all NVIDIA news from CES in this online press kit . A New Engine for Intelligence: The Rubin Platform Introducing the audience to pioneering American astronomer Vera Rubin, after whom NVIDIA named its next-generation computing platform, Huang announced that the NVIDIA Rubin platform , the successor to the record‑breaking NVIDIA Blackwell architecture and the company’s first extreme-codesigned, six‑chip AI platform, is now in full production. Built from the data center outward, Rubin platform components span: Rubin GPUs with 50 petaflops of NVFP4 inference Vera CPUs engineered for data movement and agentic processing NVLink 6 scale‑up networking Spectrum‑X Ethernet Photonics scale‑out networking ConnectX‑9 SuperNICs BlueField‑4 DPUs Extreme codesign — designing all these components together — is essential because scaling AI to gigascale requires tightly integrated innovation across chips, trays, racks, networking, storage and software to eliminate bottlenecks and dramatically reduce the costs of training and inference, Huang explained. He also introduced AI-native storage with NVIDIA Inference Context Memory Storage Platform — an AI‑native KV‑cache tier that boosts long‑context inference with 5x higher tokens per second, 5x better performance per TCO dollar and 5x better power efficiency. Put it all together and the Rubin platform promises to dramatically accelerate AI innovation, delivering AI tokens at one-tenth the cost. “The faster you train AI models, the faster you can get the next frontier out to the world,” Huang said. “This is your time to market. This is technology leadership.” Open Models for All NVIDIA’s open models — trained on NVIDIA’s own supercomputers — are powering breakthroughs across healthcare, climate science, robotics, embodied intelligence and autonomous driving. “Now on top of this platform, NVIDIA is a frontier AI model builder, and we build it in a very special way. We build it completely in the open so that we can enable every company, every industry, every country, to be part of this AI revolution.” The portfolio spans six domains — Clara for healthcare, Earth-2 for climate science, Nemotron for reasoning and multimodal AI, Cosmos for robotics and simulation, GR00T for embodied intelligence and Alpamayo for autonomous driving — creating a foundation for innovation across industries. “These models are open to the world,” Huang said, underscoring NVIDIA’s role as a frontier AI builder with world-class models topping leaderboards. “You can create the model, evaluate it, guardrail it and deploy it.” AI on Every Desk: RTX, DGX Spark and Personal Agents Huang emphasized that AI’s future is not only about supercomputers — it’s personal. Huang showed a demo featuring a personalized AI agent running locally on the NVIDIA DGX Spark desktop supercomputer and embodied through a Reachy Mini robot using Hugging Face models — showing how open models, model routing and local execution turn agents into responsive, physical collaborators. “The amazing thing is that is utterly trivial now, but yet, just a couple of years ago, that would have been impossible, absolutely unimaginable,” Huang said. The world’s leading enterprises are integrating NVIDIA AI to power their products, Huang said, citing companies including Palantir, ServiceNow, Snowflake, CodeRabbit, CrowdStrike, NetApp and Semantec. “Whether it’s Palantir or ServiceNow or Snowflake — and many other companies that we’re working with — the agentic system is the interface.” At CES, NVIDIA also announced that DGX Spark delivers up to 2.6x performance for large models, with new support for Lightricks LTX‑2 and FLUX image models, and upcoming NVIDIA AI Enterprise availability. Physical AI AI is now grounded in the physical world, through NVIDIA’s technologies for training, inference and edge computing. These systems can be trained on synthetic data in virtual worlds long before interacting with the real world. Huang showcased NVIDIA Cosmos open world foundation models trained on videos, robotics data and simulation. Cosmos: Generates realistic videos from a single image Synthesizes multi‑camera driving scenarios Models edge‑case environments from scenario prompts Performs physical reasoning and trajectory prediction Drives interactive, closed‑loop simulation Advancing this story, Huang announced Alpamayo , an open portfolio of reasoning vision language action models, simulation blueprints and datasets enabling level 4‑capable autonomy. This includes: Alpamayo R1 — the first open, reasoning VLA model for autonomous driving AlpaSim — a fully open simulation blueprint for high‑fidelity AV testing “Not only does it take sensor input and activates steering wheel, brakes and acceleration, it also reasons about what action it is about to take,” Huang said, teeing up a video showing a vehicle smoothly navigating busy San Francisco traffic. Huang announced the first passenger car featuring Alpamayo built on NVIDIA DRIVE full-stack autonomous vehicle platform will be on the roads soon in the all‑new Mercedes‑Benz CLA — with AI‑defined driving coming to the U.S. this year, and follows the CLA’s recent EuroNCAP five‑star safety rating. Huang also highlighted growing momentum behind DRIVE Hyperion , the open, modular, level‑4‑ready platform adopted by leading automakers, suppliers and robotaxi providers worldwide. “Our vision is that, someday, every single car, every single truck will be autonomous, and we’re working toward that future,” Huang said. Huang was then joined on stage by a pair of tiny beeping, booping, hopping robots as he explained how NVIDIA’s full‑stack approach is fueling a global physical AI ecosystem . Huang rolled a video showing how robots are trained in NVIDIA Isaac Sim and Isaac Lab in photorealistic, simulated worlds — before highlighting the work of partners in physical AI across the industry, including Synopsys and Cadence, Boston Dynamics and Franka, and more. Huang also appeared with Siemens CEO Roland Busch at the company’s Tuesday keynote to announce an expanded partnership , supported by a montage showing how NVIDIA’s full stack integrates with Siemens’ industrial software, enabling physical AI from design and simulation through production. “These manufacturing plants are going to be essentially giant robots,” Huang said at NVIDIA’s presentation on Monday. Roland Busch, president and CEO of Siemens, with Jensen Huang, founder and CEO of NVIDIA, during the Siemens keynote at CES 2026. Building the Future, Together Huang explained that NVIDIA builds entire systems now because it takes a full, optimized stack to deliver AI breakthroughs. “Our job is to create the entire stack so that all of you can create incredible applications for the rest of the world,” he said. Watch the full presentation replay: DLSS 4.5 and Other Gaming and Creating Updates On Monday evening, NVIDIA announced DLSS 4.5 , which introduces Dynamic Multi Frame Generation, a new 6X Multi Frame Generation mode and a second-generation transformer model for DLSS Super Resolution, so gamers can experience the latest and greatest titles with enhanced performance and visuals. Over 250 games and apps now support NVIDIA DLSS 4 technology, with this year’s biggest titles adding support, including 007 First Light, Phantom Blade Zero, PRAGMATA and Resident Evil Requiem at launch. RTX Remix Logic debuted, expanding the capabilities of the Remix modding platform to enable modders to trigger dynamic graphics effects throughout a game based on real-time game events. Plus, NVIDIA ACE technology demonstrated in Total War: PHARAOH showcases how AI can assist players in navigating the complexities of the game’s many systems and mechanics. In PUBG: BATTLEGROUNDS , PUBG Ally powered by NVIDIA ACE adds long-term memory, evolving its intelligence and capabilities. And G-SYNC Pulsar monitors are available this week, delivering a tear-free experience together with a perceived 1,000Hz+ effective motion clarity and G-SYNC Ambient Adaptive Technology — all setting a new gold standard for gamers. In addition, NVIDIA is bringing GeForce RTX gaming to more devices with new GeForce NOW Apps for Linux PC and Amazon Fire TV . And NVIDIA RTX accelerates 4K AI video generation on PCs with LTX-2 and ComfyUI upgrades. Read more about these announcements from Monday night at CES on this GeForce news article . Learn more about all NVIDIA announcements at CES . Categories: Corporate | Deep Learning | Driving | Hardware | Supercomputing Tags: CES 2026]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 23:30:18 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/2026-ces-special-presentation/</guid>
    </item>
    <item>
      <title>NVIDIA BlueField-Powered Cybersecurity and Acceleration Arrive on NVIDIA Enterprise AI Factory Validated Design</title>
      <link>https://blogs.nvidia.com/blog/bluefield-cybersecurity-acceleration-enterprise-ai-factory-validated-design/</link>
      <description><![CDATA[AI is powering breakthroughs across industries, helping enterprises operate with greater intelligence and speed. As AI factories scale, the next generation of enterprise AI depends on infrastructure that can efficiently manage data, secure every stage of the pipeline and accelerate the core services that move, protect and process information alongside AI workloads. NVIDIA has expanded the NVIDIA Enterprise AI Factory validated design to include NVIDIA BlueField cybersecurity and infrastructure acceleration capabilities. This accelerates the data center services that keep AI factories running and delivers security that can operate at the same speed as AI. Leading software platforms from NVIDIA BlueField ecosystem partners Armis , Check Point , F5 , Fortinet , Palo Alto Networks , Rafay , Red Hat , Spectro Cloud and Trend Micro are now validated as part of NVIDIA Enterprise AI Factory, helping enterprises enhance runtime protection, streamline operations and strengthen the infrastructure that powers AI. How NVIDIA BlueField Accelerates the Operating System of AI Factories NVIDIA BlueField, the processor powering the operating system of AI factories, accelerates the essential services that support AI — networking, storage, security and orchestration — so that CPUs and GPUs can stay focused on AI itself. By handling these tasks on a dedicated processor, BlueField enables enterprises to scale AI with consistent performance while maintaining strong, real-time security across the entire AI pipeline. NVIDIA BlueField delivers zero-trust, hardware-accelerated security that operates at the same speed and scale as enterprise AI workloads. By offloading and isolating security functions to a dedicated processor, BlueField strengthens security at every layer while keeping AI operations running smoothly. With the NVIDIA DOCA Argus framework providing AI runtime visibility and threat detection, enterprises gain real-time security across data ingestion, fine-tuning and inference. Expanding the AI Factory Ecosystem With New Validated Partner Solutions The expanded NVIDIA Enterprise AI Factory validated design has integrated leading cybersecurity and infrastructure software, each validated to run on NVIDIA RTX PRO Servers and designed to harness NVIDIA BlueField acceleration. The integrations with BlueField reduce cyber threats and vulnerabilities by enabling real-time monitoring across enterprise AI infrastructure, improving isolation between AI workloads, and providing visibility and control over AI data, among other use cases — all while maintaining efficiency and performance as traffic and scale increase. The newly integrated software platforms include: Armis Centrix for continuous cyber exposure management across AI factories of any size. Check Point Infinity AI Cloud Protect for real-time network and host security against AI threats using NVIDIA DOCA Argus telemetry. F5 BIG-IP Next for Kubernetes , which provides efficient AI workload isolation, policy enforcement and reliable performance for evolving Kubernetes environments. Fortinet FortiGate VM to provide next-generation firewalling and zero-trust segmentation directly in data center infrastructure, extending the Fortinet Security Fabric to protect AI factories. Palo Alto Networks Prisma AIRS for enforcing zero-trust, AI runtime protection at the infrastructure layer with NVIDIA DOCA Argus. Rafay for operating production AI factories with consistent controls, multi-tenant isolation and extensible infrastructure services. Red Hat OpenShift for providing a production-grade application platform with enhanced networking and security for AI factories. Spectro Cloud’s PaletteAI and PaletteAI Secure — built for highly sensitive and regulated environments — make full-stack AI easy to integrate, fast to deploy and efficient to manage, supporting optimal resource utilization and ongoing innovation across the AI ecosystem. Trend Vision One for infrastructure-level monitoring and policy enforcement that combines real-time telemetry with global threat intelligence. With BlueField acceleration and new integrated applications from ecosystem partners, the NVIDIA Enterprise AI Factory validated design gives enterprises a stronger foundation for deploying secure, efficient AI at scale. Learn more about the NVIDIA BlueField platform and other AI innovations by joining NVIDIA at CES , running through Friday, Jan. 9, in Las Vegas. Categories: Data Center | Networking Tags: Artificial Intelligence | CES 2026 | Cybersecurity | NVIDIA BlueField]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 22:50:20 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/bluefield-cybersecurity-acceleration-enterprise-ai-factory-validated-design/</guid>
    </item>
    <item>
      <title>NVIDIA DGX SuperPOD Sets the Stage for Rubin-Based Systems</title>
      <link>https://blogs.nvidia.com/blog/dgx-superpod-rubin/</link>
      <description><![CDATA[NVIDIA DGX SuperPOD is paving the way for large-scale system deployments built on the NVIDIA Rubin platform — the next leap forward in AI computing. At the CES trade show in Las Vegas, NVIDIA today introduced the Rubin platform, comprising six new chips designed to deliver one incredible AI supercomputer, and engineered to accelerate agentic AI, mixture‑of‑experts (MoE) models and long‑context reasoning. The Rubin platform unites six chips — the NVIDIA Vera CPU , Rubin GPU, NVLink 6 Switch , ConnectX-9 SuperNIC, BlueField-4 DPU and Spectrum-6 Ethernet Switch — through an advanced codesign approach that accelerates training and reduces the cost of inference token generation. DGX SuperPOD remains the foundational design for deploying Rubin‑based systems across enterprise and research environments. The NVIDIA DGX platform addresses the entire technology stack — from NVIDIA computing to networking to software — as a single, cohesive system, removing the burden of infrastructure integration and allowing teams to focus on AI innovation and business results. “Rubin arrives at exactly the right moment, as AI computing demand for both training and inference is going through the roof,” said Jensen Huang, founder and CEO of NVIDIA. New Platform for the AI Industrial Revolution The Rubin platform used in the new DGX systems introduces five major technology advancements designed to drive a step‑function increase in intelligence and efficiency: Sixth‑Generation NVIDIA NVLink — 3.6TB/s per GPU and 260TB/s per Vera Rubin NVL72 rack for massive MoE and long‑context workloads. NVIDIA Vera CPU — 88 NVIDIA custom Olympus cores, full Armv9.2 compatibility and ultrafast NVLink-C2C connectivity for industry-leading efficient AI factory compute. NVIDIA Rubin GPU — 50 petaflops of NVFP4 compute for AI inference featuring a third-generation Transformer Engine with hardware‑accelerated compression. Third‑Generation NVIDIA Confidential Computing — Vera Rubin NVL72 is the first rack-scale platform delivering NVIDIA Confidential Computing, which maintains data security across CPU, GPU and NVLink domains. Second‑Generation RAS Engine — Spanning GPU, CPU and NVLink, the NVIDIA Rubin platform delivers real-time health monitoring, fault tolerance and proactive maintenance, with modular cable-free trays enabling 3x faster servicing. Together, these innovations deliver up to 10x reduction in inference token cost of the previous generation — a critical milestone as AI models grow in size, context and reasoning depth. DGX SuperPOD: The Blueprint for NVIDIA Rubin Scale‑Out Rubin-based DGX SuperPOD deployments will integrate: NVIDIA DGX Vera Rubin NVL72 or DGX Rubin NVL8 systems NVIDIA BlueField‑4 DPUs for secure, software‑defined infrastructure NVIDIA Inference Context Memory Storage Platform for next-generation inference NVIDIA ConnectX‑9 SuperNICs NVIDIA Quantum‑X800 InfiniBand and NVIDIA Spectrum‑X Ethernet NVIDIA Mission Control for automated AI infrastructure orchestration and operations NVIDIA DGX SuperPOD with DGX Vera Rubin NVL72 unifies eight DGX Vera Rubin NVL72 systems , featuring 576 Rubin GPUs, to deliver 28.8 exaflops of FP4 performance and 600TB of fast memory. Each DGX Vera Rubin NVL72 system — combining 36 Vera CPUs, 72 Rubin GPUs and 18 BlueField‑4 DPUs — enables a unified memory and compute space across the rack. With 260TB/s of aggregate NVLink throughput, it eliminates the need for model partitioning and allows the entire rack to operate as a single, coherent AI engine. NVIDIA DGX SuperPOD with DGX Rubin NVL8 systems delivers 64 DGX Rubin NVL8 systems featuring 512 Rubin GPUs. NVIDIA DGX Rubin NVL8 systems bring Rubin performance into a liquid-cooled form factor with x86 CPUs to give organizations an efficient on-ramp to the Rubin era for any AI project in the develop‑to‑deploy pipeline. Powered by eight NVIDIA Rubin GPUs and sixth-generation NVLink, each DGX Rubin NVL8 delivers 5.5x NVFP4 FLOPS compared with NVIDIA Blackwell systems. Next‑Generation Networking for AI Factories The Rubin platform redefines the data center as a high-performance AI factory with revolutionary networking, featuring NVIDIA Spectrum-6 Ethernet switches, NVIDIA Quantum-X800 InfiniBand switches, BlueField-4 DPUs and ConnectX-9 SuperNICs, designed to sustain the world’s most massive AI workloads. By integrating these innovations into the NVIDIA DGX SuperPOD, the Rubin platform eliminates the traditional bottlenecks of scale, congestion and reliability. Optimized Connectivity for Massive-Scale Clusters The next-generation 800Gb/s end-to-end networking suite provides two purpose-built paths for AI infrastructure, ensuring peak efficiency whether using InfiniBand or Ethernet: NVIDIA Quantum-X800 InfiniBand : Delivers the industry’s lowest latency and highest performance for dedicated AI clusters. It utilizes Scalable Hierarchical Aggregation and Reduction Protocol (SHARP v4) and adaptive routing to offload collective operations to the network. NVIDIA Spectrum-X Ethernet : Built on the Spectrum-6 Ethernet switch and ConnectX-9 SuperNIC, this platform brings predictable, high-performance scale-out and scale-across connectivity to AI factories using standard Ethernet protocols, optimized specifically for the “east-west” traffic patterns of AI workloads. Engineering the Gigawatt AI Factory These innovations represent an extreme codesign with the Rubin platform. By mastering congestion control and performance isolation, NVIDIA is paving the way for the next wave of gigawatt AI factories. This holistic approach ensures that as AI models grow in complexity, the networking fabric of the AI factory remains a catalyst for speed rather than a constraint. NVIDIA Software Advances AI Factory Operations and Deployments NVIDIA Mission Control — AI data center operation and orchestration software for NVIDIA Blackwell-based DGX systems — will be available for Rubin-based NVIDIA DGX systems to enable enterprises to automate the management and operations of their infrastructure. NVIDIA Mission Control accelerates every aspect of infrastructure operations, from configuring deployments to integrating with facilities to managing clusters and workloads. With intelligent, integrated software, enterprises gain improved control over cooling and power events for NVIDIA Rubin, as well as infrastructure resiliency. NVIDIA Mission Control enables faster response with rapid leak detection, unlocks access to NVIDIA’s latest efficiency innovations and maximizes AI factory productivity with autonomous recovery. NVIDIA DGX systems also support the NVIDIA AI Enterprise software platform, including NVIDIA NIM microservices, such as for the NVIDIA Nemotron-3 family of open models, data and libraries. DGX SuperPOD: The Road Ahead for Industrial AI DGX SuperPOD has long served as the blueprint for large‑scale AI infrastructure. The arrival of the Rubin platform will become the launchpad for a new generation of AI factories — systems designed to reason across thousands of steps and deliver intelligence at dramatically lower cost, helping organizations build the next wave of frontier models, multimodal systems and agentic AI applications. NVIDIA DGX SuperPOD with DGX Vera Rubin NVL72 or DGX Rubin NVL8 systems will be available in the second half of this year. See notice regarding software product information. Categories: Data Center Tags: CES 2026 | NVIDIA BlueField | NVIDIA DGX]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 22:50:10 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/dgx-superpod-rubin/</guid>
    </item>
    <item>
      <title>NVIDIA Unveils New Open Models, Data and Tools to Advance AI Across Every Industry</title>
      <link>https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</link>
      <description><![CDATA[Expanding the open model universe, NVIDIA today released new open models, data and tools to advance AI across every industry. These models — spanning the NVIDIA Nemotron family for agentic AI, the NVIDIA Cosmos platform for physical AI , the new NVIDIA Alpamayo family for autonomous vehicle development, NVIDIA Isaac GR00T for robotics and NVIDIA Clara for biomedical — will empower companies with the tools to develop real-world AI systems. NVIDIA contributes open-source training frameworks and one of the world’s largest collections of open multimodal data, including 10 trillion language training tokens, 500,000 robotics trajectories, 455,000 protein structures and 100 terabytes of vehicle sensor data. This is an unprecedented scale of diverse open resources to accelerate innovation in language, robots, scientific research and autonomous vehicles. Leading technology companies — including Bosch, CodeRabbit, CrowdStrike, Cohesity, Fortinet, Franka Robotics, Humanoid, Palantir, Salesforce, ServiceNow, Hitachi and Uber — are adopting and building on NVIDIA’s open model technologies. NVIDIA Nemotron Brings Speech, Multimodal Intelligence and Safety to AI Agents Building on the recently released NVIDIA Nemotron 3 family of open models and data, NVIDIA is releasing Nemotron models for speech, multimodal retrieval-augmented generation (RAG) and safety. Nemotron Speech comprises leaderboard-topping open models, including a new ASR model , that deliver real-time, low-latency speech recognition for live captions and speech AI applications. Daily and Modal benchmarks show that the model delivers 10x faster performance than other models in its class. Nemotron RAG comprises new embed and rerank vision language models (VLMs) that provide highly accurate multilingual and multimodal data insights to enhance document search and information retrieval. Nemotron Safety models, which strengthen the safety and trustworthiness of AI applications, now include the Llama Nemotron Content Safety model, featuring expanded language support, and Nemotron PII , which detects sensitive data with high accuracy. Bosch is adopting Nemotron Speech to enable drivers to interact with their vehicles. ServiceNow trains its Apriel model family on open datasets, including Nemotron for cost-efficient multimodal performance. Cadence and IBM are piloting NVIDIA Nemotron RAG models to improve search and reasoning across complex technical documents. CrowdStrike, Cohesity and Fortinet are adopting NVIDIA Nemotron Safety models to strengthen the trustworthiness of their AI applications. Palantir is integrating Nemotron models into its Ontology framework to build a first-of-its-kind, integrated technology stack for specialized AI agents. CodeRabbit is using Nemotron models to power and scale its AI code reviews, improving speed and cost efficiency while maintaining high review accuracy. NVIDIA is also releasing open-source datasets, training resources and blueprints to developers, including the dataset and training code for the Llama Embed Nemotron 8B model, featured on the MMTEB leaderboard . This is in addition to the updated LLM Router that shows developers how to automatically direct AI requests to the best model for the job, and the dataset used to build the new Nemotron Speech ASR model. New Models for Every Type of Physical AI and Robot Developing physical AI for robots and autonomous systems requires large, diverse datasets and models that can perceive, reason and act in complex, real-world environments. On Hugging Face, robotics is the fastest-growing segment, with NVIDIA’s open robotics models and datasets leading the platform’s downloads . NVIDIA is releasing NVIDIA Cosmos open world foundation models that bring humanlike reasoning and world generation to accelerate physical AI development and validation. Cosmos Reason 2 is a new, leaderboard-topping reasoning VLM that helps robots and AI agents see, understand and interact with higher accuracy in the physical world. Cosmos Transfer 2.5 and Cosmos Predict 2.5 are leading models that generate large-scale synthetic videos across diverse environments and conditions. NVIDIA has also released open models and blueprints for each physical AI embodiment, built on Cosmos: Isaac GR00T N1.6 is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding. The NVIDIA Blueprint for video search and summarization , part of the NVIDIA Metropolis platform, is a reference workflow for building vision AI agents that can analyze large volumes of recorded and live video to improve operational efficiency and public safety. Salesforce , Milestone , Hitachi, Uber, VAST Data and Encord are using Cosmos Reason for traffic and workplace productivity AI agents. Franka Robotics, Humanoid and NEURA Robotics are using Isaac GR00T to simulate, train and validate new behaviors for robots before scaling to production. NVIDIA Alpamayo for Reasoning-Based Autonomous Vehicles Developing safe, scalable autonomous driving depends on AI that can perceive, reason and act in complex real-world environments and scenarios, with development workflows that support rapid training, testing and improvement at scale. NVIDIA is releasing NVIDIA Alpamayo, a new family of open models, simulation tools and large datasets to advance reasoning-based autonomous vehicle development. It includes: Alpamayo 1 , the first open, large-scale reasoning VLA model for autonomous vehicles (AVs) that enables vehicles to understand their surroundings, as well as explain their actions.​ AlpaSim , an open-source simulation framework that enables closed-loop training and evaluation of reasoning-based AV models across diverse environments and edge cases. NVIDIA is also releasing Physical AI Open Datasets , including over 1,700 hours of driving data collected across the widest range of geographies and conditions, covering rare and complex real-world edge cases essential for advancing reasoning architectures. NVIDIA Clara for Healthcare and Life Sciences To lower costs and deliver treatments faster, NVIDIA is launching new Clara AI models that bridge the gap between digital discovery and real-world medicine. Helping researchers design treatments that are safer, more effective and easier to produce, these models include: La-Proteina enables the design of large, atom-level-precise proteins for research and drug candidate development, giving scientists new tools to study diseases previously considered untreatable. ReaSyn v2 ensures AI-designed drugs are practical to synthesize by incorporating a manufacturing blueprint into the discovery process. KERMT provides high-accuracy, computational safety testing early in development by predicting how a potential drug will interact with the human body. RNAPro unlocks the potential of personalized medicine by predicting the complex 3D shapes of RNA molecules. In addition, an NVIDIA dataset of 455,000 synthetic protein structures helps AI researchers build more accurate AI models. Get Started With NVIDIA Open Models and Technologies NVIDIA open models, data and frameworks are now available on GitHub and Hugging Face and from a range of cloud, inference and AI infrastructure platforms, as well as build.nvidia.com , giving developers flexible access to supporting resources. Many of these models are also available as NVIDIA NIM microservices for secure, scalable deployment on any NVIDIA-accelerated infrastructure, from the edge to the cloud. Learn more by watching NVIDIA Live at CES . Categories: Driving | Generative AI | Robotics Tags: Agentic AI | CES 2026 | Cosmos | Healthcare and Life Sciences | Nemotron | NVIDIA Clara | NVIDIA NIM | Open Source | Physical AI]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 21:50:50 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</guid>
    </item>
    <item>
      <title>Marine Biological Laboratory Explores Human Memory With AI and Virtual Reality</title>
      <link>https://blogs.nvidia.com/blog/mbl-human-memory-ai-vr-rtx/</link>
      <description><![CDATA[The works of Plato state that when humans have an experience, some level of change occurs in their brain, which is powered by memory — specifically long-term memory. This change is what Andre Fenton, professor of neural science at New York University, and Abhishek Kumar, assistant professor of cell and regenerative biology at the University of Wisconsin–Madison, are studying at the Marine Biological Laboratory (MBL) in Woods Hole, Massachusetts. “My life’s work is to understand how minds operate, and especially to understand memory — not merely as a trace of the past in the brain but as an estimate of the future that the brain is afforded,” Fenton said. The researchers have upleveled their project by harnessing NVIDIA RTX GPUs and HP Z Workstations to visualize massive datasets and by integrating custom AI tools and syGlass , a virtual-reality (VR) platform for scientific exploration. This project is additionally supported by grants from the National Institute of Mental Health and the Chan Zuckerberg Initiative. A Neural Forest Uncovered Memory is the job of the brain’s hippocampus. This C-shaped structure, resembling a seahorse, is the main focus of the MBL research group. Fenton describes the cells within the hippocampus as a forest, where billions of neurons look like tiny tree trunks and the lines coming off the trunks look like leaves. Projection image of neuronal cell nuclei (left) and dendrites (right) or branched extensions of a nerve cell. Images were acquired by Matthew Parent and Daryl Watkins. The team is studying a small portion of these “leaves” — representing protein markers: an incredibly tedious task due to their length, at about a micrometer each. A researcher must search through the forest of brain cells to find the correct protein markers, which make up only about 1% of all protein markers in the hippocampus. The researchers were looking to ease the process of studying these proteins and what their varying structures may reveal about memory encoding. Collecting and analyzing enough 3D volumetric data on protein markers was a bottleneck within the project until NVIDIA and HP technologies were introduced into the workflow. “This is a massive computational challenge, and the HP and NVIDIA technologies have enabled us to do the first step: capture, check and store the 3D image data,” Fenton said. Using these technologies, the MBL researchers captured 10 terabytes of volumetric data and then performed human visual-quality inspections. Understanding Memory Could Prevent Neurological Diseases The team’s ultimate goal of discovering the function of memory at a molecular level can boost research into the root causes of brain diseases tied to neurocognition, such as Alzheimer’s and dementia. “People don’t normally think of memory as part of their mental health, but almost all mental dysfunction depends on what your brain stores — the beliefs, the anticipations, the anxieties that you have and the things that you expect,” said Fenton. “These are all different aspects of what happens when you have a memory, so almost all neuropsychiatric illnesses and manipulation depend on this understanding.” As a step toward solving these large-scale problems, the researchers are looking at how memory is affected when proteins go to incorrect locations in the hippocampus. The team is also examining the correlation between the structure and function of brain cells through high-resolution 3D images curated and stored using syGlass on the HP Z high performance workstation powered and supported with multiple NVIDIA RTX GPUs. “If we can understand how something is built, then if there’s a problem, we can dissect that and get to the bottom of it,” said Kumar. “That’s what we’re trying to do: understand how we retain memory, so if a problem arises, we know how to fix it.” Enabling Virtual Reality and Student Exploration The use of syGlass on the HP Z6 desktop workstation, running on NVIDIA RTX GPUs, turned the researchers’ endeavor from a time-consuming operation into an interactive scientific exploration — ideal for high-school-student engagement. “The HP-NVDIA-syGlass system lets us innovate by engaging three high-school interns,” said Kumar. “They had an abstract interest in our science, and we recognized that the syGlass virtual experience might enthrall them. We were right.” The researchers brought these three curious students into their lab this summer to analyze the memory proteins using the VR headsets, which allowed for 3D visuals of the data. Their task was to find the specific proteins that were memory-related and label them as such. While this may sound like a simple task, the interns had to sift through a sea of billions of neurons to find only a few thousand protein markers that were relevant to the research. High school intern using the syGlass VR headset to identify protein markers. Image taken by Andre Fenton. Due to the success of this pilot program, the team is now looking to expand high-school research opportunities for the project. “Why leave it at three students?” said Fenton. “Next year, it could be 10 at multiple locations helping us learn about brains while they learn about brains.” Learn more about NVIDIA-accelerated academic research . Categories: Generative AI | Research | Workstation Tags: Education | NVIDIA RTX | Science | Scientific Visualization | Virtual Reality]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 22 Dec 2025 16:00:10 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/mbl-human-memory-ai-vr-rtx/</guid>
    </item>
    <item>
      <title>Now Generally Available, NVIDIA RTX PRO 5000 72GB Blackwell GPU Expands Memory Options for Desktop Agentic AI</title>
      <link>https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</link>
      <description><![CDATA[Top-notch options for AI at the desktops of developers, engineers and designers are expanding. The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available, bringing robust agentic and generative AI capabilities powered by the NVIDIA Blackwell architecture to more desktops and professionals across the world. The new GPU configuration offers AI developers, data scientists and creative professionals the hardware for modern, memory-hungry workflows — and arrives at a time when demand for NVIDIA Blackwell-class compute is higher than ever. With the flexibility to choose between this 72GB variant and the existing NVIDIA RTX PRO 5000 48GB model, AI developers can right-size their systems for a wider range of budgets and project requirements. The NVIDIA RTX PRO 5000 Blackwell GPU. Fueling the Next Generation of AI Development As generative AI evolves into complex, multimodal agentic AI , more demand is placed on the hardware required to develop and deploy these technologies. One defining challenge of AI development is memory capacity. Running cutting-edge AI workflows — especially those involving large language models ( LLMs ) and AI agents — places significant stress on GPU memory, particularly as models, context windows and multimodal pipelines grow in size and complexity. Agentic AI systems involve chains of tools, retrieval-augmented generation ( RAG ), and multimodal understanding. These systems often need to keep multiple AI models, data sources and code formats active simultaneously within the GPU’s memory. Built on NVIDIA Blackwell — which delivers high throughput for AI, neural rendering and simulation with multi-workload scheduling and other architectural innovations — the RTX PRO 5000 72GB helps solve this bottleneck, offering 2,142 TOPS of AI performance. Plus, with 72GB of ultrafast GDDR7 memory — a 50% increase over the 48GB model — developers can train, fine-tune and prototype larger models locally. This enables users to maintain data privacy, low latency and cost efficiency, allowing teams to serve models directly from their workstations rather than relying on data-center-scale infrastructure for every AI task. Performance Enhancements For local AI development, raw compute is only half the battle — memory capacity determines what users can run, and throughput determines how fast it runs. In industry-standard benchmarks for generative AI, the RTX PRO 5000 72GB offers 3.5x the performance of prior-generation NVIDIA hardware for image generation, and 2x the performance of prior-generation hardware for text generation. In creative workflows, time saved in rendering is time gained for iteration. Across path-tracing engines like Arnold, Chaos V-Ray and Blender, as well as real-time GPU renderers like D5 Render and Redshift, the RTX PRO 5000 72GB slashes render times by up to 4.7x. And for computer-aided engineering and product design, the RTX PRO 5000 72GB offers more than 2x graphics performance. InfinitForm Optimizes Generative AI-Powered Design With RTX PRO 5000 InfinitForm, a provider of generative AI software for engineering design and a member of the NVIDIA Inception program for startups, is an early adopter of the RTX PRO 5000 72GB Blackwell GPU. The company is using the GPU to optimize its software with enhanced performance and speed, enabling advanced simulations to streamline processes for computer-aided design and manufacturing. “InfinitForm is thrilled to evaluate its CUDA-accelerated generative AI design optimization software on NVIDIA RTX PRO 5000 72GB to help customers like Yamaha Motor and NASA accelerate innovation and optimize products for performance and manufacturability,” said Michael Bogomolny, founder and CEO of InfinitForm. Versatile Media Accelerates Virtual Production Workflows With RTX PRO 5000 For creative professionals, such as those at Versatile Media — a global media company specializing in virtual production — the RTX PRO 5000 can significantly enhance real-time rendering performance for large-scale scenes and complex assets, delivering a meaningful leap in efficiency. As 3D pipelines now often integrate AI denoisers, generative tools and real-time physics, the new GPU’s 72GB memory capacity allows for the manipulation of massive 3D scenes and asset libraries without slowing down the creative flow. An early adopter of the new GPU, Versatile Media plans to use it to design a series of complex, high-resolution real-time rendering scenarios that take advantage of the GPU’s memory capacity. “For film-grade virtual production, memory capacity directly translates into creative freedom,” said Eddy Shen, general manager of the production center at Versatile Media. “With 72GB of GPU memory, the RTX PRO 5000 enables us to iterate with higher-resolution scenes and more complex lighting in real time without compromising performance.” Availability and Ecosystem Support The NVIDIA RTX PRO 5000 72GB Blackwell GPU is now generally available from partners including Ingram Micro, Leadtek, Unisplendour and xFusion, providing manufacturers and systems integrators with a powerful new option to anchor AI-ready workstations. Broader availability through global system builders will start early next year. As industries race to integrate AI into every facet of operation — from generative design to coding copilots — RTX PRO 5000 72GB is equipped to meet the moment. Learn more about the NVIDIA RTX PRO 5000 GPU family and join NVIDIA at SIGGRAPH Asia to discover how graphics and simulation innovations come together to drive creativity, industrial digitalization, robotics, and physical and spatial AI. Categories: Data Center | Generative AI | Pro Graphics Tags: Agentic AI | Artificial Intelligence | GPU | Hardware | NVIDIA RTX | Simulation and Design]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 18 Dec 2025 16:00:28 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-pro-5000-72gb-blackwell-gpu/</guid>
    </item>
    <item>
      <title>UC San Diego Lab Advances Generative AI Research With NVIDIA DGX B200 System</title>
      <link>https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</link>
      <description><![CDATA[The Hao AI Lab research team at the University of California San Diego — at the forefront of pioneering AI model innovation — recently received an NVIDIA DGX B200 system to elevate their critical work in large language model inference . Many LLM inference platforms in production today, such as NVIDIA Dynamo , use research concepts that originated in the Hao AI Lab, including DistServe . How Is Hao AI Lab Using the DGX B200? Members of the Hao AI Lab standing with the NVIDIA DGX B200 system. With the DGX B200 now fully accessible to the Hao AI Lab and broader UC San Diego community at the School of Computing, Information and Data Sciences’ San Diego Supercomputer Center , the research opportunities are boundless. “DGX B200 is one of the most powerful AI systems from NVIDIA to date, which means that its performance is among the best in the world,” said Hao Zhang, assistant professor in the Halıcıoğlu Data Science Institute and department of computer science and engineering at UC San Diego. “It enables us to prototype and experiment much faster than using previous-generation hardware.” Two Hao AI Lab projects the DGX B200 is accelerating are FastVideo and the Lmgame benchmark. FastVideo focuses on training a family of video generation models to produce a five-second video based on a given text prompt — in just five seconds. The research phase of FastVideo taps into NVIDIA H200 GPUs in addition to the DGX B200 system. Lmgame-bench is a benchmarking suite that puts LLMs to the test using popular online games including Tetris and Super Mario Bros . Users can test one model at a time or put two models up against each other to measure their performance. The illustrated workflow of Hao AI Lab’s Lmgame-Bench project. Other ongoing projects at Hao AI Labs explore new ways to achieve low-latency LLM serving, pushing large language models toward real-time responsiveness. “Our current research uses the DGX B200 to explore the next frontier of low-latency LLM-serving on the awesome hardware specs the system gives us,” said Junda Chen, a doctoral candidate in computer science at UC San Diego. How DistServe Influenced Disaggregated Serving Disaggregated inference is a way to ensure large-scale LLM-serving engines can achieve the optimal aggregate system throughput while maintaining acceptably low latency for user requests. The benefit of disaggregated inference lies in optimizing what DistServe calls “goodput” instead of “throughput” in the LLM-serving engine. Here’s the difference: Throughput is measured by the number of tokens per second that the entire system can generate. Higher throughput means lower cost to generate each token to serve the user. For a long time, throughput was the only metric used by LLM-serving engines to measure their performance against one another. While throughput measures the aggregate performance of the system, it doesn’t directly correlate to the latency that a user perceives. If a user demands lower latency to generate the tokens, the system has to sacrifice throughput. This natural trade-off between throughput and latency is what led the DistServe team to propose a new metric, “goodput”: the measure of throughput while satisfying the user-specified latency objectives, usually called service-level objectives. In other words, goodput represents the overall health of a system while satisfying user experience. DistServe shows that goodput is a much better metric for LLM-serving systems, as it factors in both cost and service quality. Goodput leads to optimal efficiency and ideal output from a model. How Can Developers Achieve Optimal Goodput? When a user makes a request in an LLM system, the system takes the user input and generates the first token, known as prefill. Then, the system creates numerous output tokens, one after another, predicting each token’s future behavior based on past requests’ outcomes. This process is known as decode. https://blogs.nvidia.com/wp-content/uploads/2025/12/distserve.mp4 Prefill and decode have historically run on the same GPU, but the researchers behind DistServe found that splitting them onto different GPUs maximizes goodput. “Previously, if you put these two jobs on a GPU, they would compete with each other for resources, which could make it slow from a user perspective,” Chen said. “Now, if I split the jobs onto two different sets of GPUs — one doing prefill, which is compute intensive, and the other doing decode, which is more memory intensive — we can fundamentally eliminate the interference between the two jobs, making both jobs run faster. This process is called prefill/decode disaggregation, or separating the prefill from decode to get greater goodput. Increasing goodput and using the disaggregated inference method enables the continuous scaling of workloads without compromising on low-latency or high-quality model responses. NVIDIA Dynamo — an open-source framework designed to accelerate and scale generative AI models at the highest efficiency levels with the lowest cost — enables scaling disaggregated inference. In addition to these projects, cross-departmental collaborations, such as in healthcare and biology, are underway at UC San Diego to further optimize an array of research projects using the NVIDIA DGX B200, as researchers continue exploring how AI platforms can accelerate innovation. Learn more about the NVIDIA DGX B200 system. Categories: Data Center | Generative AI | Research | Supercomputing Tags: Artificial Intelligence | Education | Inference | NVIDIA DGX | Open Source]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 17 Dec 2025 16:00:15 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ucsd-generative-ai-research-dgx-b200/</guid>
    </item>
    <item>
      <title>How to Fine-Tune an LLM on NVIDIA GPUs With Unsloth</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</link>
      <description><![CDATA[Modern workflows showcase the endless possibilities of generative and agentic AI on PCs. Of many, some examples include tuning a chatbot to handle product-support questions or building a personal assistant for managing one’s schedule. A challenge remains, however, in getting a small language model to respond consistently with high accuracy for specialized agentic tasks. That’s where fine-tuning comes in. Unsloth , one of the world’s most widely used open-source frameworks for fine-tuning LLMs, provides an approachable way to customize models. It’s optimized for efficient, low-memory training on NVIDIA GPUs — from GeForce RTX desktops and laptops to RTX PRO workstations and DGX Spark , the world’s smallest AI supercomputer. Another powerful starting point for fine-tuning is the just-announced NVIDIA Nemotron 3 family of open models, data and libraries. Nemotron 3 introduces the most efficient family of open models, ideal for agentic AI fine-tuning. Teaching AI New Tricks Fine-tuning is like giving an AI model a focused training session. With examples tied to a specific topic or workflow, the model improves its accuracy by learning new patterns and adapting to the task at hand. Choosing a fine-tuning method for a model depends on how much of the original model the developer wants to adjust. Based on their goals, developers can use one of three main fine-tuning methods: Parameter-efficient fine-tuning (such as LoRA or QLoRA) : How it works: Updates only a small portion of the model for faster, lower-cost training. It’s a smarter and efficient way to enhance a model without altering it drastically. Target use case: Useful across nearly all scenarios where full fine-tuning would traditionally be applied — including adding domain knowledge, improving coding accuracy, adapting the model for legal or scientific tasks, refining reasoning, or aligning tone and behavior. Requirements: Small- to medium-sized dataset (100-1,000 prompt-sample pairs). Full fine-tuning : How it works: Updates all of the model’s parameters — useful for teaching the model to follow specific formats or styles. Target use case: Advanced use cases, such as building AI agents and chatbots that must provide assistance about a specific topic, stay within a certain set of guardrails and respond in a particular manner. Requirements: Large dataset (1,000+ prompt-sample pairs). Reinforcement learning : How it works: Adjusts the behavior of the model using feedback or preference signals. The model learns by interacting with its environment and uses the feedback to improve itself over time. This is a complex, advanced technique that interweaves training and inference — and can be used in tandem with parameter-efficient fine-tuning and full fine-tuning techniques. See Unsloth’s Reinforcement Learning Guide for details. Target use case: Improving the accuracy of a model in a particular domain — such as law or medicine — or building autonomous agents that can orchestrate actions on a user’s behalf. Requirements: A process that contains an action model, a reward model and an environment for the model to learn from. Another factor to consider is the VRAM required per each method. The chart below provides an overview of the requirements to run each type of fine-tuning method on Unsloth. Fine-tuning requirements on Unsloth. Unsloth: A Fast Path to Fine-Tuning on NVIDIA GPUs LLM fine-tuning is a memory- and compute-intensive workload that involves billions of matrix multiplications to update model weights at every training step. This type of heavy parallel workload requires the power of NVIDIA GPUs to complete the process quickly and efficiently. Unsloth shines at this workload, translating complex mathematical operations into efficient, custom GPU kernels to accelerate AI training. Unsloth helps boost the performance of the Hugging Face transformers library by 2.5x on NVIDIA GPUs. These GPU-specific optimizations, combined with Unsloth’s ease of use, make fine-tuning accessible to a broader community of AI enthusiasts and developers. The framework is built and optimized for NVIDIA hardware — from GeForce RTX laptops to RTX PRO workstations and DGX Spark — providing peak performance while reducing VRAM consumption. Unsloth provides helpful guides on how to get started and manage different LLM configurations, hyperparameters and options, along with example notebooks and step-by-step workflows. Check out some of these Unsloth guides: Fine-Tuning LLMs With NVIDIA RTX 50 Series GPUs and Unsloth Fine-Tuning LLMs With NVIDIA DGX Spark and Unsloth Learn how to install Unsloth on NVIDIA DGX Spark . Read the NVIDIA technical blog for a deep dive of fine-tuning and reinforcement learning on the NVIDIA Blackwell platform. For a hands-on local fine-tuning walkthrough, watch Matthew Berman showing reinforcement learning running on a NVIDIA GeForce RTX 5090 using Unsloth in the video below. Available Now: NVIDIA Nemotron 3 Family of Open Models The new Nemotron 3 family of open models — in Nano, Super, and Ultra sizes — built on a new hybrid latent Mixture-of-Experts (MoE) architecture, introduces the most efficient family of open models with leading accuracy, ideal for building agentic AI applications. Nemotron 3 Nano 30B-A3B, available now, is the most compute-efficient model in the lineup. It’s optimized for tasks such as software debugging, content summarization, AI assistant workflows and information retrieval at low inference costs. Its hybrid MoE design delivers: Up to 60% fewer reasoning tokens, significantly reducing inference cost. A 1 million-token context window, allowing the model to retain far more information for long, multistep tasks. Nemotron 3 Super is a high-accuracy reasoning model for multi-agent applications, while Nemotron 3 Ultra is for complex AI applications. Both are expected to be available in the first half of 2026. NVIDIA also released today an open collection of training datasets and state-of-the-art reinforcement learning libraries. Nemotron 3 Nano fine-tuning is available on Unsloth. Download Nemotron 3 Nano now from Hugging Face , or experiment with it through Llama.cpp and LM Studio. DGX Spark: A Compact AI Powerhouse DGX Spark enables local fine-tuning and brings incredible AI performance in a compact, desktop supercomputer, giving developers access to more memory than a typical PC. Built on the NVIDIA Grace Blackwell architecture, DGX Spark delivers up to a petaflop of FP4 AI performance and includes 128GB of unified CPU-GPU memory, giving developers enough headroom to run larger models, longer context windows and more demanding training workloads locally. For fine-tuning, DGX Spark enables: Larger model sizes. Models with more than 30 billion parameters often exceed the VRAM capacity of consumer GPUs but fit comfortably within DGX Spark’s unified memory. More advanced techniques. Full fine-tuning and reinforcement-learning-based workflows — which demand more memory and higher throughput — run significantly faster on DGX Spark. Local control without cloud queues. Developers can run compute-heavy tasks locally instead of waiting for cloud instances or managing multiple environments. DGX Spark’s strengths go beyond LLMs. High-resolution diffusion models, for example, often require more memory than a typical desktop can provide. With FP4 support and large unified memory, DGX Spark can generate 1,000 images in just a few seconds and sustain higher throughput for creative or multimodal pipelines. The table below shows performance for fine-tuning the Llama family of models on DGX Spark. Performance for fine-tuning Llama family of models on DGX Spark. As fine-tuning workflows advance, the new Nemotron 3 family of open models offer scalable reasoning and long-context performance optimized for RTX systems and DGX Spark. Learn more about how DGX Spark enables intensive AI tasks . #ICYMI — The Latest Advancements in NVIDIA RTX AI PCs 🚀 FLUX.2 Image-Generation Models Now Released, Optimized for NVIDIA RTX GPUs The new models from Black Forest Labs are available in FP8 quantizations that reduce VRAM and increase performance by 40%. ✨ Nexa.ai Expands Local AI on RTX PCs With Hyperlink for Agentic Search The new on-device search agent delivers 3x faster retrieval-augmented generation indexing and 2x faster LLM inference, indexing a dense 1GB folder from about 15 minutes to just four to five minutes. Plus, DeepSeek OCR now runs locally in GGUF via NexaSDK, offering plug-and-play parsing of charts, formulas and multilingual PDFs on RTX GPUs. 🤝Mistral AI Unveils New Model Family Optimized for NVIDIA GPUs The new Mistral 3 models are optimized from cloud to edge and available for fast, local experimentation through Ollama and Llama.cpp. 🎨Blender 5.0 Lands With HDR Color and Major Performance Gains The release adds ACES 2.0 wide-gamut/HDR color, NVIDIA DLSS for up to 5x faster hair and fur rendering, better handling of massive geometry, and motion blur for Grease Pencil. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X — and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | GeForce | NVIDIA RTX | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 15 Dec 2025 14:00:11 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/</guid>
    </item>
    <item>
      <title>NVIDIA Awards up to $60,000 Research Fellowships to PhD Students</title>
      <link>https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</link>
      <description><![CDATA[For 25 years, the NVIDIA Graduate Fellowship Program has supported graduate students doing outstanding work relevant to NVIDIA technologies. Today, the program announced the latest awards of up to $60,000 each to 10 Ph.D. students involved in research that spans all areas of computing innovation. Selected from a highly competitive applicant pool, the awardees will participate in a summer internship preceding the fellowship year. Their work puts them at the forefront of accelerated computing — tackling projects in autonomous systems, computer architecture, computer graphics, deep learning, programming systems, robotics and security. The NVIDIA Graduate Fellowship Program is open to applicants worldwide. The 2026-2027 fellowship recipients are: Jiageng Mao , University of Southern California — Solving complex physical AI problems by using diverse priors from internet-scale data to enable robust, generalizable intelligence for embodied agents in the real world. Liwen Wu , University of California San Diego — Enriching realism and efficiency in physically based rendering with neural materials and neural rendering. Sizhe Chen , University of California, Berkeley — Securing AI in real-world applications, currently securing AI agents against prompt injection attacks with general and practical defenses that preserve the agent’s utility. Yunfan Jiang , Stanford University — Developing scalable approaches to build generalist robots for everyday tasks through hybrid data sources spanning real-world whole-body manipulation, large-scale simulation and internet-scale multimodal supervision. Yijia Shao , Stanford University — Researching human-agent collaboration by developing AI agents that can communicate and coordinate with humans during task execution, and designing new human-agent interaction interfaces. Shangbin Feng , University of Washington — Advancing model collaboration: multiple machine learning models, trained on different data and by different people, collaborate, compose and complement each other for an open, decentralized and collaborative AI future. Shvetank Prakash , Harvard University — Advancing hardware architecture and systems design with AI agents built on new algorithms, curated datasets and agent-first infrastructure. Irene Wang , Georgia Institute of Technology — Developing a holistic codesign framework that integrates accelerator architecture, network topology and runtime scheduling to enable energy-efficient and sustainable AI training at scale. Chen Geng , Stanford University — Modeling 4D physical worlds with scalable data-driven algorithms and physics-inspired principles, advancing physically grounded 3D and 4D world models for robotics and scientific applications. Alexander Root , Stanford University — Designing programming systems for productive exploration of work-efficiency, compute and memory tradeoffs in irregular applications such as sparse array programming and path tracing. We also acknowledge the 2026-2027 fellowship finalists: Zizheng Guo , Peking University Peter Holderrieth , Massachusetts Institute of Technology Xianghui Xie , Max Planck Institute for Informatics Daniel Palenicek , Technical University of Darmstadt Categories: Generative AI | Research Tags: Artificial Intelligence | Education]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 04 Dec 2025 17:00:44 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/graduate-fellowship-recipients-2026-2027/</guid>
    </item>
    <item>
      <title>NVIDIA and AWS Expand Full-Stack Partnership, Providing the Secure, High-Performance Compute Platform Vital for Future Innovation</title>
      <link>https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</link>
      <description><![CDATA[At AWS re:Invent, NVIDIA and Amazon Web Services expanded their strategic collaboration with new technology integrations across interconnect technology, cloud infrastructure, open models and physical AI. As part of this expansion, AWS will support NVIDIA NVLink Fusion — a platform for custom AI infrastructure — for deploying its custom-designed silicon, including next-generation Trainium4 chips for inference and agentic AI model training, Graviton CPUs for a broad range of workloads and the Nitro System virtualization infrastructure. Using NVIDIA NVLink Fusion, AWS will combine NVIDIA NVLink scale-up interconnect and the NVIDIA MGX rack architecture with AWS custom silicon to increase performance and accelerate time to market for its next-generation cloud-scale AI capabilities. AWS is designing Trainium4 to integrate with NVLink and NVIDIA MGX, the first of a multigenerational collaboration between NVIDIA and AWS for NVLink Fusion. AWS has already deployed MGX racks at scale with NVIDIA GPUs. Integrating NVLink Fusion will allow AWS to further simplify deployment and systems management across its platforms. AWS can also harness the NVLink Fusion supplier ecosystem, which provides all the components required for full rack-scale deployment, from the rack and chassis, to power-delivery and cooling systems. By supporting AWS’s Elastic Fabric Adapter and Nitro System, the NVIDIA Vera Rubin architecture on AWS will give customers robust networking choices while maintaining full compatibility with AWS’s cloud infrastructure and accelerating new AI service rollout. “GPU compute demand is skyrocketing — more compute makes smarter AI, smarter AI drives broader use and broader use creates demand for even more compute. The virtuous cycle of AI has arrived,” said Jensen Huang, founder and CEO of NVIDIA. “With NVIDIA NVLink Fusion coming to AWS Trainium4, we’re unifying our scale-up architecture with AWS’s custom silicon to build a new generation of accelerated platforms. Together, NVIDIA and AWS are creating the compute fabric for the AI industrial revolution — bringing advanced AI to every company, in every country, and accelerating the world’s path to intelligence.” “AWS and NVIDIA have worked side by side for more than 15 years, and today marks a new milestone in that journey,” said Matt Garman, CEO of AWS. “With NVIDIA, we’re advancing our large-scale AI infrastructure to deliver customers the highest performance, efficiency and scalability. The upcoming support of NVIDIA NVLink Fusion in AWS Trainium4, Graviton and the Nitro System will bring new capabilities to customers so they can innovate faster than ever before.” Convergence of Scale and Sovereignty AWS has expanded its accelerated computing portfolio with the NVIDIA Blackwell architecture, including NVIDIA HGX B300 and NVIDIA GB300 NVL72 GPUs, giving customers immediate access to the industry’s most advanced GPUs for training and inference. Availability of NVIDIA RTX PRO 6000 Blackwell Server Edition GPUs, designed for visual applications, on AWS is expected in the coming weeks. These GPUs form part of the AWS infrastructure backbone powering AWS AI Factories, a new AI cloud offering that will provide customers around the world with the dedicated infrastructure they need to harness advanced AI services and capabilities in their own data centers, operated by AWS, while also letting customers maintain control of their data and comply with local regulations. NVIDIA and AWS are committing to deploy sovereign AI clouds globally and bring the best of AI innovation to the world. With the launch of AWS AI Factories, the companies are providing secure, sovereign AI infrastructure to deliver unprecedented computing capabilities for organizations around the world while meeting increasingly rigorous sovereign AI requirements. For public sector organizations, AWS AI Factories will transform the federal supercomputing and AI landscape. AWS AI Factories customers will be able to seamlessly integrate AWS’s industry-leading cloud infrastructure and services — known for its reliability, security and scalability — with NVIDIA Blackwell GPUs and the full-stack NVIDIA accelerated computing platform, including NVIDIA Spectrum-X Ethernet switches. The unified architecture will ensure customers can access advanced AI services and capabilities, as well as train and deploy massive models, while maintaining absolute control of proprietary data and full compliance with local regulatory frameworks. NVIDIA Nemotron Integration With Amazon Bedrock Expands Software Optimizations Beyond hardware, the partnership expands integration of NVIDIA’s software stack with the AWS AI ecosystem. NVIDIA Nemotron open models are now integrated with Amazon Bedrock , enabling customers to build generative AI applications and agents at production scale. Developers can access Nemotron Nano 2 and Nemotron Nano 2 VL to build specialized agentic AI applications that process text, code, images and video with high efficiency and accuracy. The integration makes high-performance, open NVIDIA models instantly accessible via Amazon Bedrock’s serverless platform where customers can rely on proven scalability and zero infrastructure management. Industry leaders CrowdStrike and BridgeWise are the first to use the service to deploy specialized AI agents. NVIDIA Software on AWS Simplifies Developer Experience NVIDIA and AWS are also co-engineering at the software layer to accelerate the data backbone of every enterprise. Amazon OpenSearch Service now offers serverless GPU acceleration for vector index building, powered by NVIDIA cuVS , an open-source library for GPU-accelerated vector search and data clustering. This milestone represents a fundamental shift to using GPUs for unstructured data processing, with early adopters seeing up to 10x faster vector indexing at a quarter of the cost. These dramatic gains reduce search latency, accelerate writes and unlock faster productivity for dynamic AI techniques like retrieval-augmented generation by delivering the right amount of GPU power precisely when it’s needed. AWS is the first major cloud provider to offer serverless vector indexing with NVIDIA GPUs. Production-ready AI agents require performance visibility, optimization and scalable infrastructure. By combining Strands Agents for agent development and orchestration, the NVIDIA NeMo Agent Toolkit for deep profiling and performance tuning, and Amazon Bedrock AgentCore for secure, scalable agent infrastructure, organizations can empower developers with a complete, predictable path from prototype to production. This expanded support builds on AWS’s existing integrations with NVIDIA technologies — including NVIDIA NIM microservices and frameworks like NVIDIA Riva and NVIDIA BioNeMo , as well as model development tools integrated with Amazon SageMaker and Amazon Bedrock — that enable organizations to deploy agentic AI, speech AI and scientific applications faster than ever. Accelerating Physical AI With AWS Developing physical AI demands high-quality and diverse datasets for training robot models, as well as frameworks for testing and validation in simulation before real-world deployment. NVIDIA Cosmos world foundation models (WFMs) are now available as NVIDIA NIM microservices on Amazon EKS , enabling real-time robotics control and simulation workloads with seamless reliability and cloud-native efficiency. For batch-based tasks and offline workloads such as large-scale synthetic data generation , Cosmos WFMs are also available on AWS Batch as containers. Cosmos-generated world states can then be used to train and validate robots using open-source simulation and learning frameworks such as NVIDIA Isaac Sim and Isaac Lab . Leading robotics companies such as Agility Robotics, Agile Robots, ANYbotics, Diligent Robotics, Dyna Robotics, Field AI, Haply Robotics, Lightwheel, RIVR and Skild AI are using the NVIDIA Isaac platform with AWS for use cases ranging from collecting, storing and processing robot-generated data to training and simulation for scaling robotics development. Sustained Collaboration Underscoring years of continued collaboration, NVIDIA earned the AWS Global GenAI Infrastructure and Data Partner of the Year award, which recognizes top technology partners with the Generative AI Competency that support vector embeddings, data storage and management or synthetic data generation in multiple types and formats. Learn more about NVIDIA and AWS’s collaboration and join sessions at AWS re:Invent , running through Friday, Dec. 5, in Las Vegas. Categories: Cloud | Data Center | Generative AI | Hardware | Networking | Robotics | Software Tags: Cosmos | Events | Isaac | Nemotron | NVIDIA Blackwell | NVIDIA NIM | NVIDIA Spectrum-X Ethernet | NVLink | Open Source | Physical AI | Riva]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 02 Dec 2025 16:00:27 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/aws-partnership-expansion-reinvent/</guid>
    </item>
    <item>
      <title>At NeurIPS, NVIDIA Advances Open Model Development for Digital and Physical AI</title>
      <link>https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</link>
      <description><![CDATA[Researchers worldwide rely on open-source technologies as the foundation of their work. To equip the community with the latest advancements in digital and physical AI, NVIDIA is further expanding its collection of open AI models, datasets and tools — with potential applications in virtually every research field. At NeurIPS , one of the world’s top AI conferences, NVIDIA is unveiling open physical AI models and tools to support research, including Alpamayo-R1, the world’s first industry-scale open reasoning vision language action (VLA) model for autonomous driving. In digital AI, NVIDIA is releasing new models and datasets for speech and AI safety. NVIDIA researchers are presenting over 70 papers, talks and workshops at the conference, sharing innovative projects that span AI reasoning, medical research, autonomous vehicle (AV) development and more. These initiatives deepen NVIDIA’s commitment to open source — an effort recognized by a new Openness Index from Artificial Analysis , an independent organization that benchmarks AI. The Artificial Analysis Open Index rates the NVIDIA Nemotron family of open technologies for frontier AI development among the most open in the AI ecosystem based on the permissibility of the model licenses, data transparency and availability of technical details. NVIDIA DRIVE Alpamayo-R1 Opens New Research Frontier for Autonomous Driving NVIDIA DRIVE Alpamayo-R1 (AR1) , the world’s first open reasoning VLA model for AV research, integrates chain-of-thought AI reasoning with path planning — a component critical for advancing AV safety in complex road scenarios and enabling level 4 autonomy . While previous iterations of self-driving models struggled with nuanced situations — a pedestrian-heavy intersection, an upcoming lane closure or a double-parked vehicle in a bike lane — reasoning gives autonomous vehicles the common sense to drive more like humans do. AR1 accomplishes this by breaking down a scenario and reasoning through each step. It considers all possible trajectories, then uses contextual data to choose the best route. For example, by tapping into the chain-of-thought reasoning enabled by AR1, an AV driving in a pedestrian-heavy area next to a bike lane could take in data from its path, incorporate reasoning traces — explanations on why it took certain actions — and use that information to plan its future trajectory, such as moving away from the bike lane or stopping for potential jaywalkers. https://blogs.nvidia.com/wp-content/uploads/2025/12/construction_worker.mp4 AR1’s open foundation, based on NVIDIA Cosmos Reason , lets researchers customize the model for their own non-commercial use cases, whether for benchmarking or building experimental AV applications. For post-training AR1, reinforcement learning has proven especially effective — researchers observed a significant improvement in reasoning capabilities with AR1 compared with the pretrained model. NVIDIA DRIVE Alpamayo-R1 is now available on GitHub and Hugging Face , and a subset of the data used to train and evaluate the model is available in the NVIDIA Physical AI Open Datasets . NVIDIA has also released the open-source AlpaSim framework to evaluate AR1. Learn more about reasoning VLA models for autonomous driving . Customizing NVIDIA Cosmos for Any Physical AI Use Case Developers can learn how to use and post-train Cosmos-based models using step-by-step recipes, quick-start inference examples and advanced post-training workflows now available in the Cosmos Cookbook . It’s a comprehensive guide for physical AI developers that covers every step in AI development, including data curation, synthetic data generation and model evaluation. There are virtually limitless possibilities for Cosmos-based applications. The latest examples from NVIDIA include: LidarGen , the first world model that can generate lidar data for AV simulation. Omniverse NuRec Fixer , a model for AV and robotics simulation that taps into NVIDIA Cosmos Predict to near-instantly address artifacts in neurally reconstructed data, such as blurs and holes from novel views or noisy data. Cosmos Policy , a framework for turning large pretrained video models into robust robot policies — a set of rules that dictate a robot’s behavior. ProtoMotions3 , an open-source, GPU-accelerated framework built on NVIDIA Newton and Isaac Lab for training physically simulated digital humans and humanoid robots with realistic scenes generated by Cosmos world foundation models (WFMs) . Sample outputs from the LidarGen model, built on Cosmos. The top row shows the input data with generated lidar data overlaid. The middle row shows generated and real lidar range maps. Bottom left shows the real lidar point cloud, while bottom right shows the point cloud generated by LidarGen. Policy models can be trained in NVIDIA Isaac Lab and Isaac Sim , and data generated from the policy models can then be used to post-train NVIDIA GR00T N models for robotics. Humanoid policy trained with ProtoMotions3 in Isaac Sim, with 3D background scene generated by Lyra with Cosmos WFM. NVIDIA ecosystem partners are developing their latest technologies with Cosmos WFMs. AV developer Voxel51 is contributing model recipes to the Cosmos Cookbook. Physical AI developers 1X , Figure AI, Foretellix, Gatik, Oxa, PlusAI and X-Humanoid are using WFMs for their latest physical AI applications. And researchers at ETH Zurich are presenting a NeurIPS paper that highlights using Cosmos models for realistic and cohesive 3D scene creation. NVIDIA Nemotron Additions Bolster the Digital AI Developer Toolkit NVIDIA is also releasing new multi-speaker speech AI models, a new model with reasoning capabilities and datasets for AI safety, as well as open tools to generate high-quality synthetic datasets for reinforcement learning and domain-specific model customization. These tools include: MultiTalker Parakeet : An automatic speech recognition model for streaming audio that can understand multiple speakers, even in overlapped or fast-paced conversations. Sortformer : A state-of-the-art model that can accurately distinguish multiple speakers within an audio stream — a process called diarization — in real time. Nemotron Content Safety Reasoning : A reasoning-based AI safety model that dynamically enforces custom policies across domains. Nemotron Content Safety Audio Dataset : A synthetic dataset that helps train models to detect unsafe audio content, enabling the development of guardrails that work across text and audio modalities. NeMo Gym : an open-source library that accelerates and simplifies the development of reinforcement learning environments for LLM training. NeMo Gym also contains a growing collection of ready-to-use training environments to enable Reinforcement Learning from Verifiable Reward (RLVR). NeMo Data Designer Library : Now open-sourced under Apache 2.0, this library provides an end-to-end toolkit to generate, validate and refine high-quality synthetic datasets for generative AI development, including domain-specific model customization and evaluation. NVIDIA ecosystem partners using NVIDIA Nemotron and NeMo tools to build secure, specialized agentic AI include CrowdStrike, Palantir and ServiceNow. NeurIPS attendees can explore these innovations at the Nemotron Summit , taking place today, from 4-8 p.m. PT, with an opening address by Bryan Catanzaro, vice president of applied deep learning research at NVIDIA. NVIDIA Research Furthers Language AI Innovation Of the dozens of NVIDIA-authored research papers at NeurIPS , here are a few highlights advancing language models: Audio Flamingo 3: Advancing Audio Intelligence With Fully Open Large Audio Language Models : This large audio language model is capable of reasoning across speech, sound and music. It can understand and reason audio segments up to 10 minutes in length, achieving state-of-the-art results on over 20 benchmarks. Minitron-SSM: Efficient Hybrid Language Model Compression Through Group-Aware SSM Pruning : This poster introduces a pruning method capable of compressing hybrid models, demonstrated by pruning and distilling Nemotron-H 8B from 8 billion to 4 billion parameters. The resulting model surpasses the accuracy of similarly sized models while achieving 2x faster inference throughput. Jet-Nemotron: Efficient Language Model With Post Neural Architecture Search : This work presents a cost-efficient post-training pipeline for developing new efficient language model architectures, and introduces a hybrid-architecture model family produced with the pipeline. These models match or surpass the accuracy of leading full-attention baselines while delivering substantially higher generation throughput. Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models : This project introduces a new small language model (SLM) architecture that redesigns SLMs around real-world latency rather than parameter count — achieving state-of-the-art speed and accuracy. ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models : Prolonged reinforcement learning, or ProRL, is a technique that extends model training over longer periods. In this NeurIPS poster, NVIDIA researchers describe how this methodology results in models that consistently outperform base models for reasoning. View the full list of events at NeurIPS , running through Sunday, Dec. 7, in San Diego. See notice regarding software product information. Categories: Corporate | Driving | Generative AI | Research | Robotics | Software Tags: Agentic AI | Artificial Intelligence | Cosmos | NVIDIA Research | Open Source | Physical AI | Synthetic Data Generation | Transportation]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 01 Dec 2025 17:00:48 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/</guid>
    </item>
  </channel>
</rss>
