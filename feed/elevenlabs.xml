<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>ElevenLabs Blog</title>
    <link>https://elevenlabs.io/blog</link>
    <description><![CDATA[Latest updates from ElevenLabs]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 28 Jan 2026 09:47:25 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Introducing Eleven v3 (alpha)</title>
      <link>https://elevenlabs.io/blog/eleven-v3</link>
      <description><![CDATA[We're pleased to reveal Eleven v3 (alpha) — themost expressive Text to Speech model. This research preview brings unprecedented control and realism to speech generation with: 70+ languages Multi-speaker dialogue Audio tagslike [excited], [whispers], and [sighs] Eleven v3 (alpha)requires moreprompt engineeringthan previous models — but the generations are breathtaking. If you’re working on videos, audiobooks, or media tools — this unlocks a new level of expressiveness. For real-time and conversational use cases, we recommend staying with v2.5 Turbo or Flash for now. A real-time version of v3 is in development. Eleven v3 is available today on our website and in theAPI. Why we built v3 Since launching Multilingual v2, we’ve seen voice AI adopted in professional film, game development, education, and accessibility. But the consistent limitation wasn’t sound quality — it wasexpressiveness. More exaggerated emotions, conversational interruptions, and believable back-and-forth were difficult to achieve. Eleven v3 addresses this gap. It was built from the ground up to deliver voices that sigh, whisper, laugh, and react — producing speech that feels genuinely responsive and alive. What’s new in Eleven v3 (alpha) Hear v3 for yourself Using audio tags Audio tags live inline with your script and are formatted with lowercase square brackets. You can see more about audio tags in ourprompting guide for v3 in the docs. Professional Voice Clones (PVCs) are currently not fully optimized for Eleven v3, resulting in potentially lower clone quality compared to earlier models. During this research preview stage it would be best to find an Instant Voice Clone (IVC) or designed voice for your project if you need to use v3 features. PVC optimization for v3 is coming in the near future. For example, you could prompt: “[whispers] Something’s coming… [sighs] I can feel it.” Or for more expressive control, you can combine multiple tags: Crafting multi-speaker dialogue Eleven v3 is supported in our existing Text to Speech endpoint. Additionally, we introduce a newText to Dialogue API endpoint. Provide a structured array of JSON objects — each representing a speaker turn — and the model generates a cohesive, overlapping audio file: The endpoint automatically manages speaker transitions, emotional changes, and interruptions. Learn morehere. v3 is our most expressive model Pricing and availability To enable v3: Use theModel Pickerand selectEleven v3 (alpha) API access and support in Studio are coming soon. For early access, pleasecontact sales. When not to use v3 Eleven v3 (alpha) requires more prompt engineering than our previous models. When it works the output is breathtaking but the reliability and higher latency means it’s not suitable for real-time and conversational use cases. For these, we recommend Eleven v2.5 Turbo/Flash. For more, refer to the fullv3 documentationand FAQ. Try it today Log in toElevenLabs UI Select v3 (alpha)in the model dropdown Paste your script — use tags or dialogue Generate audio We’re excited to see how you bring v3 to life across new use cases — from immersive storytelling to cinematic production pipelines. How does the Eleven v3 80% discount work? How were the samples in the video and website generated? How does dialogue generation work? Is this available over API? What audio tags are supported? What languages does it support?]]></description>
      <pubDate>Wed, 21 Jan 2026 13:56:53 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/eleven-v3</guid>
    </item>
    <item>
      <title>Introducing Agent Workflows</title>
      <link>https://elevenlabs.io/blog/introducing-agent-workflows</link>
      <description><![CDATA[Our agents platform now offers Workflows - a visual editor for designing conversation flows. Instead of building all business logic in a single agent, Agent Workflows let you handle complex scenarios by routing to specialized Subagents and, when needed, transferring to human operators. You map the flow, declare decision points, and control handoffs to Subagents or humans - making behavior explicit, auditable, and testable. This moves beyond a single large prompt so you can decompose tasks, apply guardrails, and orchestrate the right action at the right time. Scope context and tools with Subagents Subagents each have their own system prompt, tools, and scoped knowledge base. You choose which Subagent handles a task, what data it can access, and when to hand off to another agent or a human. This reduces prompt bloat, limits access to sensitive systems, and improves response quality with narrower context. Enforce security and business policy Workflows connect agents to your internal systems with scoped credentials and access. You can embed business rules - validations, approvals, thresholds, and escalation paths - so conversations follow the same policies your teams use today. Optimize for cost, latency, and accuracy With Agent Workflows, you can pick the ideal LLM for each step. Use lightweight models for classification and routing, and higher-accuracy models for complex reasoning. Because prompts and knowledge bases are scoped per Subagent, you reduce token usage and latency while improving precision. An example workflow Intake and classify the request with a lightweight model. Route billing questions to a billing Subagent with scoped tools and data and route technical issues to a support Subagent that can run diagnostics. Escalate to a human when confidence drops below a threshold or when approval is required. Return to the parent agent to summarize the outcome for the user. Start building Agent Workflows put you in control of how conversations are designed, routed, and supervised. Build structured, secure, and scalable agents with ElevenLabs Agents. To learn moreread the documentationor get started with setting upan Agent Workflow today.]]></description>
      <pubDate>Wed, 21 Jan 2026 13:54:17 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-agent-workflows</guid>
    </item>
    <item>
      <title>Expanding access: patients and clinicians can now apply directly on the ElevenLabs website</title>
      <link>https://elevenlabs.io/blog/expanding-access-patients-and-clinicians-can-now-apply-directly-on-the-elevenlabs-website</link>
      <description><![CDATA[Patients with permanent speech loss - and their speech-language pathologists, occupational therapists, or AAC specialists - can now apply for free voice licenses directly on the ElevenLabs website through our Impact partners. How it works To begin, applicants need to create a free ElevenLabs account. After clicking on their profile photo, they will select"Apply for Impact Program,"then follow the on-screen steps to apply through the nonprofit organization that supports their region and diagnosis.Clinicianscan also apply for a free 1-year licenses by selecting "Clinicians/Staff" rather than a nonprofit organization when applying. This enables them to guide their patients through the process of creating personalized synthetic voices. There are no discount codes or credit card details required. Approved applicants receive a 5-year, extendable free license. How individuals with permanent speech loss can apply for a free voice How clinicians supporting individuals with permanent speech loss can apply for a free voice Who is eligible We partner with nonprofit organizations who help us distribute free access to ElevenLabs for individuals affected by permanent voice loss or visual impairment. Our current application partners support individuals across a range of diagnoses and regions, including: ALS/MND(USA –Bridging Voice; UK –MND Association; Australia –MND and Me,MND NSW - FlexEquip; Global –Scott-Morgan Foundation,UCL) PSP, MSA, CBD(USA/Canada –CurePSP; UK/Ireland –MSA Trust; Global –Mission MSA) Stroke(USA –Stroke Onward; New Zealand –TalkLink Trust) Tay-Sachs & SandhoffDisease (England, Wales, Northern Ireland –CATS Foundation) Head & Neck Cancer, Laryngectomy, Glossectomy(Global –Lary’s Speakeasy,TalkLink Trust) Permanent speech impairment and specific AAC users(Global –Smartbox,Jabbla,Therapy Box,Cboard,REHAVISTA,UCL) Blind and Low-Vision(USA –National Federation of the Blind) Building toward one million voices By enabling users to apply directly through the ElevenLabs website, we’re removing friction and making accessibility faster and simpler. Each improvement brings us closer to our goal of giving one million people their voices back. Note: This new workflow isonly for individuals with permanent speech loss and their clinicians, who can use it to support patients through onboarding andvoice cloning. Organizations seeking to help distribute free access to patients, especially to represent a new region or diagnosis not currently supported, should apply through our standardImpact Program Application. If your nonprofit organization would like to complete a project using ElevenLabs technology, please also applythere.]]></description>
      <pubDate>Wed, 21 Jan 2026 13:54:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/expanding-access-patients-and-clinicians-can-now-apply-directly-on-the-elevenlabs-website</guid>
    </item>
    <item>
      <title>Introducing Scribe v2 Realtime</title>
      <link>https://elevenlabs.io/blog/introducing-scribe-v2-realtime</link>
      <description><![CDATA[Scribe v2 Realtime: the most accurate model for live transcription Scribe v2 Realtime sets a new standard for low-latencySpeech to Text. Designed for live use cases—voice agents, meeting assistants, and real-time captioning—it transcribes speech in under 150 ms across English, French, German, Italian, Spanish, and Portuguese, and 90 languages. Scribe v2 Realtime is specifically built for agentic use cases. On 500 hard samples containing background noise and complex information, it significantly outperforms all other models. Key features Negative latency:Next word and punctuation prediction Automatic language detection:Speak in any language, switch language mid conversation Text conditioning: Scribe v2 Realtime continues the transcription based on the previous batch, useful when restarting a connection Voice Activity Detection(VAD) Manual commit: Full control over when to finalize transcript segments Multiple audio formats: Support for PCM (48kHz) and μ-law encoding Enterprise readywith SOC 2, ISO 27001, PCI DSS L1, HIPAA, and GDPR compliance, EU and India data residency options and Zero retention mode for sensitive workloads Scribe v2 Realtime delivers human-level understanding in real time, enabling natural conversation and immediate response in live environments. Scribe v2 Realtime achieves 93.5% accuracy across 30 commonly used European and Asian languages. Build with the API Scribe v2 Realtime is available today through the ElevenLabs API. Explore the documentation:https://elevenlabs.io/docs/cookbooks/speech-to-text/streaming Use Scribe v2 Realtime in ElevenLabs Agents Deploy natural, human-sounding agents powered by Scribe v2 Realtime. Build voice assistants for support, sales, or in-product experiences that can understand and respond in real time. Learn more:https://elevenlabs.io/agents Start building today Use Scribe v2 Realtime through our API or directly within ElevenLabs Agents. Sign up here:https://elevenlabs.io/app/sign-up]]></description>
      <pubDate>Wed, 21 Jan 2026 13:53:56 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-scribe-v2-realtime</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs UI: Open-source audio &amp; agent components for the web</title>
      <link>https://elevenlabs.io/blog/elevenlabs-ui</link>
      <description><![CDATA[ElevenLabs UIis a new open source library of customizable React components for building interfaces with theElevenLabs Agents& Audio SDKs. Built onshadcn/ui, it provides full control over UI primitives like waveforms, orbs, messages & more. Examples transcriber-01- An open-source voice dictation component you can drop into any web app: voice-chat-03- a rich multimodal chat interface with state management built in. Pass your Elevenlabs Agent ID as a prop and ship it: Getting started Components are available via the@elevenlabs/agents-clicommand. For example, to install theOrbcomponent, you can run: Read the docsand start building better agent & audio interfaces, faster.]]></description>
      <pubDate>Wed, 21 Jan 2026 13:53:54 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/elevenlabs-ui</guid>
    </item>
    <item>
      <title>Giving voice back to stroke survivors</title>
      <link>https://elevenlabs.io/blog/giving-voice-back-to-stroke-survivors</link>
      <description><![CDATA[Each year,more than 12 million people worldwide experience a stroke. For many, recovery involves relearning how to walk, think, and communicate. Some regain their speech fully. Others live with long-term motor-speech challenges that make verbal communication difficult or impossible. Today, on World Stroke Day, we’re proud to announce our partnership withStroke Onward. Founded by stroke survivors, Stroke Onward helps people navigate the emotional and identity challenges of rebuilding life after a stroke. Together, we aim to make voice restoration technology available to everyone who needs it, and to strengthen the community of those rebuilding life after stroke. Bringing voice restoration to the stroke survivor community Through this partnership, individuals affected by permanent speech loss can now apply for the ElevenLabs Impact Program. Approved applicants receive free access to our advancedvoice cloningandText to Speechtools, allowing them to create and use a digital voice that represents them authentically. For those living with dysarthria or other motor-speech impairments, this technology can help restore a vital part of their identity. It enables them to communicate naturally with loved ones, participate in conversations, and express themselves in their own voice. Stroke Onward Community Circle Stroke Onward has also launched theStroke Onward Community Circle (SOCC)—a free, online community where survivors, carepartners, and professionals connect around the emotional and identity sides of recovery. In a world that often focuses only on physical rehabilitation, SOCC creates space for everything else—the emotional, invisible, and deeply personal parts of rebuilding life after stroke. Members can join live events, share experiences, and access curated tools that support emotional recovery. By introducing ElevenLabs voice technology into this community, we hope to empower members to rediscover and reclaim their voices as a part of that recovery process. Our shared goal We believe everyone should have the ability to express themselves in their own voice. Partnering with Stroke Onward helps us reach people living with speech loss, ensuring that technology serves as an enabler, not a barrier, to human connection. On World Stroke Day, we’re reminded that recovery from stroke is about more than physical healing. It’s about rebuilding identity, connection, and purpose. Together with Stroke Onward, we aim to make that journey more accessible, helping survivors regain their voices and their sense of self. Learn more about the partnership and how to apply.]]></description>
      <pubDate>Wed, 21 Jan 2026 13:53:49 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/giving-voice-back-to-stroke-survivors</guid>
    </item>
    <item>
      <title>Honoring veterans and their voices: Lt Col Thomas Brittingham’s story</title>
      <link>https://elevenlabs.io/blog/honoring-veterans-and-their-voices-lt-col-thomas-brittinghams-story</link>
      <description><![CDATA[When Lt Col Thomas Brittingham first heard his own voice again, it was Mother’s Day and his wedding anniversary. His wife Jessi was sitting beside him when he typed out a short message using his new ElevenLabs voice: “Hey Jessi, does this sound like me? Happy Mother’s Day and Happy Anniversary. I love you.” The moment stopped her. “It brought tears to my eyes,” she recalled. “It was the most amazing gift I could have ever received—hearing his voice again.” Lt Col Thomas Brittingham A lifetime of service Lt Col Brittingham has dedicated his life to service and excellence. A 2006 graduate of the Coast Guard Academy and a 2004 Air Force Academy Exchange Cadet, he began his career aboard the Coast Guard Cutter Sequoia in Guam, where he conducted the first bilateral boardings of foreign ships in the Western Pacific. From there, his path led to commanding his own vessel, the Coast Guard Cutter Haddock, and later serving as Military Aide to the Coast Guard’s Chief Acquisition Officer, overseeing $30 billion in modernization programs. In 2011, he was selected for Air Force pilot training—a transition that would mark the next chapter in a distinguished career. Over the following decade, Lt Col Brittingham served as flight lead and mission commander across the Mediterranean, Atlantic, and Pacific. He deployed four times in support of Operation Inherent Resolve, logging nearly 1,000 combat hours. Confronting ALS In 2023, Thomas received a diagnosis that would change his life—amyotrophic lateral sclerosis (ALS). The disease began in his legs and moved upward, affecting his arms, diaphragm, and eventually his ability to speak. “His voice was always strong,” Jessi said. “Even as ALS progressed, it was something that made him feel like himself.” But as muscle weakness advanced, Thomas’s voice grew quieter until it disappeared entirely in April 2024. Without a natural voice, Thomas initially relied on a generic computer-generated one that was robotic and difficult to understand. “We depended mostly on reading his screen,” Jessi said. “It didn’t sound like him. It didn’t sound human.” Finding his voice again Through Team Gleason, a nonprofit that supports people living with ALS, Thomas connected withBridging Voice, an organization that helps individuals preserve and recreate their voices. There, he met Trinity, who guided him through the process of restoring his natural voice with ElevenLabs. Bridging Voice guided his family through the process—collecting past videos, preparing samples, and building a model of how Thomas sounded before ALS. “It was healing to go back through those clips,” Jessi said. “We watched them with our two boys, who loved hearing their dad’s voice again.” Once the recordings were submitted, the ElevenLabs team created a Professional Voice Clone for Thomas, a precise recreation of how he sounded before the disease. The moment it all came back When Thomas used his new AI voice for the first time, he chose to surprise Jessi. The words he typed carried all the warmth and cadence of his real speech. “I made him say it over and over again,” Jessi laughed. “Our family couldn’t believe how real it sounded. The boys thought it was hilarious hearing their dad’s voice saying silly things.” It wasn’t just a technological milestone, it was a return of identity, presence, and connection. “With two young children, it means everything for them to hear their dad’s voice,” Jessi said. “It keeps him present in their lives in a way that text alone can’t.” A message for Veterans Day For Lt Col Brittingham, Veterans Day carries deep meaning. It’s a reminder not only of service and sacrifice, but of the strength that comes from community and innovation. He hopes his story shows what’s possible when technology serves humanity, especially for veterans facing the challenges of illness or injury. Recent studies have shown that Air Force pilots are ten times more likely to be diagnosed with ALS than civilians. For Thomas, that statistic is personal. His experience underscores the urgency of advancing accessible technology that restores independence and dignity to those who have given so much in service. Continuing the mission TheElevenLabs Impact Programexists to help individuals like Lt Col Brittingham regain their voice and agency through AI. By combining advanced voice synthesis with human-centered design, the program ensures that every person can preserve the sound of who they are. Thomas’s journey, from commanding aircraft across the world to communicating again in his own voice, shows what this technology makes possible. When so much is taken away, being able to speak again, even through an artificial voice, gives something profoundly human back. This Veterans Day, we honor Lt Col Brittingham and all who have served, and recommit to building technology that gives them their voices back. It’s not just about speech. It’s about memory, identity, and the ability to stay connected to the people who matter most.]]></description>
      <pubDate>Wed, 21 Jan 2026 13:34:11 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/honoring-veterans-and-their-voices-lt-col-thomas-brittinghams-story</guid>
    </item>
    <item>
      <title>Introducing Scribe v2</title>
      <link>https://elevenlabs.io/blog/introducing-scribe-v2</link>
      <description><![CDATA[Scribe v2 is built for batch transcription, subtitling, and captioning at scale. It improves on the stability and accuracy of Scribe v1, with better handling of long-form audio, pauses, changes in tone, and extended silences. While Scribe v2 Realtime is optimized for ultra low latency and agents use cases, Scribe v2 is optimized for long and complex recordings, maintaining accuracy across diverse speakers, accents, and delivery styles. The result is consistently reliable transcripts across a wide range of real-world audio conditions. Scribe v2 achieves the lowest word error rate recorded on industry-standard benchmarks. Keyterm Prompting for context-aware transcription Keyterm prompting goes beyond standard Custom Vocabulary by using the transcript’s context. Select up to 100 words or phrases, and Scribe v2 will accurately decide when to transcribe those terms. This makes it well suited for technical domains, brand names, and industry-specific language. Built-in entity detection with precise timestamps Scribe v2 includes native entity detection for structured audio analysis.You can select up to 56 categories across Personally Identifiable Information, health data or payment details. Scribe v2 will automatically detect these instances and their exact timestamps in your transcript, making it easier to review, redact, or process sensitive information at scale. Learn more in the API documentation:https://elevenlabs.io/docs/developers/guides/cookbooks/speech-to-text/batch/entity-detection Automatic multi-language transcription Scribe v2 supports smart multi-language workflows out of the box. You can send audio that contains multiple languages in a single file. The model automatically detects each language and transcribes it correctly without manual segmentation or configuration. Additional features for production workflows Scribe v2 includes a set of features designed for enterprise and developer use cases: Smart speaker diarization for clear, intuitive speaker labeling Precise word-level timestamps for accurate subtitle alignment and interactive experiences Dynamic audio tagging that detects non-speech events such as laughter or footsteps Enterprise readiness with SOC 2, ISO 27001, PCI DSS L1, HIPAA, and GDPR compliance, EU and India data residency, and zero retention mode support Scribe v2, now in ElevenLabs Studio Scribe v2 is now used in ElevenLabs Studio for more accurate subtitles, captions and transcriptions, supporting teams that manage large libraries of audio and video across marketing, media, research, training, and compliance use cases. Try it now:https://elevenlabs.io/app/studio Build with the API With Scribe v2, developers and enterprises can automate complex audio pipelines, improve accuracy in global content workflows, and scale securely with full compliance and data residency controls. Scribe v2 is available today via our API and Creative platform. Try it now:https://elevenlabs.io/app/speech-to-text Read the docs:https://elevenlabs.io/docs/capabilities/speech-to-text Sign up here:https://elevenlabs.io/speech-to-text]]></description>
      <pubDate>Wed, 21 Jan 2026 13:33:03 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-scribe-v2</guid>
    </item>
    <item>
      <title>Introducing The Eleven Album</title>
      <link>https://elevenlabs.io/blog/introducing-the-eleven-album</link>
      <description><![CDATA[Today, ElevenLabs launchesThe Eleven Album, a landmark musical release created in collaboration with world-class artists and powered by Eleven Music, our model for generating fully original, studio-quality compositions. Spanning rap, pop, R&B, EDM, cinematic scoring, and global sounds, the album brings together GRAMMY-winning legends, chart-topping producers, and next-generation creators to explore what’s possible when artists and AI create together. The Eleven Album features original tracks from: Liza Minnelli(EGOT (EMMY GRAMMY OSCAR TONY Award recipient)) Art Garfunkel(8x Grammy Award Winner) Patrick Patrikios(Written/Produced for Britney Spears, Christina Aguilera, Backstreet Boys, and more) King Willonius(Songwriter and vocalist known for creating the viral BBL music tracks) IAMSU!(Renowned Bay Area rapper and producer with charting releases and collaborations including E-40 and Too $hort) Demitri Leiros(Film and TV composer, producer, and songwriter whose music has been featured across Apple TV+, Netflix, Amazon, Paramount, and HBO / Max.) Emily Falvey(Named “Nashville’s Top New Songwriter” by Forbes) Sunsetto(Songwriter, and producer best known for the singles “your big break” and “BIG MACHINE,” blending indie-pop and electronic textures) KondZilla(~40 billion total YouTube views across music channel) Chris Lyons(General Partner at a16z and producer blending hip-hop energy with meditative frequencies) Kai(AI music artist and producer best known for the tracks “That’s a Vibe” and “Better When It’s You,” Angelbaby(AI music artist with 400K+ monthly listeners on Spotify) Michael Feinstein(5x Grammy nominee) Collectively, these artists represent billions of streams, global tours, and decades of influence across the modern music era. A New Creative Workflow Artists used Eleven Music in different ways, from generating compositions to write over to experimenting with new genres or accelerating production. For some, the model served as a creative spark. For others, it became a way to explore unfamiliar sounds or reimagine their existing process. Each track on the album is fully original, blending the artist’s signature voice, style, and musical instincts with new creative possibilities unlocked by AI. Liza Minnelli Art Garfunkel KondZilla Beyond the Album: Expanding Artist Opportunity Several participating artists are on the ElevenLabs Iconic Marketplace, our curated, permission-based platform where iconic talent can license their voice, style, or musical identity for approved collaborations across media, entertainment, gaming, and brand partnerships. Building With the Industry, Not Around It This release builds on our broader commitment to responsible AI development in music, including our partnerships with Kobalt Music and Merlin, which enable artists and songwriters to actively participate in shaping AI music models and revenue streams. Together, these collaborations help define a new industry standard, one where innovation and rights coexist, and creators remain at the center of progress. About Eleven Music Eleven Music is a next-generation audio model that generates full compositions from simple prompts. Built in close partnership with music industry professionals, the model enables artists to: Create fully original, rights-cleared, commercially usable musicSpark new ideas and accelerate their creative workflows Edit tracks with granular control, adjusting lyrics, timing, and instrumentation Download up to six studio-quality stems for advanced mixing and arrangement The debut of The Eleven Album demonstrates how artists and AI can work side by side to expand what’s possible in modern music-making — opening new pathways for creativity, storytelling, and global collaboration. The album is out now. Experience the future of soundhere.]]></description>
      <pubDate>Wed, 21 Jan 2026 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-the-eleven-album</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs Agents</title>
      <link>https://elevenlabs.io/blog/introducing-elevenlabs-agents</link>
      <description><![CDATA[From Conversational AI ElevenLabs Agents We’re renaming Conversational AI to ElevenLabs Agents. It’s a complete platform where you can build, launch and monitor conversational agents that talk, type and take action across phone, web and apps. ElevenLabs Agents is a better name for this. Since launch, customers have created more than 2 million agents. Together, they’ve handled over 33 million conversations this year. What makes ElevenLabs Agents different Connected to your knowledge base, tools, and telephony, our multimodal agents can manage complex workflows while maintaining enterprise-grade reliability and control. Customers are already usingElevenLabs Agentsto: Resolve customer issues Qualify leads Run outbound calling at scale Support employee learning and development Power dynamic NPCs in games Act as personal assistants or tutors Our goal is simple: make conversations with technology as natural as speaking with a person. With our audio research and orchestration platform, startups, SMBs, and enterprises can deliver personalized one-to-one interactions at scale. What’s next for ElevenLabs Agents We’re expanding the platform to give developers more control and better performance. Coming soon: Visual workflow builderto handle complex business logic Testing suiteto run simulations to ensure agents act as expected Expressive modefor more realistic voice interactions Expanded integrationswith Google Calendar, Salesforce, Zendesk, and more Build your first ElevenLabs Agent We believeconversationalvoice agentswill become a core part of how people interact with technology. With ElevenLabs Agents, you can build, deploy, and monitor them in one place. Start building today:https://elevenlabs.io/app/agents]]></description>
      <pubDate>Fri, 16 Jan 2026 21:38:43 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-elevenlabs-agents</guid>
    </item>
    <item>
      <title>Deutsche Telekom and ElevenLabs announce partnership</title>
      <link>https://elevenlabs.io/blog/deutsche-telekom-and-elevenlabs-announce-partnership</link>
      <description><![CDATA[We’re proud to announce our partnership with Deutsche Telekom to bring ElevenLabs' AI voice agents to the customer service of Europe’s largest Telco (via app and phone). Deutsche Telekom customers will soon experience realistic AI voice agents - available 24/7 and without any waiting time - to augment customer support with a more personal, human-like interaction. This initiative is part of a shared vision to democratize access to AI, making advanced voice technology simple, accessible, and valuable to everyone.]]></description>
      <pubDate>Wed, 14 Jan 2026 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/deutsche-telekom-and-elevenlabs-announce-partnership</guid>
    </item>
    <item>
      <title>TVS Motor Company deploys multimodal AI agents using ElevenLabs</title>
      <link>https://elevenlabs.io/blog/tvs-motor-multimodal-agents</link>
      <description><![CDATA[TVS Motor Company, a global manufacturer and mobility solutions provider, is redefining digital customer engagement with multimodal AI agents that combine voice and text – built usingElevenLabs’ Agents Platform. Early results show a35% lift in lead capture and a 80% reduction in customer feedback turnaroundtimes across 25+ countries. The agents are live across TVS’ international websites in Africa, Latin America, Middle East, Asia, and ASEAN, powering both pre-sales and post-sales interactions. Each interaction is localized and real-time, improving conversion and accelerating feedback loops. Why TVS Motor chose ElevenLabs In Q1 of the current financial year, TVS evaluated multiple architectures for its agent platform, including speech-to-speech and pipeline-based systems. After benchmarking various solutions, they selected ElevenLabs for its: unmatched low latency natural voice quality multilingual support critical to their global footprint Integration was completed in under a week. ElevenLabs’ Agent APIs, combined with support for major cloud telephony providers such as Twilio and Plivo, allowed TVS to deploy, test, and refine agents rapidly without compromising on voice quality or responsiveness. Anand Das, Chief Digital & AI Officer, International Business, TVS Motor Company. From static CTAs to high-converting AI agents TVS replaced static web forms with Shop AI agents that now handle product discovery and lead capture autonomously through a multimodal widget that combines voice and text on their websites. Post-sales interactions, such as NPS and feedback collection, are handled via outbound voice calls initiated by Voice AI agents. Early data shows a 35% lead capture rate – compared to traditional CTAs. TVS Motor Website Agent: Real-time customer feedback, localized voices For post-sales, TVS deployed Voice AI agents to conduct NPS calls and collect customer feedback. By localizing voice personas across Latin America, ASEAN, Middle East, and Africa, and fine-tuning tone per use case – empathetic for support, energetic for sales – TVS ensured that every interaction felt natural and culturally aligned. As a result, TVS reduced the cost per feedback by more than 35% and cut turnaround times by 80%. Built for global markets TVS agents now support nine languages: English, Hindi, Tamil, Bahasa, Spanish, Arabic, French, Turkish, and Italian. All product discovery and lead capture flows are fully automated. Each voice persona reflects regional accents and speech patterns, enhancing user trust and relatability. Naga Budigam, Enterprise AI Innovation & Acceleration Lead at TVS Motor Company. Scaling with ElevenLabs’ platform TVS primarily leverages the Agents API, integrating conversational AI directly into their websites. This enables seamless multimodal interactions, local voice rendering, and continuous optimization across markets. With no agents previously deployed, this partnership represents a step-function improvement – scaling from zero to 25+ countries with fully localized automation in weeks. TVS Motor’s deployment shows how fast global teams can scale multimodal AI with natural-sounding, low-latency voice. If you are building real-time customer engagement or automated service workflows across markets, get in touch.]]></description>
      <pubDate>Tue, 06 Jan 2026 16:23:39 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/tvs-motor-multimodal-agents</guid>
    </item>
    <item>
      <title>Introducing ElevenLabs Image &amp; Video</title>
      <link>https://elevenlabs.io/blog/introducing-elevenlabs-image-and-video</link>
      <description><![CDATA[Today we’re introducing ElevenLabs Image & Video (Beta). The best audio, image, and video models all in one platform. Within ElevenLabs, you can now bring ideas to life in one complete creative workflow. Use leading models like Veo, Sora, Kling, Wan and Seedance to create high-quality visuals, then bring them to life with the best voices,music, andsound effectsfrom ElevenLabs. Creators, marketers, and content teams can generate images, compose clips, adjust narration, and export final content all inside a single, unified workflow. Create with leading image and video models ElevenLabs Image & Video (Beta) brings together the best models for visual creation. You can: Createstill images using leading models including Nanobanana, Flux Kontext, GPT Image, and Seedream. Usethese as storyboards, thumbnails, or as source material for video projects. Generatevideos with models including Veo, Sora, Kling, Wan, and Seedance. Refineoutputs and compose multiple clips for seamless storytelling. Upscaleyour images and videos for higher-quality results. Add lipsyncto your generated videos using ElevenLabs voices for perfectly aligned narration. Refine and edit in Studio Once your visuals are ready, export to Studio to complete your project. Studio lets you: Add expressivevoiceoversusing voices from our library or your own clones. Compose bespoke background music and layer in sound effects. Adjust timing and refine narration on a single timeline. Export polished, production-ready videos. Built for creators, marketers, and content teams Image & Video is designed for creators of every kind, from filmmakers and freelancers to marketers and educators. Whether you’re creating product videos, social content, or educational materials, ElevenLabs provides the full toolset to go from idea to final export in one platform. This launch marks a major step toward true multimodal creation, where every element — from visuals to sound — can be generated, edited, and refined together. Available in ElevenLabs Creative Platform Start creating with ElevenLabsImage & Video(Beta).]]></description>
      <pubDate>Tue, 06 Jan 2026 16:13:30 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/introducing-elevenlabs-image-and-video</guid>
    </item>
    <item>
      <title>ElevenLabs Welcomes Matthew McConaughey as New Investor</title>
      <link>https://elevenlabs.io/blog/elevenlabs-welcomes-matthew-mcconaughey-as-new-investor</link>
      <description><![CDATA[At ElevenLabs’ inaugural Summit, the company revealed thatAcademy Award-winning actor Matthew McConaughey has been a part of ElevenLabs story for years– as an investor, early supporter, and now, as a creator. In a video played at the Summit, Matthew McConaughey shared that his newsletter,Lyrics of Livin’, is expanding with a Spanish language version – powered by ElevenLabs. Now, Matthew’s stories and content will be made available in Spanish audio, using his unmistakable voice, and reaching an even wider audience. Subscribe here:https://lyricsoflivin.com/hola “I’m proud to share that I’ve been an investor in ElevenLabs for several years now,” said Matthew McConaughey. “It’s been amazing to see the growth from those early days to where the company, and the technology, is now. What’s remained constant is theextraordinary storytelling capabilities and creative potentialthat ElevenLabs unlocks – something that stood out to me from the start and that speaks to me as a professional storyteller.” “When I first met Matthew, I was struck by how genuinely he connected with our vision and what we’re trying to do at ElevenLabs,” addedMati Staniszewski, CEO and Co-Founder of ElevenLabs. “He wasn’t just excited by the tech – he understood what we’re aiming to achieve creatively. We’re so grateful for his continued insight and support, especially as we expand our work in the creative space.” [To see more highlights from the ElevenLabs Summit, visit:https://summit.elevenlabs.io/]]]></description>
      <pubDate>Tue, 06 Jan 2026 15:43:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/elevenlabs-welcomes-matthew-mcconaughey-as-new-investor</guid>
    </item>
    <item>
      <title>Freedom Forever scales support nationwide with ElevenLabs Agents</title>
      <link>https://elevenlabs.io/blog/freedom-forever</link>
      <description><![CDATA[Freedom Forever is one of the largest residential solar installation companies in the United States, with more than 2,500 employees and a national network of 10,000 to 20,000 independent sales representatives. The organization supports hundreds of thousands of customers and contractors through high-volume inbound support lines. Freedom Forever deployed ElevenLabs Agents, delivering real-time answers and improving support efficiency up to 90%. Modernizing support for a rapidly expanding solar leader As Freedom Forever continued to scale, the company explored ways to enhance consistency, increase capacity, and provide immediate assistance across both customer and sales support. Seasonal patterns create natural fluctuations in call demand, and the team sought a technology partner capable of delivering real-time responsiveness while enhancing the overall experience. Selecting a unified platform built for real-time voice Freedom Forever evaluated several voice and conversational AI architectures, including multimodal pipelines and real-time APIs. Many of these introduced latencies or required complex routing logic. ElevenLabs provided a unified alternative that offered: • Natural, low-latency support interactions • Integrated routing, testing, and evaluation tools • Bilingual support in English and Spanish, including Puerto Rico dialects • Smooth integration with Twilio and existing CRM systems • Scalable performance for high-volume environments This comprehensive architecture aligned with the company’s operational standards, creating a consistent, immediately available support experience. From prototype to production in one week Because Freedom Forever had a strong internal infrastructure in place, the team was able to prototype quickly and move to production in one week. The primary decision was simply determining which workflows to automate first.Today, ElevenLabs Agents support both major inbound functions: • Sales Support, where inbound inquiries are gathered, qualified, and routed efficiently • Customer Support, where routine questions, updates, and account-level interactions are resolved instantly A routing agent identifies the caller type and directs them to the specialized agent best suited for the request. When a human representative is needed, calls transfer with full transcript context for a seamless handoff. Stronger coverage and capacity through AI automation Since implementing ElevenLabs Agents, Freedom Forever has achieved measurable improvements in scale, efficiency, and availability across its support operations. Meaningful improvement in cost efficiency Internal analysis demonstrates a 90% higher efficiency of support interactions following the deployment of ElevenLabs Agents. Instant response at every entry point The AI agent greets every caller immediately, which increases the number of customers and sales partners who receive instant assistance and eliminates the need for callers to wait before being helped. A significant expansion in automated resolutions Enhanced routing capabilities allow the AI to resolve tens of thousands of calls each month. Optimized staffing and higher-value work With AI managing routine inquiries, support team members have been reassigned to roles that require human expertise. This shift strengthens both customer experience and team productivity. Consistent performance during seasonal peaks AI agents maintain the same level of speed and accuracy regardless of call volume. This provides predictable, reliable support even during busy months when demand increases. Tailored improvements across customer support and sales support On the sales side, ElevenLabs Agents help gather required information, prepare context for representatives, and streamline the routing process. Resolution rates and efficiency gains are particularly strong in this area. On the customer side, the AI now resolves a significant number of inquiries end-to-end, provides instant updates, and supports multiple languages. When customers prefer to speak with a representative, they can request a transfer at any time. Full context is handed off automatically through the CRM. Freedom Forever continues to expand the number of interactions that can be fully resolved through AI, reducing unnecessary transfers and increasing the overall speed of support. - Rob Richardson, VP of Product, Freedom Forever Future enhancements and strategic growth Freedom Forever is expanding its automation strategy with several initiatives in development, including: • Outbound agents to support its national sales network • Additional AI-driven resolution capabilities for customer support • Enhanced routing and callback optimization • Deeper integration across scheduling, CRM systems, and field operations These advancements reinforce Freedom Forever’s commitment to fast, reliable, and scalable service as the company continues to grow nationwide.]]></description>
      <pubDate>Thu, 18 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/freedom-forever</guid>
    </item>
    <item>
      <title>Eleven Music is Here</title>
      <link>https://elevenlabs.io/blog/eleven-music-is-here</link>
      <description><![CDATA[Today, we launchedEleven Music– the next step on our mission to build the most comprehensive AI audio platform in the world. With Eleven Music, businesses, creators, artists, and every single one of our users can generate studio-grade music from natural language prompts, with: - Complete control over genre, style, and structure - Vocals or just instrumental - Multi-lingual, including English, Spanish, German, Japanese and more - Edit the sound and lyrics of individual sections or the whole song A few of our favorite samples Check out a few of our favorite songs generated by the ElevenLabs team thus far: Echoes of Midnight Prompt: “Dreamy, psychedelic, slow Indie Rock, reverb-soaked vocals, retro keys, catchy chorus, analog, phased guitars, liminal, nostalgic feeling, anthem.” Saddles and Shadows Prompt: “An epic track for a cowboy show, wild west, cinematic sound design, guitar twanging with awesome orchestral elements crescendoing to a powerful finale, soundtrack.” Don’t Let Me Go Prompt: “A very retro track from the 1950s with an old crooner male vocalist, charming, vintage, classic, nostalgic, golden oldies, vinyl crackle, catchy vocal hooks.” Obsidian Prompt: “Extremely dark, tense and powerful, cinematic sound design, electronic hybrid, trailer music, evil, braam, braam horns, impacts, boom, rising tension, completely instrumental.” Wanderer of the Moor Prompt: “A young english girl singing an old english folk song, stunning, lonely, thoughtful and almost haunting, fiddle and english folk instrumentation, reverb, short song.” Yellow Bus Jam Jam band song about driving through new york city in a big yellow school bus with 2 long guitar solos and lots of harmonizing We can’t wait to see what you create. Commercial use Created in collaboration with labels, publishers, and artists, Eleven Music is cleared for nearly all commercial uses, from film and television to podcasts and social media videos, and from advertisements to gaming. For more information on supported usage across our different plans,head here. Eleven Music is available today on our website and public API access (please seeeleven music API documentation), with public API access and integration into ourConversational AIplatform coming soon. Check out ourprompt engineering guideto help you master the full range of the model’s capabilities.]]></description>
      <pubDate>Wed, 17 Dec 2025 13:19:07 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/eleven-music-is-here</guid>
    </item>
    <item>
      <title>Toyota engages fans with AI-powered Brock Purdy experience</title>
      <link>https://elevenlabs.io/blog/toyota</link>
      <description><![CDATA[Toyota’s Northern California Dealers Association andcreative agency H/Llaunched a new kind of branded experience - a dynamic voice-driven activation hosted by an AI-powered version of 49ers quarterback Brock Purdy. Built on ElevenLabs’Agents platform, the experience offers fans a natural, interactive conversation with Brock. It lives on a dedicated landing page -toyotaletsgo.com/brock-purdy- where fans play a voice-driven trivia game, guided by Brock’s digital persona as a method to win a VIP 49ers experience. Conversational fan experience built with ElevenLabs Fans enter a voice-controlled game show hosted by Brock. Behind the scenes, ElevenLabs AIvoice agentspower the full interaction - from trivia questions to contextual responses - while real-time webhooks sync voice outputs with filmed animations of Brock reacting live. Key product features: Dynamic voice interactions:Brock asks questions, adapts its tone, and responds in real time Custom brand personality:The agent was tuned to reflect Brock’s voice and personality, consistent with Toyota's brand values Live-action visual sync:Responses trigger synchronized on-page animations via webhooks Sweepstakes integration:A completed game triggers a contest entry flow, capturing leads directly from the experience Brand safety protocols:A custom knowledge base maintained compliance with Toyota and NFL brand guidelines The result is a high-engagement campaign that brings Brock’s presence into fans’ homes - not as a video ad, but as a voice-first, two-way interaction. Early results The campaign outperformed traditional marketing channels: 12k+ voice interactionsin the first few weeks ~2 minutes average engagement time- compared to 15-30 seconds for TV or 3-6 seconds for social media More than25% of conversations resulted in meaningful actions, like submitting a lead form or exploring products A new format for brand storytelling This is the first time Toyota has implemented an AI-driven voice experience in its marketing stack. The activation marks a shift from passive media consumption to active, two-way interaction - where users engage with branded content that talks back. By combining ElevenLabsAgents Platformwith custom webhook integrations and real-time animation sync,H/Land Toyota built a fully branded voice-first microsite that deepens fan affinity while driving qualified leads. Built for creative teams and marketing agencies Creative agencies and marketing teams can use ElevenLabs Creative Platform to design and generate content in any format - audio, music, image and video - and use our Agents Platform for immersive and engaging brand experiences. Voice agents can be customized for personality, tone, behavior, and safety - with native webhooks to power dynamic media and animations.]]></description>
      <pubDate>Thu, 04 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://elevenlabs.io/blog/toyota</guid>
    </item>
  </channel>
</rss>
