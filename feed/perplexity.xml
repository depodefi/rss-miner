<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Perplexity AI Hub</title>
    <link>https://www.perplexity.ai/hub</link>
    <description>Latest news and updates from Perplexity AI</description>
    <language>en-US</language>
    <lastBuildDate>Wed, 28 Jan 2026 09:47:06 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Bringing Perplexity to education and not-for-profits </title>
      <link>https://www.perplexity.ai/hub/blog/bringing-perplexity-to-education-and-not-for-profits</link>
      <description>&lt;![CDATA[AI will be an integral part of our lives, influencing how we learn, consume information, and work. For AI to make the most positive impact, it needs to be accessible to all organizations, not just for-profit companies. Earlier this year, we introduced Perplexity Enterprise Pro to enable companies to leverage our AI with enhanced features like user management, increased data privacy, SOC2 compliance, and fortified security. Now, we’re excited to extend these benefits to philanthropic and public organizations by offering Perplexity Enterprise Pro at a reduced cost. For all schools, universities, nonprofits, government agencies, and other not-for-profit organizations, Enterprise Pro will be available for $30/seat per month or $300/seat annually. Perplexity for education Increasingly, we’re hearing from educators that they want to teach their students to use AI effectively and responsibly. As AI becomes more prevalent, people of all ages will need to understand how to utilize it with strong critical thinking and media literacy skills. As a knowledge-based platform, we want to make Perplexity Enterprise Pro available to students ages 13 and up while ensuring it is conducive to learning. Our commitment to citing sources and prioritizing accuracy can teach students valuable skills like researching and vetting information. By making Perplexity more affordable for schools, we hope to expand access to our technology and equip students with the knowledge they need to solve the problems of tomorrow. We’ve begun rolling out Perplexity Enterprise Pro to several schools, including ‘Iolani School in Hawaii. Over the next few months of summer school, teachers will learn how tools like Perplexity can support both their growth and their students’ development. “‘Iolani is thrilled to be the first K-12 school to partner with Perplexity, providing our students and faculty with safe access to an advanced AI research and knowledge tool. This partnership is an exciting opportunity to develop case studies that demonstrate how AI can enhance learning, research, creativity, and productivity within an academic setting. We believe that integrating Perplexity into our educational framework will empower our community to explore innovative approaches to research and foster a deeper understanding of AI’s potential in various fields.” - Dr. Michael Lomuscio, Dean of Studies at ‘Iolani School Perplexity isn’t just a tool for students—it’s also a valuable resource for teachers and administrators. It can help find resources, develop lesson plans, and quickly answer any question. If you’re an educator looking to learn how to use Perplexity, check out our focused seven-day course, Perplexity for Educators . Perplexity for not-for-profits and public servants Employees at nonprofits and government agencies are already turning to Perplexity for quick answers and real-time updates on trending news, but not all organizations can afford Enterprise Pro subscriptions. That’s why we’re reducing the cost of Enterprise Pro for not-for-profits and government agencies — so more teams can benefit from our research and data analysis capabilities, freeing up time and resources and allowing staff to focus on higher-value activities. Here are some ways existing Enterprise Pro customers, like the U.S. Anti-Doping Agency and the Montana Department of Natural Resources and Conservation, benefit from Perplexity: Conduct research on real-time policy updates and current events with no knowledge cutoff Find relevant statistics for marketing materials and grant proposals Generate translations, transcriptions, and summaries quickly Create internal documents, including first drafts of new policies, job vacancy postings, and meeting agendas Build initial project plans and work breakdowns Analyze large datasets to gain valuable insights into operations, programs, and donor behaviors Verify server and coding issue fixes quickly Simplify the language of complex documents &quot;I use Perplexity daily to speed up my workflows around research and analysis, as well as getting past the initial blank page when drafting documents. Just providing a good starting point is often enough to save hours of time weekly. Having the sources of answers cited in-text is a huge benefit to verify the generated answer, and access to multiple AI models is icing on the cake.&quot; - Chris Powell, Chief Information Officer at the Montana Department of Natural Resources and Conservation We’re so appreciative of the work nonprofits and government agencies do to address critical societal issues and are excited to see the value they gain from using Perplexity. Visit perplexity.ai/enterprise or reach out to enterprise@perplexity.ai to learn more about how to get started with Perplexity as a school, university, not-for-profit organization, or government agency.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:06 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/bringing-perplexity-to-education-and-not-for-profits</guid>
    </item>
    <item>
      <title>Just a moment...</title>
      <link>https://www.perplexity.ai/hub/blog/bringing-european-ai-models-to-global-audiences</link>
      <description>&lt;![CDATA[]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:06 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/bringing-european-ai-models-to-global-audiences</guid>
    </item>
    <item>
      <title>Book a Table with Perplexity and OpenTable</title>
      <link>https://www.perplexity.ai/hub/blog/book-a-table-with-perplexity-and-opentable</link>
      <description>&lt;![CDATA[Searching for the perfect restaurant is a sport. You're matching a mood, a vibe, a dietary quirk, and wrangling friends, family, or a partner who “doesn’t care” but also has strong opinions. The endless browsing? Overwhelming. The pressure? Real. But it doesn’t have to be. Meet the new way to find and book your next great meal—right inside Perplexity, powered by OpenTable . THE TABLE IS THE ANSWER Perplexity has always been about answers—not just flooding you with links. Looking for an Italian spot in San Francisco with a romantic ambiance and great cacio e pepe? Just ask. Perplexity understands exactly what you mean, then serves up choices among OpenTable’s 60k global restaurant partners that meet your criteria. No more losing your place, no more decision fatigue—inspiration to action in seconds. &quot;As more diners rely on AI to uncover new dining experiences, this integration connects Perplexity’s millions of users directly to our global restaurant network,&quot; said Sagar Mehta, CTO of OpenTable. &quot;Integrating with trusted AI platforms is yet another way we’re making it effortless to find and secure the right table.&quot; FROM “WHERE SHOULD WE GO?” TO GOING Perplexity removes the headache and clarifies the action. Here's how it feels in real life: A group of friends blows into town at the last minute—it’s late, everyone’s hungry, half the crew is vegetarian, and someone wants “a view.” Just ask. You want to impress on date night: sushi that’s actually good, quiet enough to talk, allergen-safe for your shellfish-averse date. Ask Perplexity. Whatever you’re looking for, just type or speak your wishlist into Perplexity. Instantly, you’ll see restaurants on OpenTable that actually fit—and you can grab a reservation right there. No more shuffling between apps or reading reviews until you go cross-eyed. ACCURATE, PERSONAL, EFFORTLESS Perplexity isn’t just a shortcut. It’s your second brain, trained to care as much as you do about the details (and quirks) that make a meal memorable. It aggregates opinions from all over the web, but tailors the results to what you actually want—not just what’s generically “top-rated.” And yes, you can get as specific as you want—bring on the “fanciest decor in town” and “happy hour with giant margaritas” requests. BOOK WITH A TAP When you see the “Reserve” button, you’re set. It’s as direct as it sounds—your table is booked via OpenTable, confirmation and all. The future of dining out is fewer clicks, no more call-and-wait, and finally making good on your group chat’s dinner plans. Ready to make finding food way less stressful—and way more you? Ask Perplexity, book it, and get on with the fun part. Your next memorable meal is already waiting.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:06 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/book-a-table-with-perplexity-and-opentable</guid>
    </item>
    <item>
      <title>Arc x Perplexity</title>
      <link>https://www.perplexity.ai/hub/blog/arc-x-perplexity</link>
      <description>&lt;![CDATA[Perplexity now available in Arc Browser We are excited to announce that Perplexity is now integrated into Arc Browser as a default search engine option, allowing users to access the power of AI-powered search. With this integration, Arc users can now enjoy our answer engine, getting precise answers in real time without clutter or information overload. Perplexity leverages models like GPT-4, Claude 2.1, and Gemini Pro to provide a conversational search experience that evolves alongside AI innovations. By combining Arc's minimalist interface and Perplexity's intelligent search, we have created a streamlined browsing experience. No more sifting through irrelevant results or dead-end links - just fast, straightforward access to the information you need. Perplexity and Arc Browser both aim to enhance how people interact with the internet. We couldn't ask for a better partner than Arc to help achieve our goal of accessible, human-centered AI search. In Arc founder Josh Miller's words: &quot;AI Search is the next frontier &amp; it will be distributed via the Browser too. It’s our chance to start anew. Let's do it!&quot; We will keep refining our search technology to deliver the most intuitive, seamless user experience possible. Welcome to the new internet .]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:06 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/arc-x-perplexity</guid>
    </item>
    <item>
      <title>Answers for Every Investor</title>
      <link>https://www.perplexity.ai/hub/blog/answers-for-every-investor</link>
      <description>&lt;![CDATA[All investors share one thing: curiosity. From retirement savers building long-term wealth to day traders making hundreds of decisions a day. Small business owners researching competitors. Students learning about markets. Financial advisors serving clients. Bankers researching comps, and lawyers reviewing deal documents. All of them need questions. But not every investor can get clear, fast answers from the most important financial data there is: SEC Filings. Today we are bringing answers to every investor. Financial Intelligence For Everyone Starting today, Perplexity is providing answers leveraging SEC data for all investors. Our new SEC/EDGAR integration provides direct access to comprehensive financial data for all investors, delivered through our answer engine, making complex information instantly understandable. These documents contain the deeper story behind public companies—their actual financials, their strategic plans, their material risks. Yet for too many investors, this critical information remains buried in dense, technical documents that are difficult to navigate and interpret. Every answer comes with direct citations to the source documents, so you can verify the information and dive deeper when needed. Beyond Traditional Financial Platforms Traditional financial data platforms often require expensive subscriptions, complex interfaces, or specialized knowledge to navigate effectively. They're built for professional analysts and institutional investors, leaving individual investors to piece together information from fragmented sources or rely on simplified summaries that miss crucial details. We believe that everyone deserves access to the same financial information that drives professional investment decisions. Our SEC integration works seamlessly with Perplexity Search, Research, and newly launched Labs , so you can combine financial data with market analysis, news coverage, and industry research in a single conversation. Ask about a company's recent earnings, then immediately explore how those results compare to industry peers or what analysts are saying about the sector's outlook. Perplexity Enterprise Pro customers also can search through SEC filings alongside Factset's M&amp;A and transcript data, Crunchbase's firmographic data, and company files. Ready to explore? Start asking your financial questions on Perplexity today.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:06 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/answers-for-every-investor</guid>
    </item>
    <item>
      <title>Perplexity raises Series A funding round </title>
      <link>https://www.perplexity.ai/hub/blog/announcing-our-series-a-funding-round-and-mobile-app-launch</link>
      <description>&lt;![CDATA[At Perplexity.ai , we strive to bring you the best possible knowledge discovery experience. Founded in August 2022, our journey began with the release of Ask, our answer engine, in December 2022. Since then, we've experienced rapid growth, reaching 2 million monthly active users in just four months. As our user base grows, so does our commitment to innovation. We are delighted to announce that we have recently raised a $25.6 million series A funding round led by Peter Sonsini of New Enterprise Associates (Board member, Databricks) with participation from our seed round investors Elad Gil (Founder, Color Health), Nat Friedman (Former CEO of GitHub) and Bob Muglia (Former President of Microsoft), as well as new investors Susan Wojcicki (Former CEO of Youtube), Paul Buchheit (Creator of Gmail), Soleio (Designer of Messenger, Dropbox), and Databricks Ventures . We are also grateful to our angel investors who participated in our $3.1 million seed round in September 2022, led by Elad Gil and Nat Friedman , with participation from Pieter Abbeel (UC Berkeley, AIX Ventures), Yann LeCun (Chief Scientist, Meta), Andrej Karpathy (Founding Member, OpenAI), Ashish Vaswani (Lead Inventor of Transformer), Amjad Masad (CEO, Replit), Clem Delangue (CEO, HuggingFace) and others. Our long-term mission is to become the best platform for answers and information, serving as the go-to source for people seeking quick, accurate answers tailored to their asks. We envision Perplexity AI as a platform beyond a traditional search engine, evolving into a comprehensive knowledge hub where anyone can explore and learn effortlessly. In pursuit of this vision, we are committed to providing citations with every answer, providing proper attribution for sources of information and allowing for verification. As part of our mission, we are excited to expand our platform from web to mobile, with almost half of our users already accessing us via mobile web. To improve our mobile experience, today we are launching the Perplexity AI iPhone app. With instant answers, cited sources, voice search, follow-up questions, and thread history, our app delivers a comprehensive interface for information. As a team, we are honored to have the support of our users and investors as we pursue our mission of redefining the way people search for and access information. Thank you for joining us on this journey, and we look forward to continuing to push the limits of language models and search. The frontier of interactive possibilities is just beginning to be explored. Download our iPhone app today at perplexity.ai/download-iphone . Our team is small but growing: see perplexity.ai/careers for the latest job opportunities at Perplexity. Follow our Twitter at twitter.com/perplexity_ai for new releases. Join our Discord community at discord.gg/perplexity-ai .]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:05 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/announcing-our-series-a-funding-round-and-mobile-app-launch</guid>
    </item>
    <item>
      <title>Announcing our Partnership with the United States Government</title>
      <link>https://www.perplexity.ai/hub/blog/announcing-our-partnership-with-the-united-states-government</link>
      <description>&lt;![CDATA[Today, we are announcing a first-of-its-kind partnership with the United States government to bring Perplexity to all federal agencies. America’s public servants deserve the most powerful, accurate AI to help them serve our nation. To meet this need, we’re collaborating with the General Services Administration (GSA) to provide our flagship enterprise AI platform to federal employees and servicemembers. Through our partnership, federal agencies can acquire Enterprise Pro for Government via GSA’s Multiple Award Schedule (MAS IT) at essentially no cost for up to 18 months. We’ve also negotiated long-term pricing commitments that ensure Perplexity remains a sustainable AI solution for agencies long into the future. This milestone establishes Perplexity as the first major AI company to enter a direct government-wide contract. Consistent with President Trump’s AI Action Plan and GSA’s OneGov Strategy , Perplexity is proud to help streamline the way agencies deliver frontier AI capabilities to those who serve. Our products already serve tens of thousands of users across the federal workforce, who rely on Perplexity for everything from accelerating daily tasks to informing decisions of national importance. Now, agencies can bring an enterprise-ready edition of Perplexity to all their users, directly from the source itself. As AI models and product capabilities continue to evolve, we’ve heard government users consistently call out for one key element: choice. Agency administrators are wary of trapping their agencies on a single model family or a static universe of information—and rightfully so, given the risks to mission readiness and compliance. Enterprise Pro for Government offers a comprehensive suite of frontier AI models including Perplexity Sonar, GPT, Claude, and Gemini. These models are empowered with a rich, reliable, and configurable ecosystem of information sources, including Perplexity’s internet-scale knowledge index and each agency’s own systems. Perplexity’s model-agnostic approach means agencies can choose the best model for the specific task at hand, integrate relevant information, and ensure uninterrupted access to frontier AI even when individual model providers become unavailable or incompatible with agency requirements. Security and compliance are key pillars of government AI adoption. In September 2025, we led the way by making Perplexity the first (and currently only) publicly available AI platform to provide secure-by-default guarantees to all federal users, including users with no subscription or contract. Perplexity’s leadership in security continues with Enterprise Pro for Government, which stands today as one of only two services currently designated by the government for AI Prioritization . Awarded based on objective criteria established by GSA, our AI Prioritization designation demonstrates the strength of our security posture and expedites Perplexity’s FedRAMP authorization process through the cloud-native 20x pathway , enabling agencies to adopt Perplexity with full security and compliance. Perplexity is proud to contribute to America’s success. It’s this success that made our own journey possible, and we’re excited to put Perplexity in the hands of those whose work will propel our nation forward. Please visit our US Government hub to learn more about Perplexity’s federal offerings. Questions about procurement via the Multiple Award Schedule can be directed to the National Customer Service Center at ITCSC@gsa.gov ⁠, or to usgov@perplexity.ai .]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:05 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/announcing-our-partnership-with-the-united-states-government</guid>
    </item>
    <item>
      <title>Announcing Our Global Partnership with Motorola</title>
      <link>https://www.perplexity.ai/hub/blog/announcing-our-global-partnership-with-motorola</link>
      <description>&lt;![CDATA[We're excited to announce our global partnership with Motorola. Starting with the new generation of Motorola devices, Perplexity will be pre-installed on millions of smartphones worldwide, giving Motorola users direct access to our answer engine and assistant. Integration Across Motorola Devices We've worked closely with Motorola to ensure Perplexity is optimized for their latest devices, including the innovative Razr series. The Perplexity app will come pre-installed on all new Motorola devices, making our search capabilities immediately available. But it’s not just about saving you a trip to the Play Store. We've created custom optimizations for Motorola's hardware and software, including: Functionality on Razr devices' external display when folded shut Perplexity Assistant that extends beyond just information retrieval, allowing users to send emails, set smart reminders, play media, request rides, and book restaurant reservations Direct access through Moto AI by typing &quot;Ask Perplexity&quot; Content exploration leveraging Perplexity’s Related Questions from the screen with &quot;Next Move&quot; This is one of our first and most comprehensive integrations with a mobile phone brand, designed to provide a seamless search and assistant experience directly within Motorola's ecosystem. Perplexity Pro for Motorola Users All users of new Motorola phones—including the Razr and Edge 60 devices—will receive 3 months of Perplexity Pro at no cost. This provides access to: Deep Research for comprehensive analysis on complex topics Choice of advanced AI models including Sonar, Claude 3.5 Sonnet, and GPT-4o Pro Shopping features Unlimited file uploads and Pro/Reasoning searches This collaboration with Motorola significantly expands Perplexity's reach. By integrating with one of the world's leading smartphone manufacturers, we're bringing our search and assistant capabilities to millions of new users. It also fundamentally changes the relationship between you and your device. Your phone is now a personal assistant, answer machine and research analyst all in one, available to you on demand, 24/7. The seamless integration of information synthesis and actions is the future of mobile technology and we are happy to get it into the hands of millions of users through our Motorola partnership.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:05 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/announcing-our-global-partnership-with-motorola</guid>
    </item>
    <item>
      <title>Announcing Comet Plus Launch Partners</title>
      <link>https://www.perplexity.ai/hub/blog/announcing-comet-plus-launch-partners</link>
      <description>&lt;![CDATA[Today, we’re proud to unveil the initial launch partners for Comet Plus. Comet Plus is the new business model designed by Perplexity to deliver premium journalism to a more curious, empowered internet and ensure publishers benefit from the new demands of the AI age. What is Comet Plus? Comet Plus is our answer to the critical question: What does a better internet look like in the age of AI? Comet Plus is a $5 standalone subscription (included at no additional cost with Perplexity Pro and Max) that gives users and their AI assistants direct access to high-quality journalism and answers supported by trusted and accurate reporting. Participating publishers are compensated according to each of the three ways their participating content is valuable in the age of AI. (Read all the details about Comet Plus and the new model we announced for publisher compensation.) Publishers are at the center of our vision for a better internet. The web was created for questions and curiosity, but somewhere along the way, users were funneled into clickbait, misinformation, and low-quality content optimized for an era of clicks and traffic. At Perplexity, we believe in a better internet. That’s why we built Comet. The internet is already better on Comet, but a truly better internet must also reward accurate, well-written news that fuels more accurate AI answers and builds the trust that readers place in their AI assistants. Our vision with Comet and Comet Plus is to create the business model that enables this: Publishers participating in Comet Plus are compensated according to human and AI-driven interactions with their participating content, shifting away from the &quot;pageview at any cost&quot; model that has undermined quality journalism for years. Meet our Comet Plus launch partners. We are honored to launch Comet Plus with some of the world’s best-known, most respected sources of news and culture: Conde Nast, publishers of The New Yorker, Wired, Architectural Digest, Allure, Ars Technica, Bon Appétit, Condé Nast Traveler, Epicurious, GQ, Glamour, Pitchfork, Self, Teen Vogue, them, Vanity Fair, and Vogue. Fortune Le Figaro Le Monde The Los Angeles Times The Washington Post The participation of these leading publishers means Comet Plus users get immediate, frictionless access to premium stories that matter, from global headlines to in-depth analysis and exceptional cultural writing all directly on the web as part of the Comet browsing experience. The agentic internet is for people A better internet starts with Comet. With Comet Plus, a better internet includes the expertise and perspective of top journalists and publishers. Most importantly, Comet Plus makes the internet better by ensuring great publishers and journalists benefit from the new forms of usage people demand in the age of AI. At the same time, Comet Plus is specifically designed to allow publishers to own the direct relationship with their audience. We’re building an ecosystem that rewards trust, accuracy, and curiosity, so everyone can confidently ask bigger, better questions, and get the best answers that move them forward. We’re grateful to our launch partners for believing in this vision. Together, we are supporting the world’s curiosity.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:04 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/announcing-comet-plus-launch-partners</guid>
    </item>
    <item>
      <title>AI at work: getting more done with less hype</title>
      <link>https://www.perplexity.ai/hub/blog/ai-at-work-getting-more-done-with-less-hype</link>
      <description>&lt;![CDATA[Our view of AI is pretty simple: the most powerful application of AI is for it to be useful to people. That’s because our founding principle, curiosity, rests on the simple premise that a human superpower will always be asking more questions. In fact, history shows the most successful and transformational people in work and life are always those with the most questions. Whatever your vision for your work and career, every vision begins with a question. So how do we zoom this lofty ideal into your day-to-day workflow? Perplexity launched more than 75 features and products this year, many of which integrate directly into your workflow and help you do more. These include Comet, Comet Assistant, Labs, Spaces, Email Assistant, connectors for apps like Gmail, Notion, Linear, and GitHub, a voice assistant to ask questions and get to work on the go, video and image generation, Enterprise protections, and more. For practical tips on how they actually come together we recently published our Perplexity at Work guide. The bottom line is we suggest using AI at work in three ways: First, AI can help you get rid of distractions to reclaim your focus. Everything starts with more focus. Then, AI can multiply your effectiveness. AI should help you scale everything you’re already good at. And finally AI should deliver results. AI needs to support the real outcomes that matter to you. Block distractions: using AI to reclaim your focus A key advantage of AI is its potential to protect your attention in an era of endless interruptions. Notifications, calendar pop-ups, and administrative tasks eat away at the time you could spend truly solving problems. Persistent context-switching—jumping between apps, email, and documents—drains focus and makes thoughtful work difficult. Perplexity is structured to minimize distractions: Routine tasks (like sorting email, scheduling, or gathering content across platforms) are delegated to your AI assistant. Research, updates, and project summaries are surfaced directly, cutting back on the need to bounce between tabs or apps. Context follows your work, so you’re never forced to reassemble information that should simply be there. By assigning these tasks to Perplexity’s system, you gain uninterrupted time for focused effort. The reduction in micro-interruptions is not a small benefit—it’s foundational for deep, high-quality output. Scale yourself: AI is best when you lead with your own talent After clearing a path to deep work, Perplexity enables you to multiply your effectiveness. Its approach isn’t to overrule your expertise; it’s to amplify what you already know and do well, letting you operate on a larger and more strategic scale. Integration is key. Instead of approaching AI as an add-on, Perplexity sits within your regular workflows—research, writing, project management, and communication. You can quickly process complex research, synthesize insights from multiple sources, and produce professional deliverables without splitting your attention across the tools required to create them. Advanced functions such as research agents, content synthesis, and task automation let you: Conduct competitive analysis or gather market intelligence across hundreds of sources without hours of manual effort. Produce client-ready reports, presentations, or summaries by giving the intent and letting Perplexity handle formatting and integration. Manage cross-functional projects with a coherent flow of information, making sure nothing is lost in handoffs or transitions. Your expertise remains in charge, but the scale of your impact grows without demanding extra hours or more resources. Get results: use AI to get more done, faster Ultimately, AI should help you achieve results that matter. This means moving beyond outputs to real, measurable progress—documents delivered, sales closed, partnerships formed, strategies clarified, and projects completed efficiently. Perplexity keeps you on track by: Summarizing project histories, feedback, and data for performance reviews or professional growth Supporting personalized outreach and lead generation with timely, relevant intelligence for sales or business development Streamlining project execution, ensuring research, planning, and reporting are both accurate and actionable When routine tasks, research, and summarization are handled, your time is freed for strategic work. You put energy into the activities that lead to career progression or business impact, rather than shuffling information or chasing down small details. AI for every workflow AI earns its place at work when it is practical, accountable, and direct. Whether you’re handling a single meeting, preparing for a product launch, or managing the day-to-day grind, AI should be there in the background—quietly supporting your goals, not adding friction. Great AI should support the way you work, extend your strengths, and help you focus on what matters most. It shouldn’t be a gimmick, a distraction, or source of uncertainty. AI’s current best use is to make work easier by taking on repetitive or high-friction tasks, bringing order to scattered information, and helping you get clarity from noise. Instead of being another app to juggle or subscription to maintain, worthwhile AI should feel like a seamless extension of your abilities and priorities. The bar for using AI at work shouldn't be impressive. It should be useful . For practical tips on how to use Perplexity, download our Perplexity at Work guide to see how teams at NVIDIA, Bridgewater, and PayPal are getting real work done.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:04 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/ai-at-work-getting-more-done-with-less-hype</guid>
    </item>
    <item>
      <title>Just a moment...</title>
      <link>https://www.perplexity.ai/hub/blog/agents-or-bots-making-sense-of-ai-on-the-open-web</link>
      <description>&lt;![CDATA[]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:04 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/agents-or-bots-making-sense-of-ai-on-the-open-web</guid>
    </item>
    <item>
      <title>Accelerating Sonar Through Speculation</title>
      <link>https://www.perplexity.ai/hub/blog/accelerating-sonar-through-speculation</link>
      <description>&lt;![CDATA[Speculative decoding speeds up the generation speed of Large Language Models (LLMs) by using a quick and small draft model to produce completion candidates that are verified by the larger target model. Under this scheme, instead of a run of the expensive target producing a single token, multiple are emitted in a single step. Here we present the implementation details of various kinds of speculative decoding, applied at Perplexity to reduce inter-token latency on Sonar models. Speculative Decoding Speculative Decoding leverages the structure of natural languages and the auto-regressive nature of transformers to speed up token generation. Even though larger models, such as Llama-70B, carry more knowledge than smaller ones, such as Llama-1B, on some simpler tasks they perform similarly. This overlap does suggest that certain sequences are better generated by the less expensive models, leaving complex problems to larger ones. The challenge lies in determining which completions are better and whether the generation of the smaller model is of the same quality as that of the larger one. Fortunately, LLMs are auto-regressive transformers: when given a sequence of tokens, they output the probability distribution of the next token. Additionally, the logits derived from the intermediate features associated with the tokens in the input sequence also indicate how likely it is for the model to issue those exact tokens. This property enables speculation: if a sequence of tokens is generated by a smaller one starting from an input prefix, it can be run through the larger one to determine how well it lines up with the target model. Each prefix of the candidates is scored with a probability and the longest one above an acceptance threshold is picked. As a bonus, the target model also provides a subsequent token for free: if a draft model generates n tokens, up to n + 1 can be emitted in one step. At inference time, speculative sampling process can be split into roughly 4 stages: Prefill: both the target and the draft models must be run on the input sequence to populate the KV cache entries. While some schemes, such as Medusa, use simpler dense layers for prediction, in this post we focus on transformer-based drafts that need their own KV caches. Draft generation: the draft model iterates to produce a number of fixed tokens. The draft sequence can be linear or the model can explore a tree-like structure up to a given depth (EAGLE, Medusa). Here, we focus on linear sequences. Acceptance: the target model runs on the draft sequence, building logits corresponding to each draft token. The length of the longest acceptable sequence is determined. Target generation: since the target generated logits, at the mismatched position or the tail end of the sequence the logits correspond to a subsequent token. These logits can be sampled to provide a robust token from the target, capping off the sequence. Various methods exist to implement speculative decoding. In this post, we will focus on the schemes we used to accelerate Sonar models using an in-house 1B model, as well as the prediction mechanisms we are building out to speed up models at the scale of DeepSeek. Target-Draft Speculative decoding can be achieved by coupling an existing small LLM as a draft model to a target model to generate candidate sequences. In production, we have accelerated Sonar using a Llama-1B model fine-tuned on the same dataset as the target. While this approach did not require training a draft from scratch, the small model still uses significant KV cache capacity and introduces a slight prefill overhead, increasing TTFT. Under this scheme, the decoder only speculates on decode-only batches, generating tokens through standard sampling during prefill or on mixed prefill-decode batches. In the prefill stage, the target logits are immediately sampled to also prefill the newly-generated token in the KV cache of the draft. The draft is not sampled yet, but the logits it produces are carried over to the decode stage. In decode, the draft model is advanced, sampling the top token at each stage. After the desired draft length is reached, the tokens are run through the target model to produce the logits based on which the sampler identifies the accepted sequence length. Acceptance is determined by comparing the full probability distributions from the draft and the target. Since the target always outputs one set of logits following the accepted draft sequence, that is sampled to produce an additional output. Since the draft model has not yet seen that accepted token, it is re-run to populate its corresponding KV cache entries in preparation for the next decode step, carrying the logits over again. EAGLE EAGLE is speculative decoding scheme which explores multiple draft sequences, generated through a tree-like traversal of probable draft tokens. A fixed (EAGLE) or dynamically-shaped (EAGLE-2) tree is explored using consecutive executions of the draft tokens, considering the Top-K candidates at each node instead of following the highest scoring token in a linear sequence. The sequences are then scored and the longest suitable one is selected to continue, also appending an additional token from the target. In order to achieve more accurate prediction, an EAGLE draft model predicts not only based on tokens, but also using the target features (last layer hidden states) of the target model. The disadvantage of EAGLE is the need to train custom, small draft models which are accurate enough to generate suitable candidates within a low latency budget. Typically, a draft model is a single transformer layer identical to a decoder layer of the original model, which is tightly coupled to the target by tying to its embeddings and lm_head projections. Since this requires less KV cache capacity, EAGLE has a lower memory footprint. To verify tree-like sequences in the target model, custom attention masks must be used. Unfortunately, using a custom attention mask for a whole sequence significantly slows down attention for realistic input lengths (by up to 50%), nullifying some of the speedup achievable through speculation. We have not yet deployed full tree exploration to production for this reason, focusing instead on the special case of single-token prediction via MTP-like schemes presented in the DeepSeek-V3 Technical Report. MTP This scheme is similar to draft-target decoding, with the exception of hidden states being used alongside tokens for prediction. Slightly more work must be done in both the prefill and decode stages compared to regular draft-target speculation. The draft model uses both tokens and hidden states: token t_{i+1} is sampled from the logits L_i corresponding to token t_i , which in turn are derived from the hidden states H_i . Consequently, the input token buffers must be shifted one step to the left relative to the hidden state vectors output by the target. The figure below marks the correspondences used for training, as well as the shift during inference. The decoding flow is quite similar to draft-target decoding, with the exception of both hidden states and logits being carried over. Our implementation shares all the associated sampling and logit processing logits, specializing only the model forward invocations. When multiple tokens are predicted, the draft model uses draft hidden states for prediction, also populating KV cache entries based on its own features. In the long run, this can degrade accuracy. Subsequently, when running the draft model to populate the KV cache entry for the target prediction, we run it on the whole sequence taking the more accurate target hidden states as inputs. Since these draft models are small, the added cost of processing the additional tokens is negligible. Training MTP Heads In order to benefit from MTP, we built the infrastructure required to train MTP heads attached to our fine-tuned models on Perplexity’s datasets, running on one node with 8xH100 devices. In about one day, we can build heads for models ranging from Llama-1B to Llama-70B and DeepSeek V2-Lite. For larger models, we rely on MTP heads built during the fine-tuning process. The target of MTP training is to match up the draft hidden states and logits extrapolated from the target hidden states to the next token logits and hidden states of the target. Since inference for hidden states is expensive, we pre-compute them using our inference-optimized implementation of the target model, to be used during training. However, to validate the inference MTP implementation and ensure that numerical differences due to quantization or optimizations do not hinder results, for validation loss and accuracy estimation we fully re-use the inference implementation of both the target and the draft models. When scaling from the ShareGPT dataset used in the original paper to larger samples, we noticed that the MTP head architecture outlined and implemented in the EAGLE paper failed to train for 70B-sized models. Unlike ShareGPT which contained a larger number of shorter sequences, we train on a slightly smaller number of substantially longer prompts. Since the original EAGLE heads slightly diverged in structure from a typical transformer, we re-introduced some RMS Normalization layers that were stripped. We found that this not only allowed training to converge, but it also boosted the accuracy of the heads by a few percentage points. Not only do layer norms facilitate training, re-introducing the norms is also mathematically intuitive. MTP heads re-use the embeddings and the logit projections of the target model, as they can be substantial in size (about 2 GB for Llama 70B). During training, these are frozen and the expectation is that the MTP layer learns to embed predictions into the same vector space as what the projection layer of the original model learnt during training. By dropping the norms, a single MLP is expected to learn the same function as an MLP followed by a norm, which hinders the matchup between the hidden states of the draft and the target models. Inference with Speculative Decoding In the inference engine, in order to generate tokens for input sequences, they need to be first grouped into reasonably-sized batches, then pages must be allocated in the KV cache for the next tokens. The input tokens and the KV page information is then packed into a buffer broadcast to all parallel ranks running the model. Finally, the metadata is copied into GPU memory and the model is executed to produce the logits from which the next token is sampled. Unlike certain implementations which loosely couple a draft and target inference server via a wrapper that orchestrates requests between them, our draft-target pairs are tightly coupled and step through generation in unison. Batch scheduling and KV page allocation is shared between the models for all forms of speculative decoding: this unifies the logic that bridges a model with the overarching inference server, as they all expose the same interface. The inference runtime at Perplexity is shaped around FlashInfer, which determines the metadata that needs to be built in order to configure and schedule the attention kernel. Given some input sequences forming a batch, for prefill, decode or verification, CPU-side work must be done to allocate intermediate buffers and populate certain constant buffers used in attention. This work is in addition to the cost of batch scheduling and KV page allocation, which also incur latencies that must be hidden in order to maximize GPU utilization. While we fully parallelized CPU-side and GPU-side work for inference without speculation, we found that the CPU-GPU balance for speculative decoding is more intricate. The main challenge arises from the fact that the number of accepted tokens determines the sequence length for a subsequent run, introducing a difficult-to-avoid GPU-to-CPU synchronization point. We experimented with different scheduling schemes in order to best hide the latency of CPU work. Draft-Target Schedule Despite being smaller than a target model, when an entire LLM is used as the draft, it still introduces considerable latency on the GPU, providing some headroom to hide expensive CPU operations. Since smaller models do not benefit from tensor parallelism, there is a mismatch between the number of ranks a target and a draft are sharded across. In our implementation, the draft model runs only on the leader rank of a TP group. As indicated before, a decode step carries over logits into the next run. This allows us to overlap one execution of the draft model with the CPU-side batch scheduling work. After the batch is put together, repeated calls to the sampler and the draft produce the draft tokens. In parallel, the batch for verification is put together for the target model and synchronized with the parallel workers. The target logits are verified and sampled to determine the accepted sequence lengths. At this point, GPU-to-CPU synchronization is necessary in order to determine subsequent sequence lengths. Since the draft model is only run on the leader node, its batch is set up sequentially and its execution is kicked off to populate its KV cache entries with the additional token that the target produced. The logits produced by this draft run in the current run will be used to sample the first draft token in the subsequent run. Most importantly, while the draft is running, the next batch can be scheduled. MTP Schedule for a Single Token While the runtime does not yet provide Eagle-style draft tree exploration, we implemented a special case of this scheme, considering a linear sequence of draft tokens produced by a model the size of a single transformer decoder layer. This scheme can be used for draft prediction using the open-source weights of DeepSeek R1. The sub-case of predicting a single token is interesting, as large MTP layers achieve sufficiently high acceptance rates to justify their overhead. MTP scheduling is somewhat more complex, as the draft model is much faster, hiding less CPU-side latency. Additionally, the draft is sharded alongside the target model, requiring shared memory transfers for batch information. A run starts by transferring batch info and sampling the first token from carry-over logits, similarly to the previous scheme. Next, the target is run to validate tokens, processing 2 * D tokens, where D is the decode batch size. This is ideal for micro-batching in Mixture-of-Experts (MoE) models over slower interconnects such as InfiniBand, as the batch splits evenly into two halves. The hidden states of the target carry over to the next draft run, while the logits are passed into the sampler for verification. By performing a limited amount of additional work on the GPU, we avoid CPU-to-GPU synchronization after draft sequence acceptance. After the input tokens of the targets are shifted, a kernel plugs in the next target tokens into their corresponding locations. The draft is then re-run with the same batch information as the target, populating KV cache entries and building the logits and hidden states for the next run, doing some redundant work on tokens which were not accepted. In these situations, the latency of the unused work is barely measurable due to the small size of the draft model. In parallel with the draft run, sequence lengths are determined on the CPU and the scheduling of the next batch is kicked off, without having to wait for GPU work to terminate. The overhead of additional work in the draft layer is not noticeable in attention, however MLP layers are more problematic. Since matrix multiplication instructions pad to a boundary of 64 along the dimension of number of tokens, if doubling doesn’t require significantly more blocks, the overhead is hidden. For longer draft sequences the overhead is more expensive and the scheme used for regular draft-target models works better. References Fast Inference from Transformers via Speculative Decoding EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads Layer Normalization FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving DeepSeek-V3 Technical Report]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:04 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/accelerating-sonar-through-speculation</guid>
    </item>
    <item>
      <title>About the Dow Jones lawsuit</title>
      <link>https://www.perplexity.ai/hub/blog/about-the-dow-jones-lawsuit</link>
      <description>&lt;![CDATA[On Monday, we got sued by the Wall Street Journal and the New York Post. We were disappointed and surprised to see this. There are around three dozen lawsuits by media companies against generative AI tools. The common theme betrayed by those complaints collectively is that they wish this technology didn’t exist. They prefer to live in a world where publicly reported facts are owned by corporations, and no one can do anything with those publicly reported facts without paying a toll. That is not our view of the world. We believe that tools like Perplexity provide a fundamentally transformative way for people to learn facts about the world. Perplexity not only does so in a way that the law has always recognized but is essential for the sound functioning of a cultural ecosystem in which people can efficiently and effectively obtain and engage with knowledge created by others. Perplexity, from its founding moment, has always listed sources above answers and provided in-line citations for every part of an answer. We are glad that other AI chatbots have begun copying Perplexity's transparency and emphasis on sources in their products. In fact, the Wall Street Journal itself earlier this year ranked Perplexity the #1 overall chatbot in their “Great AI Challenge.” The lawsuit reflects an adversarial posture between media and tech that is—while depressingly familiar—fundamentally shortsighted, unnecessary, and self-defeating. We should all be working together to offer people amazing new tools and build genuinely pie-expanding businesses. There are countless things we would love to do beyond what the default application of law allows, which entail mutually beneficial commercial relationships with counterparties like the companies here who chose to sue rather than cooperate. Perplexity is proud to have launched a first-of-its-kind revenue-sharing program with leading publishers like TIME, Fortune, and Der Spiegel, which have already signed on. And our door is always open if and when the Post and the Journal decide to work with us in good faith, just as numerous others already have. Unless and until that happens, though, we will defend ourselves in this lawsuit. This is not the place to get into the weeds of all of that, but we want to make two quick points at the outset: First, the facts alleged in the complaint are misleading at best. Cited examples of “regurgitated” outputs explicitly mischaracterize the source of the material. They are disingenuous in their description of what happened even in the specific cited instances, as well as in their broader depiction of what Perplexity is for (spoiler alert: it’s not for reprising the full text of articles that can be more directly and efficiently obtained elsewhere). And the suggestion that we never responded to outreach from News Corp. is simply false: they reached out; we responded the very same day; instead of continuing the dialogue, they filed this lawsuit. Second, we have learned in the short time since this lawsuit was filed, a disturbing trend in these types of cases: The companies that are suing make all kinds of salacious allegations in their complaints about all kinds of seemingly bad things they were able to coax the AI tools to do—and then, when pressed in the litigation for details of things like how they achieved such obviously unrepresentative results, they immediately disavow the very examples they put in the public record, and swear they won’t actually use them in the case. We presume that is what will happen here. And that will tell you everything you need to know about the strength of their case. AI-enhanced search engines are not going away. Perplexity is not going away. We look forward to a time in the future when we can focus all of our energy and attention on offering innovative tools to customers, in collaboration with media companies.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:04 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/about-the-dow-jones-lawsuit</guid>
    </item>
    <item>
      <title>Just a moment...</title>
      <link>https://www.perplexity.ai/hub/blog/a-student-s-guide-to-using-perplexity-spaces</link>
      <description>&lt;![CDATA[]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:03 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/a-student-s-guide-to-using-perplexity-spaces</guid>
    </item>
    <item>
      <title>A Personal Assistant for Your Inbox</title>
      <link>https://www.perplexity.ai/hub/blog/a-personal-assistant-for-your-inbox</link>
      <description>&lt;![CDATA[Introducing Email Assistant Today we're launching Perplexity Email Assistant, exclusive for Perplexity Max subscribers. Email Assistant brings a powerful personal assistant to your email, transforming your inbox to action. Perplexity Max subscribers have shown powerful adoption of the Comet Assistant that launched with Comet and Comet Max Assistant launched in July exclusively for Max subscribers. In fact, early data shows the Comet assistants increase the number of questions and tasks users accomplish per day by 3-18X. Simply put, a powerful and personal AI assistant helps you get a lot more done. Your inbox deserves the same intelligent partnership, which is why we’ve built Perplexity Email Assistant. Email Assistant connects directly with your email account across your phone and computer. It drafts replies, organizes messages, schedules meetings, and more. You power through inbox tasks, in less time, while staying in control. A powerful, personal assistant Email is more than a message center. Your inbox contains your professional memory, your relationships, calendaring, and coordination. Meanwhile, your outbox is your productivity and your reputation. The most successful workers and thinkers hire personal assistants for their email instead of relying on AI tools and algorithms that handle rote tasks. There’s too much at stake. Perplexity's Email Assistant is more than a tool, and available to anyone. Email Assistant learns your communication style and priorities. It drafts responses matching your tone and suggests meeting times based on your calendar preferences, saving time on routine tasks. Email Assistant is also secure. Email Assistant is SOC 2 and GDPR compliance by default and never trains on your data. Just ask Beginning today, Max Subscribers can sign up for Email Assistant here . Then get started simply by emailing assistant@perplexity.com from your own inbox. The email assistant knows it's you and gets right to work. Calendar meetings by cc’ing your assistant on any email. The assistant will work with your contacts in your style on the back-and-forth exchanges that steal hours from your day, all while ensuring you still have total control. You can also ask your Email Assistant questions about your inbox: &quot;What emails should I prioritize before my board meeting?&quot; &quot;Summarize all messages about the Q4 budget.&quot; &quot;Show me anything urgent from the design team this week.” Comet users already enjoy this capability, where questions can be asked of any web interface. Email assistant powers your curiosity beyond predefined functions. Ask anything about your inbox and discover new ways to extract value from your email data. The more you ask, the more you uncover possibilities we haven't even considered yet. Work smarter, faster Email Assistant connects with Gmail and Outlook. Smart labels automatically organize your inbox, showing what's completed, what needs action, and what requires your attention. No more scanning hundreds of messages to find what’s critical. Another benefit is auto-draft. Email Assistant writes responses for you to edit or send, eliminating the biggest email bottleneck: getting started. Email Assistant adapts to your communication patterns and priorities. It drafts responses matching your tone, suggests meeting times based on your calendar habits, and applies labels you actually use. Every interaction teaches it to work more like you would. Built for you Email Assistant works where you work, answering the questions that drive your most important decisions. It’s built to help you focus on what matters, no matter where. The time you reclaim from routine correspondence and rote tasks in your inbox becomes time for deeper work, meaningful conversations, and even bigger questions. Email Assistant is available now for Perplexity Max subscribers. Just go to the Email Assistant hub and connect your email. Then, start with a simple question: &quot;What needs my attention first?&quot; Add your email assistant to any conversation, and turn your inbox into action.]]&gt;</description>
      <pubDate>Wed, 28 Jan 2026 09:47:03 GMT</pubDate>
      <guid isPermaLink="true">https://www.perplexity.ai/hub/blog/a-personal-assistant-for-your-inbox</guid>
    </item>
  </channel>
</rss>