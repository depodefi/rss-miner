<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>NVIDIA Generative AI News</title>
    <link>https://blogs.nvidia.com/blog/category/generative-ai/</link>
    <description><![CDATA[Latest news from NVIDIA Generative AI Blog]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 28 Jan 2026 09:46:56 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Defining Her Own Path: Asma Farjallahâ€™s Pursuit of Excellence in AI</title>
      <link>https://blogs.nvidia.com/blog/nvidia-life-asma-farjallah/</link>
      <description><![CDATA[For Asma Farjallah, striving for excellence has never been a goal â€” but a way of life. As an AI developer technology engineer at NVIDIA, she builds GPU-accelerated solutions for deep learning workloads. Born and raised in Sousse, Tunisia, Farjallah is the eldest of three daughters in a family deeply rooted in academics. Her father was an engineer, and her mother had a background in advanced physics. Scientific curiosity and the pursuit of knowledge were regular topics at the dinner table. After excelling in preparatory school, Farjallah earned a government-sponsored scholarship to study computer science at the Enseirb-Matmeca engineering school in Bordeaux, France. She later pursued a doctoral degree in computer science at the University of Versailles Saint-Quentin-en-Yvelines. She joined NVIDIA in 2019 as a senior solution architect based in the Courbevoie office, near Paris, where she worked on high-performance computing and AI. Her role involved helping French computing centers port their applications to GPUs. She also collaborated with the global energy team to integrate GPUDirect Storage into a seismic imaging code known as Kirchhoff migration . Over five months, they optimized input and output operations to significantly boost GPU performance. â€œThis project showcased the best of collaboration,â€ Farjallah said. â€œOur close work with the GPUDirect Storage team enabled us to successfully integrate the technology and realize measurable performance gains.â€ Farjallah values the autonomy and flexibility sheâ€™s found working at NVIDIA. â€œYouâ€™re encouraged to shape your role based on your strengths and interests,â€ she said. â€œAfter years of working in scientific computing, I had the chance to shift focus to AI and GPU programming, which required a new way of thinking and presenting proof-of-concept solutions.â€ That curiosity and drive for hands-on engineering led her to a six-month rotation with the AI developer technology team last year. The experience rekindled her passion for application engineering, exploring technologies at a deeper level and building from the ground up. Since officially joining that team in late 2024, sheâ€™s worked with tools like NVIDIA Nsight Systems and NVIDIA Nsight Compute , collaborating with the NVIDIA TensorRT-LLM team on performance analysis as well as with an AI team to benchmark applications and project their performance on future hardware. Farjallah is optimistic about carving her own path in developer technology. Sheâ€™s especially excited by the transformative power of AI tools, including coding assistants that boost developer productivity and streamline workflows. She remains grounded in the fundamentals and offers this guidance to aspiring technologists: â€œTake the time to understand the core concepts. Be curious, question everything, stay open-minded and always recognize the contributions of others as you forge your professional journey.â€ Follow @nvidialife on Instagram and learn more about NVIDIA life, culture and careers . Categories: NVIDIA Life]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 27 Jan 2026 16:00:59 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-life-asma-farjallah/</guid>
    </item>
    <item>
      <title>NVIDIA Launches Earth-2 Family of Open Models â€” the Worldâ€™s First Fully Open, Accelerated Set of Models and Tools for AI Weather</title>
      <link>https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/</link>
      <description><![CDATA[At the American Meteorological Societyâ€™s Annual Meeting, NVIDIA today unveiled a new NVIDIA Earth-2 family of open models,â€¦Read Article]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 26 Jan 2026 14:00:53 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-earth-2-open-models/</guid>
    </item>
    <item>
      <title>NVIDIA DRIVE AV Raises the Bar for Vehicle Safety as Mercedes-Benz CLA Earns Top Euro NCAP Award</title>
      <link>https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/</link>
      <description><![CDATA[AI-powered driver assistance technologies are becoming standard equipment, fundamentally changing how vehicle safety is assessed and validated. The recent recognition of the Mercedes-Benz CLA as Euro NCAPâ€™s Best Performer of 2025 underscores this shift, as the vehicle combines traditional passive safety features with NVIDIA DRIVE AV software to achieve the highest overall safety score of the year. â€‹â€‹â€œWhen Euro NCAP assesses vehicle safety, it evaluates both passive and active systems â€” achieving a perfect score requires a state-of-the-art advanced driver assistance system,â€ said Ola KÃ¤llenius, CEO of the Mercedes-Benz Group. â€œThis milestone represents the culmination of five years of collaboration between Mercedes-Benz and NVIDIA to enhance real-world safety and deliver tangible value to customers.â€ Euro NCAP (European New Car Assessment Programme) has for nearly 30 years served as Europeâ€™s independent vehicle safety authority, backed by European governments, motoring organizations and consumer groups. Euro NCAP evaluates vehicles across four categories that reflect real-world safety. For AI-powered driver assistance, the most relevant are the â€œVulnerable Road Userâ€ and â€œSafety Assistâ€ categories, which assess technologies designed to help prevent crashes â€” including automatic emergency braking, lane-keeping support and speed assistance. Only vehicles achieving five-star ratings with standard equipment qualify for â€œBest in Classâ€ recognition, with winners determined by weighted scores across all categories. In 2025, Euro NCAP tested a record 49 models. Safety Comes First: How DRIVE AV Is Built for Trust Safety ratings like Euro NCAP are increasingly recognizing vehicles that combine strong passive protection with advanced active safety performance. As AI becomes central to driving, the benchmark for the â€œsafestâ€ car will increasingly be defined not only by how well a vehicle handles a crash, but how effectively it helps prevent one. The Mercedes-Benz CLA is built with NVIDIA DRIVE AV, a dual-stack architecture thatâ€™s designed to help automakers deliver systems that arenâ€™t only intelligent, but predictable, verifiable and resilient in the real world. The architecture pairs an AI-driven end-to-end driving system with a parallel classical safety stack to provide redundancy across AV sensing, planning and execution. The CLA is also built on the NVIDIA DRIVE Hyperion architecture, which incorporates sensor diversity and hardware redundancy into the vehicleâ€™s overall design. At the heart of this approach is NVIDIA Halos â€” a comprehensive safety system spanning hardware, software, tools, development processes and certification support. Halos delivers a structured safety foundation for developing automated driving and other AI capabilities while staying anchored to robust guardrails, redundancy and fault tolerance. Third-party certification and assessments are also important to build trust: TÃœV SÃœD granted the ISO 21434 Cybersecurity Process certification to NVIDIA for its automotive system-on-a-chip, platform and software engineering processes. Additionally, NVIDIA DriveOS 6.0 conforms to ISO 26262 Automotive Safety Integrity Level (ASIL) D standards. TÃœV Rheinland performed an independent United Nations Economic Commission for Europe (UNECE) safety assessment of NVIDIA DRIVE AV related to safety requirements for complex electronic systems, which NVIDIA successfully completed. NVIDIA recently released its Alpamayo family of open AI models, simulation tools and datasets â€” which enables AVs to navigate even rare, â€œlong-tailâ€ events they havenâ€™t been trained on by breaking the scenario down into smaller steps, reasoning through multiple possible actions and ultimately selecting the safest one. Using these models with the parallel classical safety stack in the NVIDIA DRIVE AV dual-stack architecture provides an additional layer of protection to keep vehicles operating within safe boundaries. Training Safety Through Data and Simulation Modern AI-driven safety systems learn from exponentially more driving scenarios than any human could experience in a lifetime. NVIDIAâ€™s cloud-to-car development approach transforms real-world data into billions of simulated miles using NVIDIA DGX systems for neural network training, the NVIDIA Omniverse and Cosmos platforms for simulation, and NVIDIA DRIVE AGX for in-vehicle computing. This methodology addresses a critical challenge in safety validation: training AI to navigate rare but high-risk edge cases that are too dangerous â€” or too infrequent â€” to test reliably in the real world. By generating synthetic scenarios that represent these rare situations, AI systems can learn appropriate responses during development without putting people at risk. The CLAâ€™s recognition is more than a single model earning a top rating â€” it reflects a broader shift in what safety means in the modern vehicle, where trusted crash protection is paired with AI-enabled driver assistance designed to help avoid accidents in the first place. Categories: Driving Tags: NVIDIA DRIVE]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 22 Jan 2026 18:21:49 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/drive-av-mercedes-benz-cla-euro-ncap-safety-award/</guid>
    </item>
    <item>
      <title>How to Get Started With Visual Generative AI on NVIDIA RTX PCs</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/</link>
      <description><![CDATA[AI-powered content generation is now embedded in everyday tools like Adobe and Canva, with a slew of agencies and studios incorporating the technology into their workflows. Image models now deliver photorealistic results consistently, video models can generate long and coherent clips, and both can follow creative directions. Creators are increasingly running these workflows locally on PCs to keep assets under direct control, remove cloud service costs and eliminate the friction of iteration â€” making it easier to refine outputs at the pace real creative projects demand. Since their inception, NVIDIA RTX PCs have been the system of choice for running creative AI due to their high performance â€” reducing iteration time â€” and the fact that users can run models on them for free, removing token anxiety. With recent RTX optimizations and new open-weight models introduced at CES earlier this month, creatives can work faster, more efficiently and with far greater creative control. How to Get Started Getting started with visual generative AI can feel complex and limiting. Online AI generators are easy to use but offer limited control. Open source community tools like ComfyUI simplify setting up advanced creative workflows and are easy to install. They also provide an easy way to download the latest and greatest models, such as FLUX.2 and LTX-2, as well as top community workflows. Hereâ€™s how to get started with visual generative AI locally on RTX PCs using ComfyUI and popular models: Visit comfy.org to download and install ComfyUI for Windows. Launch ComfyUI. Create an initial image using the starter template: Click on the â€œTemplatesâ€ button, then on â€œGetting Startedâ€ and choose â€œ1.1 Starter â€“ Text to Image.â€ Connect the model â€œNodeâ€ to the â€œSave Image Node.â€ The nodes work in a pipeline to generate content using AI. Press the blue â€œRunâ€ button and watch the green â€œNodeâ€ highlight as the RTX-powered PC generates its first image. Change the prompt and run it again to enter more deeply into the creative world of visual generative AI. Read more below on how to dive into additional ComfyUI templates that use more advanced image and video models. Model Sizes and GPUs As users get more familiar with ComfyUI and the models that support it, theyâ€™ll need to consider GPU VRAM capacity and whether a model will fit within it. Here are some examples for getting started, depending on GPU VRAM: *Use FP4 models with NVIDIA GeForce RTX 50 Series GPUs, and FP8 models with RTX 40 Series GPUs for best results. This lets models use less VRAM while providing more performance. Generating Images To explore how to improve image generation quality using FLUX.2-Dev: From the ComfyUI â€œTemplatesâ€ section, click on â€œAll Templatesâ€ and search for â€œFLUX.2 Dev Text to Image.â€ Select it, and ComfyUI will load the collection of connected nodes, or â€œWorkflow.â€ FLUX.2-Dev has model weights that will need to be downloaded. Model weights are the â€œknowledgeâ€ inside an AI model â€” think of them like the synapses in a brain. When an image generation model like FLUX.2 was trained, it learned patterns from millions of images. Those patterns are stored as billions of numerical values called â€œweights.â€ ComfyUI doesnâ€™t come with these weights built in. Instead, it downloads them on demand from repositories like Hugging Face. These files are large (FLUX.2 can be >30GB depending on the version), which is why systems need enough storage and download time to grab them. A dialog will appear to guide users through downloading the model weights. The weight files (filename.safetensors) are automatically saved to the correct ComfyUI folder on a userâ€™s PC. Saving Workflows: Now that the model weights are downloaded, the next step is to save this newly downloaded template as a â€œWorkflow.â€ Users can click on the top-left hamburger menu (three lines) and choose â€œSave.â€ The workflow is now saved in the userâ€™s list of â€œWorkflowsâ€ (press W to show or hide the window). Close the tab to exit the workflow without losing any work. If the download dialog was accidentally closed before the model weights finished downloading: Press W to quickly open the â€œWorkflowsâ€ window. Select the Workflow and ComfyUI will load it. This will also prompt for any missing model weights to download. ComfyUI is now ready to generate an image using FLUX.2-Dev. Prompt Tips for FLUX.2-Dev: Start with clear, concrete descriptions of the subject, setting, style and mood â€” for example: â€œCinematic closeup of a vintage race car in the rain, neon reflections on wet asphalt, high contrast, 35mm photography.â€ Shortâ€‘toâ€‘medium length prompts â€” a single, focused sentence or two â€” are usually easier to control than long, storylike prompts, especially when getting started. Add constraints to guide consistency and quality. Specify things like: Framing (â€œwide shotâ€ or â€œportraitâ€) Detail level (â€œhigh detail, sharp focusâ€) Realism (â€œphotorealisticâ€ or â€œstylized illustrationâ€) If results are too busy, remove adjectives instead of adding more. Avoid negative prompting â€” stick to prompting whatâ€™s desired. Learn more about FLUX.2 prompting in this guide from Black Forest Labs. Save Locations on Disk: Once done refining the image, right click on â€œSave Image Nodeâ€ to open the image in a browser, or save it in a new location. ComfyUIâ€™s default output folders are typically the following, based on the application type and OS: Windows (Standalone/Portable Version): The folder is usually found in C:\ComfyUI\output or a similar path within where the program was unzipped. Windows (Desktop Application): The path is usually located within the AppData directory, like: C:\Users\%username%\AppData\Local\Programs\@comfyorgcomfyui-electron\resources\ComfyUI\output Linux: The installation location defaults to ~/.config/ComfyUI. Prompting Videos Explore how to improve video generation quality, using the new LTX-2 model as an example: Lightrickâ€™s LTXâ€‘2 is an advanced audio-video model designed for controllable, storyboard-style video generation in ComfyUI. Once the LTXâ€‘2 Image to Video Template and model weights are downloaded, start by treating the prompt like a short shot description, rather than a full movie script. Unlike the first two Templates, LTXâ€‘2 Image to Video combines an image and a text prompt to generate video. Users can take one of the images generated in FLUX.2-Dev and add a text prompt to give it life. Prompt Tips for LTXâ€‘2: For best results in ComfyUI, write a single flowing paragraph in the present tense or use a simple, scriptâ€‘style format with scene headings (sluglines), action, character names and dialogue. Aim for four to six descriptive sentences that cover all the key aspects: Establish the shot and scene (wide/medium/closeup, lighting, color, textures, atmosphere). Describe the action as a clear sequence, define characters with visible traits and body language, and specify camera moves. Lastly, add audio, such as ambient sound, music and dialogue, using quotation marks. Match the level of detail to the shot scale. For example, closeups need more precise character and texture detail than wide shots. Be clear on how the camera relates to the subject, not just where it moves. Additional details to consider adding to prompts: Camera movement language: Specify directions like â€œslow dolly in,â€ â€œhandheld tracking,â€ â€œoverâ€‘theâ€‘shoulder shot,â€ â€œpans across,â€ â€œtilts upward,â€ â€œpushes in,â€ â€œpulls backâ€ or â€œstatic frame.â€ Shot types: Specify wide, medium or closeâ€‘ups with thoughtful lighting, shallow depth of field and natural motion. Pacing: Direct for slow motion, timeâ€‘lapses, lingering shots, continuous shots, freeze frames or seamless transitions that shape rhythm and tone. Atmosphere: Add details like fog, mist, rain, golden hour light, reflections and rich surface textures that ground the scene. Style: Early in the prompt, specify styles like painterly, film noir, analog film, stopâ€‘motion, pixelated edges, fashion editorial or surreal. Lighting : Direct backlighting, specific color palettes, soft rim light, lens flares or other lighting details using specific language. Emotions : Focus on prompting for singleâ€‘subject performances with clear facial expressions and small gestures. Voice and audio : Prompt characters to speak or sing in different languages, supported by clear ambient sound descriptions. Optimizing VRAM Usage and Image Quality As a frontier model, LTX-2 uses significant amounts of video memory (VRAM) to deliver quality results. Memory use goes up as resolution, frame rates, length or steps increase. ComfyUI and NVIDIA have collaborated to optimize a weight streaming feature that allows users to offload parts of the workflow to system memory if their GPU runs out of VRAM â€” but this comes at a cost in performance. Depending on the GPU and use case, users may want to constrain these factors to ensure reasonable generation times. LTX-2 is an incredibly advanced model â€” but as with any model, tweaking the settings has a big impact on quality. Learn more about optimizing LTX-2 usage with RTX GPUs in the Quick Start Guide for LTX-2 In ComfyUI . Building a Custom Workflow With FLUX.2-Dev and LTX-2 Users can simplify the process of hopping between ComfyUI Workflows with FLUX.2-Dev to generate an image, finding it on disk and adding it as an image prompt to the LTX-2 Image to Video Workflow by combining the models into a new workflow: Open the saved FLUX.2-Dev Text to Image Workflow. Ctrl+left mouse click the FLUX.2-Dev Text to Image node. In the LTX-2 Image to Video Workflow, paste the node using Ctrl+V. Simply hover over the FLUX.2-Dev Text to Image node IMAGE dot, left click and drag to the Resize Image/Mask Input dot. A blue connector will appear. Save with a new name, and text prompt for image and video in one workflow. Advanced 3D Generation Beyond generating images with FLUX.2 and videos with LTXâ€‘2, the next step is adding 3D guidance. The NVIDIA Blueprint for 3D-guided generative AI shows how to use 3D scenes and assets to drive more controllable, production-style image and video pipelines on RTX PCs â€” with ready-made workflows users can inspect, tweak and extend. Creators can show off their work, connect with other users and find help on the Stable Diffusion subreddit and ComfyUI Discord . #ICYMI â€” The Latest Advancements in NVIDIA RTX AI PCs ðŸ’» NVIDIA @ CES 2026 CES announcements included 4K AI video generation acceleration on PCs with LTX-2 and ComfyUI upgrades. Plus, major RTX accelerations across ComfyUI, LTX-2, Llama.cpp, Ollama, Hyperlink and more unlock video, image and text generation use cases on AI PCs. ðŸ“ Black Forest Labs FLUX 2 Variants FLUX.2 [klein] is a set of compact, ultrafast models that support both image generation and editing, delivering state-of-the-art image quality. The models are accelerated by NVFP4 and NVFP8, boosting speed by up to 2.5x and enabling them to run performantly across a wide range of RTX GPUs. âœ¨Project G-Assist Update With a new â€œReasoning Modeâ€ enabled by default, Project G-Assist gains an accuracy and intelligence boost, as well as the ability to action multiple commands at once. G-Assist can now control settings on G-SYNC monitors, CORSAIR peripherals and CORSAIR PC components through iCUE â€” covering lighting, profiles, performance and cooling. Support is also coming soon to Elgato Stream Decks, bringing G-Assist closer to a unified AI interface for tuning and controlling nearly any system. For G-Assist plug-in devs, a new Cursor-based plug-in builder accelerates development using Cursorâ€™s agentic coding environment. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | Creators | GeForce RTX | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 22 Jan 2026 14:00:57 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-comfyui-tutorial/</guid>
    </item>
    <item>
      <title>From Pilot to Profit: Survey Reveals the Financial Services Industry Is Doubling Down on AI Investment and Open Source</title>
      <link>https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/</link>
      <description><![CDATA[AI has taken center stage in financial services, automating the research and execution behind algorithmic trading and helping banks more accurately detect fraud and money laundering â€” all while improving risk management practices and expediting document processing. The sixth annual â€œNVIDIA State of AI in Financial Servicesâ€ report , based on a survey of more than 800 industry professionals, found that AI usage in the industry has never been higher. Organizations are deploying and scaling AI use cases, such as fraud detection, risk management and customer service, to improve critical business functions that create meaningful return on investment. New types of AI â€” including AI agents â€” are streamlining processes ranging from back-office operations to investment research as financial institutions embrace the tools needed to build specialized AI, including open source foundation models and software. Highlights from this yearâ€™s report include: 89% said AI is helping increase annual revenue and decrease annual costs. 73% of executives said AI is crucial to their future success, and nearly 100% said their AI budgets will increase or stay the same in the next year. 65% of respondents said their company is actively using AI, up from 45% in last yearâ€™s report. 61% are using or assessing generative AI, up 52% year over year. 84% said open source models and software are important to their AI strategy. 42% are using or assessing agentic AI, with 21% saying theyâ€™ve already deployed AI agents. â€œOpen source models are fundamentally changing the competitive dynamics in financial AI,â€ said Helen Yu, CEO of Tigon Advisory Corp. â€œThe real value capture happens when institutions fine-tune these models on their proprietary transaction data, customer interaction histories and market intelligence, creating AI capabilities that competitors cannot replicate.â€ Read more below on some of the reportâ€™s key findings. Building the Foundation of the Future With Open Source Open source models allow for flexibility and efficiency, enabling organizations to tailor development tools to their unique needs and make them more accurate by incorporating a financial institutionâ€™s proprietary data. As a result, 83% percent of respondents said open source is important to their organizationâ€™s AI strategy, with 43% saying it is very to extremely important. â€œOpen source models can help banks close the gap with early movers, unlock cost efficiencies and safeguard against vendor lock-in, but theyâ€™re not without their limitations â€” proprietary approaches can unlock superior performance for domain-specific tasks,â€ said Alexandra Mousavizadeh, cofounder and co-CEO of Evident Insights. â€œLeading banks need to demonstrate proficiency in both approaches â€” applying the right kind of model to the right problem, in the right context.â€ The Return on Investment of AI in Financial Services Is Clear Financial institutions have moved from piloting AI projects to deploying solutions that create business impact and scaling them across the organization. In turn, companies have begun to see significant return on investment from AI on the top and bottom lines. As stated above, 89% of survey respondents said AI has helped increase annual revenue and decrease annual costs. For many organizations, the impact has been significant, with 64% of respondents saying AI has helped increase annual revenue by more than 5% â€” including 29% who said revenue increased more than 10%. Similarly, 61% said AI had helped decrease annual costs by more than 5%, with 25% saying costs decreased more than 10%. Respondents cited a long list of AI use cases that have provided return on investment, including document processing and management, customer experience and engagement, algorithmic trading and risk management. Creating operational efficiencies is the largest improvement AI has made in financial services, according to 52% of respondents. And 48% said employee productivity was among the biggest improvements. â€œThe most tangible ROI Iâ€™m seeing is in payment operations, specifically authorization optimization and intelligent routing,â€ said Dwayne Gefferie, payments strategist at Gefferie Group. â€œAgentic AI systems can now autonomously route transactions to the most optimized payment networks, dynamically adjust retry logic based on real-time issuer signals and make routing decisions under 200-millisecond routing that traditional rule-based systems simply canâ€™t match. What makes this compelling is that every basis point improvement in authorization rates translates directly to revenue â€” thereâ€™s no ambiguity in measurement.â€ Success Leads to Increasing AI Budgets Given the shift from running proof of concepts to deploying AI-enabled applications into production, the financial services industry is looking to significantly expand AI budgets. Nearly 100% of respondents said their AI budgets would increase or stay the same in the coming year. About 41% of respondents said investment would go toward optimizing AI workflows and production, reinvesting in and improving the AI solutions that are already working. More than a third (34%) said they had an eye toward AI expansion in their organizations, with spending focused on identifying additional use cases. And 30% said that investment would focus on building or providing more access to AI infrastructure, such as on-premises installations or in the cloud. Investment will also flow to deployment and expansion of AI agents, which are advanced AI systems designed to autonomously reason, plan and execute complex tasks based on high-level goals. About 21% of respondents said AI agents have already been deployed, with another 22% saying AI agents will be deployed within the next year and beyond. â€œThe institutions winning in AI are treating their proprietary data as a strategic asset for building differentiated AI products,â€ said Yu. Download the â€œ State of AI in Financial Services: 2026 Trends â€ report for in-depth results and insights. Explore NVIDIAâ€™s AI solutions and enterprise-level AI platforms for financial services . Categories: Generative AI Tags: Agentic AI | Artificial Intelligence | Financial Services | Open Source]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 22 Jan 2026 14:00:54 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-in-financial-services-survey-2026/</guid>
    </item>
    <item>
      <title>Flight Controls Are Cleared for Takeoff on GeForce NOW</title>
      <link>https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/</link>
      <description><![CDATA[The wait is over, pilots. Flight control support â€” one of the most community-requested features for GeForce NOW â€” is live starting today, following its announcement at CES earlier this month. Virtual captains can now bring dedicated flight gear into the cloud and feel every roll, yaw and throttle change with even more precision, starting with the Thrustmaster T.Flight HOTAS One . Time is running out on our T.Flight Hotas One MSFS Edition + 1 month of @NVIDIAGFN Ultimate giveaway! â° How to enter ðŸ‘‡ 1. Follow both @TMThrustmaster and @NVIDIAGFN accounts on social 2. Repost this and share on your feed ðŸŽ 5 winners will be chosen â€“ ends 1/24. pic.twitter.com/B89mGwJoNQ â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) January 19, 2026 Remember to enter the giveaway for a chance to score a T.Flight Hotas One MSFS Edition and one month of a GeForce NOW Ultimate membership â€” follow Thrustmaster and GeForce NOW, and repost the giveaway post to enter before Saturday, Jan. 24. Also on the horizon, Delta Force from Team Jade (TiMi Studio Group) is coming soon to GeForce NOW, adding another big-name title to the cloud lineup. Get ready for all the action by streaming the four new games in this cloud this week. Pilots Wanted Members can now jump into their favorite flight and space simulation games with a full stick-and-throttle setup streamed right from the cloud. Itâ€™s all about dialing in that authentic, hands-on flying experience while keeping latency low and gameplay responsive on GeForce NOW. Full throttle in the cloud. To get right to flying, members can look for a dedicated row in the GeForce NOW app highlighting games that support flight controls, making it simple to spot great titles for putting their new setup through its paces. This initial flight control support rollout is just the beginning, with plans to keep tuning the experience and expand compatibility to more peripherals . Fire up the rig, plug in the Thrustmaster T.Flight HOTAS One and get ready to take off â€” the cloud cockpit is open and ready for flight. Your Next Mission: â€˜Delta Forceâ€™ Team Jadeâ€™s Delta Force is coming soon to the cloud, ready to drop players into high-stakes extraction with an all-out warfare mode where coordination and precision matter just as much as raw firepower. When it launches on GeForce NOW, members will be able to jump into the action instantly from almost any device, taking advantage of highâ€‘performance streaming to stay locked at smooth frame rates even in the most chaotic firefights. Get ready to squad up, Delta Force is inbound soon. On GeForce NOW, every longâ€‘range shot, helicopter insertion and tense objective push can feel crisp and responsive, whether playing on lowâ€‘powered laptops, Macs or mobile devices. The cloud makes it easy to squad up, drop in and get straight to the mission without worrying about downloads, patches or local hardware. New in the Cloud A graceful little robot having the worst day in the universe. MIO: Memories in Orbit is a neon-tinged metroidvania about a nimble little robot waking up on a massive, overgrown ark called the Vessel with nothing but fractured memories and a whole lot of trouble coming its way. Dart through low-gravity corridors, chain together elegant wall-runs, glides and grapples, and tangle with rogue machines as the Vessel itself quietly steals the spotlight â€” moody, decaying and just alive enough to keep a few secrets. In addition, members can look for the following: MIO: Memories in Orbit (New release on Steam and Xbox , available on Game Pass, Jan. 20) Bladesong (New release on Steam , Jan. 22) Rustler (New release on Epic Games Store , free starting Jan. 22) The Gold River Project (New release on Steam , Jan. 23, GeForce RTX 5080-ready) What are you planning to play this weekend? Let us know on X or in the comments below. If you could fly to any video game destination, where would it be? ðŸ›« â€” ðŸŒ©ï¸ NVIDIA GeForce NOW (@NVIDIAGFN) January 21, 2026 Categories: Gaming Tags: Cloud Gaming | GeForce NOW]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 22 Jan 2026 14:00:53 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/geforce-now-thursday-flight-controls/</guid>
    </item>
    <item>
      <title>NVIDIA Named Best Place to Work in Tech and AI by Glassdoor</title>
      <link>https://blogs.nvidia.com/blog/nvidia-life-glassdoor-2026/</link>
      <description><![CDATA[NVIDIA is the best place to work in technology and AI , according to Glassdoor, and ranks No. 3 among Glassdoorâ€™s list of top employers . The newly created Best Places to Work in Technology and AI list reflects AIâ€™s transformative power to reshape economies, society and daily life on a scale that NVIDIA founder and CEO Jensen Huang describes as â€œthe next industrial revolution.â€ AI is already revolutionizing industrial operations through automation, driving predictive analytics for business strategies and competitive analysis, and transforming how companies tackle efficiencies, product innovation and long-term planning. NVIDIA offers the full-stack AI infrastructure that has become essential for modern AI, drawing some of the worldâ€™s best developers, researchers and engineers to build on it, underpinning and expanding a uniquely integrated ecosystem. Advancing AIâ€™s Foundational Infrastructure This year, NVIDIA kickstarted the next generation of AI with the launch of the NVIDIA Rubin platform , comprising six new chips designed to deliver one powerful AI supercomputer. The platform, already in production, comes as AI computing demand for both training and inference is soaring. NVIDIA Rubin sets a new standard for building, deploying and securing the worldâ€™s largest and most advanced AI systems at the lowest cost to accelerate mainstream AI adoption. Building on its foundational infrastructure, NVIDIA is also developing leading open models â€” trained on the companyâ€™s own supercomputers â€” to enable breakthroughs in healthcare, climate science, robotics and autonomous driving. â€œNVIDIA is a frontier AI model builder, and we build it in a very special way,â€ Huang said during NVIDIA Live at CES in Las Vegas. â€œWe build it completely in the open so that we can enable every company, every industry, every country to be part of this AI revolution.â€ How Glassdoor Chooses the Best Workplaces Also ranked in Glassdoorâ€™s list of the 25 best places to work in technology and AI are ServiceNow, EPAM Systems, Google and Motorola Solutions. Crew Carwash and In-N-Out Burger led Glassdoorâ€™s list of top employers , with NVIDIA climbing one spot from last year to No. 3. Ryan, a tax services firm, and Keller Williams, a real estate company, round out the top five. To determine the company rankings, Glassdoor evaluates anonymous feedback shared by current and former employees spanning culture, career opportunities and senior management. Glassdoorâ€™s 2026 Best Places to Work were determined using company reviews from U.S. employees between Oct. 17, 2024, and Oct. 16, 2025. Follow @nvidialife on Instagram and learn more about NVIDIA life, culture and careers . Categories: NVIDIA Life]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 21 Jan 2026 16:26:37 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/nvidia-life-glassdoor-2026/</guid>
    </item>
    <item>
      <title>â€˜Largest Infrastructure Buildout in Human Historyâ€™: Jensen Huang on AIâ€™s â€˜Five-Layer Cakeâ€™ at Davos</title>
      <link>https://blogs.nvidia.com/blog/davos-wef-blackrock-ceo-larry-fink-jensen-huang/</link>
      <description><![CDATA[From skilled trades to startups, AIâ€™s rapid expansion is the beginning of the next massive computing platform shift, and for the worldâ€™s workforce, a move from tasks to purpose. At a packed mainstage session at the annual meeting of the World Economic Forum in Davos , Switzerland, NVIDIA founder and CEO Jensen Huang described artificial intelligence as the foundation of what he called â€œthe largest infrastructure buildout in human history,â€ driving job creation across the global economy. Speaking with BlackRock CEO Larry Fink, Huang framed AI not as a single technology but as a â€œa five-layer cake,â€ spanning energy, chips and computing infrastructure, cloud data centers, AI models and, ultimately, the application layer. Because every layer of AIâ€™s five-layer stack must be built and operated, Huang said the platform shift is creating jobs across the economy â€” from energy and construction to advanced manufacturing, cloud operations and application development. The application layer might focus on integrating AI into financial services, healthcare or manufacturing. â€œThis layer on top, ultimately, is where economic benefit will happen,â€ Huang said. From energy and power generation to chip manufacturing, data center construction and cloud operations, Huang said the AI buildout is already creating demand for skilled labor. He added that the largest economic benefit will come from the application layer, where AI is transforming industries such as healthcare, manufacturing and financial services â€” and changing the nature of work across the economy. Huang pointed to venture capital investment as a signal of how quickly AI is reshaping the global economy. He said 2025 was one of the largest years for VC funding on record, with most of that capital flowing to what he described as â€œAI-native companies.â€ These firms span healthcare, robotics, manufacturing and financial services â€” industries where, Huang said, â€œfor the first time, the models are good enough to build on top of.â€ That investment, Huang said, is translating directly into jobs. He highlighted demand for plumbers, electricians, construction workers, steelworkers, network technicians and teams responsible for installing and operating advanced equipment. Jobs With Purpose AI, Huang said, likely wonâ€™t destroy jobs. Instead, itâ€™s increasing demand in fields such as radiology, and helping handle administrative work in fields impacted by labor shortages â€” such as nursing. AI has become a key tool in radiology, he said, yet there are now more radiologists than ever. â€œIf you reason from first principles, not surprisingly, the number of radiologists has gone up,â€ Huang said. He explained that the purpose of a radiologistâ€™s job is to diagnose disease and help patients, while studying scans is just one task. â€œThe fact that theyâ€™re able to study scans now infinitely fast allows them to spend more time with patients,â€ he said, adding that AI enables greater interaction with patients and other clinicians. And because they can also see more patients, thereâ€™s a need for more radiologists. Huang said the same dynamic is playing out in nursing. The U.S. faces a shortage of roughly 5 million nurses, in part because nurses spend nearly half their time on charting and documentation. â€œNow they can use AI to do the charting and the transcription of patient visits,â€ he said, pointing to work being done by companies such as Abridge and its partners. As productivity improves, Huang said, outcomes improve as well. â€œHospitals do better, and they hire more nurses,â€ he said. â€œSurprisingly â€” or not surprisingly â€” AI is increasing productivity and, as a result, hospitals want to hire more people.â€ To illustrate the broader point, Huang joked that if someone simply watched him and Fink doing their jobs, â€œyou would probably think the two of us are typists.â€ Automating typing, he said, would not eliminate their jobs because that task is not their purpose. AI helps with tasks, enabling people to fulfill their purpose and become more productive, making workers more valuable. â€œSo the question is, what is the purpose of your job?â€ Huang said. NVIDIA founder and CEO Jensen Huang in conversation with Larry Fink, chair and CEO of BlackRock, at the World Economic Forum Annual Meeting 2026 in Davos, Switzerland. Image credit: World Economic Forum / Thibaut Bouvier AI as Critical Infrastructure Huang framed AI as essential national infrastructure. â€œAI is infrastructure,â€ he said, arguing that every country should treat AI like electricity or roads. â€œYou should have AI as part of your infrastructure.â€ He urged countries to build their own AI capabilities, drawing on local language and culture. â€œDevelop your AI, continue to refine it and have your national intelligence be part of your ecosystem,â€ he said. Fink asked whether only the most educated people can use or benefit from AI. Huang countered that idea, emphasizing that AIâ€™s rapid adoption stems from its accessibility. â€œAI is super easy to use â€” itâ€™s the easiest software to use in history,â€ he said, noting that in just two to three years, AI tools have reached nearly a billion people. As a result, Huang said AI literacy is becoming essential. â€œIt is very clear that it is essential to learn how to use AI â€” how to direct it, manage it, guardrail it, evaluate it,â€ he said, comparing those skills to leadership and people management. Closing Technology Divides For developing countries, Huang said AI offers a chance to narrow long-standing technology gaps. â€œAI is likely to close the technology divide,â€ he said, citing its accessibility and abundance. Turning to Europe, Huang highlighted manufacturing and industrial strength as a major advantage. â€œYou donâ€™t write AI â€” you teach AI,â€ he said, urging countries to fuse industrial capability with artificial intelligence to unlock physical AI and robotics. â€œRobotics is a once-in-a-generation opportunity,â€ he said, particularly for nations with strong industrial bases. Fink summarized the discussion by saying that what he heard suggested the world is far from an AI bubble. Instead, he posed a different question: Are we investing enough? Huang agreed, saying large investments are required because â€œwe have to build the infrastructure necessary for all of the layers of AI above it.â€ The opportunity, he said, â€œis really quite extraordinary, and everybody ought to get involved.â€ He reiterated that 2025 was the largest year for global VC investment, with more than $100 billion deployed worldwide, most of it into AI-native startups. â€œThese companies are building the application layer above,â€ Huang said, â€œand theyâ€™re going to need infrastructure â€” and investment â€” to build this future.â€ Fink added that broad participation in that growth is critical. â€œI actually believe itâ€™s going to be a great investment for pension funds around the world to be a part of that, to grow with this AI world,â€ Fink said. â€œWe need to make sure that the average pensioner and the average saver is part of that growth. If theyâ€™re just watching it from the sidelines, theyâ€™re going to feel left out.â€ Featured image courtesy of World Economic Forum. Categories: Corporate Tags: Artificial Intelligence]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 21 Jan 2026 12:50:16 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/davos-wef-blackrock-ceo-larry-fink-jensen-huang/</guid>
    </item>
    <item>
      <title>CEOs of NVIDIA and Lilly Share â€˜Blueprint for What Is Possibleâ€™ in AI and Drug Discovery</title>
      <link>https://blogs.nvidia.com/blog/jpmorgan-healthcare-nvidia-lilly/</link>
      <description><![CDATA[NVIDIA and Lilly are putting together â€œa blueprint for what is possible in the future of drug discovery,â€ NVIDIA founder and CEO Jensen Huang told attendees at a fireside chat Monday with Dave Ricks, chair and CEO of Lilly. The conversation â€” which took place during the annual J.P. Morgan Healthcare Conference in San Francisco â€” focused on the announcement of a first-of-its-kind AI co-innovation lab by NVIDIA and Lilly. â€œWeâ€™re systematically bringing together some of the brightest minds in the field of drug discovery and some of the brightest minds in computer science,â€ Huang said. â€œWeâ€™re going to have a lab where the expertise and the scale of that lab is sufficient to attract people who really want to do their lifeâ€™s work at that intersection.â€ The initiative will bring together Lillyâ€™s world-leading expertise in the pharmaceutical industry with NVIDIAâ€™s leadership in AI to tackle one of humanityâ€™s greatest challenges: modeling the complexities of biology. The two companies will jointly invest up to $1 billion in talent, infrastructure and compute over five years to support the new lab, which will be based in the San Francisco Bay Area. During the fireside chat, Ricks reflected on the painstaking work of drug discovery and AIâ€™s potential to transform the cycle of pharmaceutical invention. â€œEach small molecule discovery is like a work of art,â€ he said. â€œIf we can make that an engineering problem, versus this sort of discovery, this artisanal drug-making problem, think of the impact on human life.â€ The lab will operate under a scientist-in-the-loop framework, where agentic wet labs are tightly connected to computational dry labs in a continuous learning system. This framework aims to enable experiments, data generation and AI model development to continuously inform and improve one another. â€œMachines are made to work day and night to solve this problem,â€ Ricks said. The co-innovation lab builds on Lillyâ€™s previously announced AI supercomputer â€” the biopharma industryâ€™s most powerful AI factory , an NVIDIA DGX SuperPOD with DGX B300 systems â€” which will train large-scale biomedical foundation and frontier models for drug discovery and development. By integrating AI into drug discovery, Ricks explained, pharmaceutical researchers can rapidly simulate a massive number of possible molecules, test them at scale in silico and filter out promising candidates. The next challenge is to find more biological targets using AI. â€œThe holy grail is that you put those two things together, and we can model the whole system at once,â€ Ricks said. Huang and Ricks also discussed Lillyâ€™s long history of harnessing computing for pharmaceutical research â€” and how diseases of the aging brain are the next frontier for drug discovery. â€œI canâ€™t imagine a more worthy field to apply computer science to,â€ Huang said. â€œHopefully we can bend the arc of history.â€ NVIDIA at J.P. Morgan Healthcare NVIDIAâ€™s full-stack AI platform is accelerating the creation and deployment of leading foundation models across digital biology and drug discovery. To recognize some of the recent advancements, Huang raised a toast at J.P. Morgan Healthcare in honor of about a dozen leaders in the field â€” and the AI models theyâ€™ve pioneered. â€œIn the last 10 years, weâ€™ve advanced AI 1 million times,â€ Huang said. â€œI believe that over the next 10 years, you will enjoy the same adventure that Iâ€™ve enjoyed in our generation â€¦ and so for each one of you â€” for your happy new year present and a thank you for everything that you do for the industry and for the future of humanity â€” I give to you a DGX Spark.â€ Over a dozen leaders in AI and drug discovery received NVIDIA DGX Spark systems signed by NVIDIA founder and CEO Jensen Huang at the J.P. Morgan Healthcare Conference. The honorees included: Zach Carpenter, CEO of VantAI , developer of the Neo model family for co-folding and design across all biological molecules. Gabriele Corso, CEO of Boltz , creator of one of the most well-established open-source families of biomolecular models. Evan Feinberg, CEO of Genesis Molecular AI , which developed Pearl, a protein and small molecule structure prediction model. Chris Gibson and Najat Khan, chairman and CEO, respectively, of Recursion , which developed the OpenPhenom vision transformer model for microscopy data. Glen Gowers, CEO of Basecamp Research , creator of EDEN, a biodiversity-scale genome language model family. Brian Hie, innovation investigator at the Arc Institute , which was a major collaborator in the development of Evo 2, part of the Evo family of DNA language models. Max Jaderberg, president of Isomorphic , which is extending the capabilities of AlphaFold, the defining family of protein structure and interaction models. Simon Kohl, CEO of Latent Labs , developer of the Latent-X family of generative models for protein sequence and structure. Joshua Meier, CEO of Chai Discovery , which developed the Chai family of generative AI models for molecular structure prediction and design. Tom Miller, cofounder and CEO of Iambic Therapeutics , developer of the NeuralPLexer model family for flexible, accurate and fast structure prediction for proteins and small molecules. Alex Rives, head of science at Biohub , which created the ESM family of leading protein language models. Alex Zhavoronkov, CEO of Insilico Medicine , which built Pharma.AI , an integrated model suite spanning target discovery, generative chemistry and clinical prediction. At J.P. Morgan Healthcare, NVIDIA also announced a major expansion of the NVIDIA BioNeMo platform for AI-driven biology and drug discovery with tools including: NVIDIA Clara open models for predicting RNA structures and ensuring AI-designed drugs are practical to synthesize. BioNeMo Recipes to accelerate and scale biological foundation model training, customization and deployment. BioNeMo data processing libraries such as nvMolKit, a GPU-accelerated cheminformatics tool for molecular design. NVIDIA also highlighted a collaboration with instrumentation leader Thermo Fisher to build autonomous lab infrastructure using NVIDIAâ€™s full-stack AI computing â€” and highlighted the work of Multiply Labs , a San Francisco-based startup that offers end-to-end robotic systems to automate cell therapy manufacturing at scale. J.P. Morgan Healthcare is the worldâ€™s largest healthcare investment symposium, attracting over 8,000 global professionals including investors, policymakers and executives from across the healthcare industry. For more from the conference, listen to the audio recording and view the presentation deck of a special address by Kimberly Powell, vice president of healthcare at NVIDIA, who discusses AIâ€™s impact across healthcare. Categories: Corporate | Generative AI Tags: Artificial Intelligence | Healthcare and Life Sciences | Social Impact]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 13 Jan 2026 20:00:43 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/jpmorgan-healthcare-nvidia-lilly/</guid>
    </item>
    <item>
      <title>NVIDIA Unveils Multi-Agent Intelligent Warehouse and Catalog Enrichment AI Blueprints to Power the Retail Pipeline</title>
      <link>https://blogs.nvidia.com/blog/multi-agent-intelligent-warehouse-and-catalog-enrichment-blueprints/</link>
      <description><![CDATA[Every â€œthat was easyâ€ shopping moment is made possible by teams working to hit shipping deadlines, scrambling to fix missing product details and striving to provide curated shopping experiences. Behind the scenes, workers are dealing with the reality of aging systems, siloed data and rising customer expectations â€” a combination that makes consistency and speed harder to deliver with every new season and added stock-keeping unit (SKU). New Multi-Agent Intelligent Warehouse (MAIW) and Retail Catalog Enrichment NVIDIA Blueprints are designed to turn this dynamic system into an advantage. These open-source developer references, launched today, empower developers to customize AI-powered solutions for the retail value chain, from warehouse to wardrobes. â€œBuilding with these blueprints will reduce the cost of integration and help our customers and partners enable applications fast,â€ said Tarik Hammadou, director of developer relations for AI for retail and consumer packaged goods at NVIDIA. â€œThey unlock the efficiency and enterpriseâ€‘grade scale the retail industry needs to compete.â€ The blueprints will be showcased next week at the National Retail Federation: Retailâ€™s Big Show . Easing Warehouse Workflows Warehouses are dynamic spaces with many moving parts, from boxes carrying a variety of retail goods to massive machines and workers fulfilling thousands of daily orders. Issues can arise in an instant â€” from out-of-stock items to cleanups needed in aisle four. An ongoing issue within this workspace is a disconnect between the IT and operational technology (OT) layers. This gap bars managers from easily handling problems such as accurately measuring product inventory, efficiently pinpointing technology issues and deploying enough workers to areas that need extra help. â€œThe idea of having an agentic AI layer on the IT or OT level is not efficient, but having agents in between IT and OT allows the AI agents to act as the coordinators,â€ said Hammadou. A look inside the MAIW Blueprint. The NVIDIA MAIW blueprint delivers a synchronized AI system that sits above existing warehouse management systems, enterprise resource planning, robotics and IoT data, so teams gain real-time, explainable operational intelligence. The blueprint comprises specialized agents for equipment asset operations, operations coordination, safety compliance, forecasting and document processing â€” all orchestrated by a central warehouse operational assistant that mirrors how warehouses actually run and turns fragmented data into proactive decision-making. For example, a supervisor can ask in natural language, â€œWhy is packing slow?â€ and the assistant analyzes equipment status, tasks queues and staffing data to highlight the bottleneck, shows the supporting evidence and recommends actions such as rebalancing work or adjusting task priorities. The blueprint also provides production-grade capabilities â€” including role-based access control and guardrails to keep recommendations within policy â€” so operations teams can trust AI to help coordinate real equipment and safety-critical decisions. By targeting metrics to detect and resolve issues and safety incidents, as well as ensure on-time order fulfillment and service level agreement adherence â€” MAIW helps warehouses move from constant fire drills to more predictable, data-driven shifts. Partners such as Kinetic Vision, a product and technology development firm, can use the MAIW blueprint to innovate and tackle decades-long issues in retail supply chains. â€œChart and graphs are yesterday, we need predictions and prescribed actions,â€ said Jeremy Jarrett, CEO of Kinetic Vision. â€œThe NVIDIA MAIW blueprint would allow you to have more of a central way to answer questions and prompt decision-making.â€ Resolving Sparse Product Data The Retail Catalog Enrichment NVIDIA Blueprint can help businesses of all sizes achieve richer and accurate product onboarding, as well as deliver localized marketing. Retailers often face a â€œsparse dataâ€ problem: product images arrive with minimal or inconsistent text, and teams spend large amounts of time writing titles, descriptions and attributes, then customizing them for each market and campaign. The blueprint addresses this by using generative AI to create high-quality, structured, localized and brand-aligned product content at scale. For example, imagine a home goods retailer trying to update their online storefront with a basic set of ceramic mug photos. With an NVIDIA Nemotron vision language model (VLM), part of the Retail Catalog Enrichment Blueprint, the photos can be fed through the VLM to develop product metadata such as color, material, capacity, style and use cases. From a single image, the system can then generate localized product titles and descriptions, extract and normalize attributes for search and recommendation systems for improved SEO and GEO, and create culturally relevant 2D lifestyle imagery and interactive 3D assets. Behind the scenes, an AI â€œjudgeâ€ checks outputs for quality and consistency. In addition, the Retail Catalog Enrichment Blueprint can create rich, on-brand marketing content by applying brand voice, tone and taxonomy instructions via prompts, alongside the product image and a target locale. The blueprint uses those brand guidelines to generate enriched product titles and descriptions, localized categories and tags, and culturally appropriate lifestyle image variations tailored to that intent. Grid Dynamicsâ€™ NVIDIA Blueprint-Powered Solution Companies are already creating their own products with the help of NVIDIAâ€™s retail blueprints. Global tech consulting firm Grid Dynamics has built a catalog enrichment and management system that increases the accuracy of item content and status of SKUs for large retailers, using the Retail Catalog Enrichment NVIDIA Blueprint. â€œThe quality of the search and the quality of the browsing experience for customers directly depends on the quality of the catalog data,â€ said Ilya Katsov, chief technology officer of Grid Dynamics. â€œItâ€™s a very critical problem for all retailers with a digital presence to ensure their catalogs have as rich and consistent of attributes as possible â€” and our solution automates this so they donâ€™t need to do manual reviews.â€ For bigger retailers with massive product catalogs, attributes can be missing or incorrect. Onboarding new vendors with differing catalog structures can further jumble the data â€” leading to inaccurate sales, frustration and, eventually, a loss of customer loyalty. This is where Grid Dynamicsâ€™ solution comes into play. â€œOur solution makes product catalogs more discoverable while giving brands the ability to enforce their business rules at scale,â€ said Dan Guja, principal software engineer at Grid Dynamics. â€œWith AI-driven business rules applied across the catalog, brands can improve data quality, sharpen customer intent signals and surface products customers actually want.â€ Piecing Together the NVIDIA Retail Pipeline The MAIW and Catalog Enrichment NVIDIA Blueprints are part of a greater initiative to reimagine the warehouse-to-consumer workflow with AI infrastructure at each level. On the backend, the MAIW blueprint helps managers and warehouse workers in their daily supply and data management tasks, while the Catalog Enrichment NVIDIA Blueprint lets digital teams easily curate stylized SKU pages at the click of a button. Plus, the Nemotron-Personas-USA open-source dataset can be used in the development and training of solutions, improtving the diversity of synthetically generated data on a variety of shopper demographics. On the front end, the previously released agentic NVIDIA Retail Shopping Assistant Blueprint can make product discovery and customersâ€™ shopping experience conversational, more effortless and enjoyable by serving as a retail expert. â€œThe next step is embedding a physical AI layer into warehouse and store operations, enabling intelligent agents to see, reason, and act on real-world inventory and supply-chain challenges,â€ said Tarik Hammadou. â€œBy training physical agents with capabilities like computer vision, weâ€™re moving toward more adaptive and autonomous operations.â€ Learn more about the MAIW and Retail Catalog Enrichment blueprints on the NVIDIA Technical Blog. Categories: Generative AI Tags: Agentic AI | Nemotron | NVIDIA Blueprints | Open Source | Retail]]></description>
      <author>NVIDIA</author>
      <pubDate>Fri, 09 Jan 2026 14:00:02 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/multi-agent-intelligent-warehouse-and-catalog-enrichment-blueprints/</guid>
    </item>
    <item>
      <title>AI Copilot Keeps Berkeleyâ€™s X-Ray Particle Accelerator on Track</title>
      <link>https://blogs.nvidia.com/blog/ai-copilot-berkeley-x-ray-particle-accelerator/</link>
      <description><![CDATA[In the rolling hills of Berkeley, California, an AI agent is supporting high-stakes physics experiments at the Advanced Light Source (ALS) particle accelerator. Researchers at the Lawrence Berkeley National Laboratory ALS facility recently deployed the Accelerator Assistant, a large language model (LLM)-driven system to keep X-ray research on track. The Accelerator Assistant â€” powered by an NVIDIA H100 GPU harnessing CUDA for accelerated inference â€” taps into institutional knowledge data from the ALS support team and routes requests through Gemini, Claude or ChatGPT. It writes Python and solves problems, either autonomously or with a human in the loop. This is no small task. The ALS particle accelerator sends electrons traveling near the speed of light in a 200-yard circular path, emitting ultraviolet and X-ray light, which is directed through 40 beamlines for 1,700 scientific experiments per year. Scientists worldwide use this process to study materials science, biology, chemistry, physics and environmental science. At the ALS, beam interruptions can last minutes, hours or days, depending on the complexity, halting concurrent scientific experiments in process. And much can go wrong: the ALS control system has more than 230,000 process variables. â€œItâ€™s really important for such a machine to be up, and when we go down, there are 40 beamlines that do X-ray experiments, and they are waiting,â€ said Thorsten Hellert, staff scientist from the Accelerator Technology and Applied Physics Division at Berkeley Lab and lead author of a research paper on the groundbreaking work. Until now, facility staff troubleshooting issues have had to quickly identify the areas, retrieve data and gather the right personnel for analysis under intense time pressure to get the system back up and running. â€œThe novel approach offers a blueprint for securely and transparently applying large language model-driven systems to particle accelerators, nuclear and fusion reactor facilities, and other complex scientific infrastructures,â€ said Hellert. The research team demonstrated that the Accelerator Assistant can autonomously prepare and run a multistage physics experiment, cutting setup time and reducing efforts by 100x. Applying Context Engineering Prompts to Accelerator Assistant The ALS operators interact with the system through either a command line interface or Open WebUI, which enables interaction with various LLMs and is accessible from control room stations, as well as remotely. Under the hood, the system uses Osprey, a framework developed at Berkeley Lab to apply agent-based AI safely in complex control systems. Each user is authenticated and the framework maintains personalized context and memory across sessions, and multiple sessions can be managed simultaneously. This allows users to organize distinct tasks or experiments into separate threads. These inputs are routed through the Accelerator Assistant, which makes connections to the database of more than 230,000 process variables, a historical database archive service and Jupyter Notebook-based execution environments. â€œWe try to engineer the context of every language model call with whatever prior knowledge we have from this execution up to this point,â€ said Hellert. Inference is done either locally â€” using Ollama, which is an open-source tool for running LLMs with a personal computer, on an H100 GPU node located within the control room network â€” or externally with the CBorg gateway, which is a lab-managed interface that routes requests to external tools such as ChatGPT, Claude or Gemini. The hybrid architecture balances secure, low-latency, on-premises inference with access to the latest foundation models. Integration with EPICS (Experimental Physics and Industrial Control System) enables operator-standard safety constraints for direct interaction with accelerator hardware. EPICS is a distributed control system used in large-scale scientific facilities such as particle accelerators. Engineers can write Python code in Jupyter Notebook that can communicate with it. Basically, conversational input is turned into a clear natural language task description for objectives without redundancy. External knowledge such as personalized memory tied to users, documentation and accelerator databases are integrated to assist with terminology and context. â€œItâ€™s a large facility with a lot of specialized expertise,â€ said Hellert. â€œMuch of that knowledge is scattered across teams, so even finding something simple â€” like the address of a temperature sensor in one part of the machine â€” can take time.â€ Tapping Accelerator Assistant to Aid Engineers, Fusion Energy Development Using the Accelerator Assistant, engineers can start with a simple prompt describing their goal. Behind the scenes, the system draws on carefully prepared examples and keywords from accelerator operations to guide the LLMâ€™s reasoning. â€œEach prompt is engineered with relevant context from our facility, so the model already knows what kind of task itâ€™s dealing with,â€ said Hellert. Each agent is an expert in that field, he said. Once the task is defined, the agent brings together its specialized capabilities â€” such as finding process variables or navigating the control system â€” and can automatically generate and run Python scripts to analyze data, visualize results or interact safely with the accelerator itself. â€œThis is something that can save you serious time â€” in the paper, we say two orders of magnitude for such a prompt,â€ said Hellert. Looking ahead, Hellert aims to have the ALS engineers put together a wiki that documents the many processes that go on to support the experiments. These documents could help the agents run the facilities autonomously â€” with a human in the loop to approve the course of action. â€œOn these high-stakes scientific experiments, even if itâ€™s just a TEM microscope or something that might cost $1 million, a human in the loop can be very important,â€ said Hellert. The work has already expanded beyond ALS as part of the DOEâ€™s Genesys mission, with the framework being deployed across U.S. particle accelerator facilities. Next up, Hellert just began collaborating with engineers at the ITER fusion reactor â€” the worldâ€™s largest â€” in France for implementing the framework for use in the fusion reactor facility. He also has a collaboration in the works with the Extremely Large Telescope ELT, in northern Chile. Benefiting Humanity: Scientific Impact of Experiments Supported by ALS Beyond optimizing the accelerator and other industrial operations, the work at the ALS directly enables scientific breakthroughs with global impact. The facilityâ€™s stable X-ray beams underpin research in health, climate resilience and planetary science. During the COVID-19 pandemic, ALS researchers helped characterize a rare antibody that could neutralize SARS-CoV-2. Structural biology experiments at Beamline 4.2.2 revealed how six molecular loops of the antibody latch onto and disable the viral spike protein. The findings supported the rapid development of a therapeutic that remained effective through multiple variants. ALS science also contributes to climate-focused research. Metal-organic frameworks (MOFs) â€” a class of porous materials capable of capturing water or carbon dioxide from air â€” were extensively studied across several ALS beamlines. These experiments supported foundational work that ultimately led to the 2025 Nobel Prize in Chemistry, recognizing the transformative potential of MOFs for sustainable water harvesting and carbon management. In planetary science, ALS measurements of samples returned from NASAâ€™s OSIRIS-REx mission helped trace the chemical history of asteroid Bennu. X-ray analyses provided evidence that such asteroids carried water and molecular precursors of life to early Earth, deepening our understanding of the origins of the planetâ€™s habitable conditions. Categories: Generative AI Tags: GPU Computing]]></description>
      <author>NVIDIA</author>
      <pubDate>Thu, 08 Jan 2026 17:00:41 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-copilot-berkeley-x-ray-particle-accelerator/</guid>
    </item>
    <item>
      <title>From Warehouse to Wallet: New State of AI in Retail and CPG Survey Uncovers How AI Is Rewiring Supply Chains and Customer Experiences</title>
      <link>https://blogs.nvidia.com/blog/ai-in-retail-cpg-survey-2026/</link>
      <description><![CDATA[AI has transformed retail and consumer packaged goods (CPG) operations, enhancing customer analysis and segmentation to enable greater personalization for marketing and advertising, and boosting the speed and accuracy of demand forecasting for supply chains and logistics. Companies are also raising the bar for customer engagement through intelligent digital shopping assistants and catalog enrichment by dynamically enhancing and localizing product information. AI agents are increasing the speed and efficiency of operations, while physical AI systems are helping streamline and automate warehouse and supply chain operations. NVIDIAâ€™s third annual State of AI in Retail and Consumer Packaged Goods survey report, which garnered hundreds of responses, showed maturation of AI within the industry as companies move AI projects from pilot to production in all areas. Highlights of this yearâ€™s report include: 91% of respondents said their companies are either actively using or assessing AI. 90% said theyâ€™d build on the success of current projects by increasing their AI budgets in 2026. 89% reported AI is helping to increase annual revenue, while 95% said it is helping decrease annual costs. 79% said open-source models and software were moderately to extremely important to their AI strategy. 47% said their companies are either using or assessing agentic AI in their operations. Read more below on some of the reportâ€™s key findings. Open Source Opens Opportunities Open source has quickly become the foundation of many retail AI systems, giving teams the flexibility to adapt models to their data and use cases while maintaining strong governance. Open, interoperable ecosystems also make it easier to plug AI into existing tools and workflows, helping retailers rapidly scale innovation. â€œMost retailers first started experimenting with AI using proprietary AI vendors,â€ said Jason Goldberg, chief commerce strategy officer of Publicis Groupe. â€œThey had the models, but they didnâ€™t own the keys to their own kingdom. Open source flips that script, allowing retailers to leverage their proprietary data, avoid vendor lock-in and benefit from open-source community innovation.â€ AI Unlocks Significant Business Impact With 91% of respondents saying their companies are either actively using or assessing AI, the competitive question in retail and CPG has shifted from whether or not to invest in AI, to how to most effectively deploy and scale AI. Across the industry, the business impact of AI has been tangible and significant. When asked how AI has improved their business, 54% cited improved employee productivity; 52% said AI has helped to create operational efficiencies; and 41% reported improved customer service. As stated above, 89% of respondents said AI has helped to increase revenue. For many companies, that increase has been significant, with 30% stating revenue has increased by more than 10%. The story is the same for AIâ€™s role in helping to decrease annual costs, with 95% agreeing AI has reduced costs and 37% saying costs have been reduced by more than 10%. â€œWhat executives should be focused on is not green-lighting vanity projects at the expense of high-ROI wins,â€ said Chris Walton, co-CEO of Omni Talk. â€œThe retailers who will succeed will start with boring use cases that solve specific P&L problems, prove the value, then scale.â€ AI investment, including infrastructure, hiring AI experts and software, will increase next year, according to nine out of 10 survey respondents. And half of respondents said the increase could be significant, with budgets increasing 10% or more year over year. Agentic AI Makes Big Debut in Retail The retail and CPG industry is piloting AI agents across lines of business. Overall, 47% of survey respondents said theyâ€™re using or assessing agentic AI â€” with 20% saying AI agents are already active in their organizations and another 21% reporting agents are coming within the next year. â€œThe truly disruptive impact of agentic AI will hit retail supply chains and operations first, such as autonomous agents handling real-time inventory rebalancing, dynamic pricing and vendor negotiations at scale, because thatâ€™s where the ROI is measurable,â€ said Walton. Survey respondents identified three clear goals for agentic AI in retail and CPG: Increased process speed and efficiency, per 57% of respondents. Enhanced customer experience and personalization, per 40%. Improved decision-making with real-time data, per 40%. Broadly speaking, agentic AI will be spread across three operational lines: internal operations, employee and customer support, and customer engagement. For instance, in customer engagement, agents go beyond analytics and act on insights in real time, adjust messages, recommend products and guide purchase decisions based on individual customer contexts. AI Providing Resilience to the Supply Chain Retail and CPG have faced intense supply chain challenges this decade, and those challenges are only growing more complex. Sixty-four percent of respondents in this yearâ€™s survey reported increased challenges in the supply chain year over year, such as geopolitical instability, labor constraints, evolving consumer expectations for speed and transparency, and regulatory complexity across global operations. â€œAI lets retailers optimize inventory at the store and customer level rather than at a regional level,â€ said Goldberg. â€œAI allows retailers to incorporate many more factors in their demand forecasts, and much more accurately predict and avoid out of stocks, by much more accurately matching supply to demand.â€ The industry is turning to AI to streamline operations and solve complexity. The top pressure valve is using AI for supply chain operational efficiency and throughput, according to 51% of respondents. Meeting customer expectations was next on the list at 45%, and solving for traceability and transparency was third, per 38%. Physical AI is gaining ground in the industry, with 17% of respondents using or evaluating the technology. â€œThe real transformation will come from AI that makes existing physical infrastructure smarter,â€ said Walton. â€œMy favorite example is in-store robotics. Through them, you get better pricing, better inventory, management and better presentation quality.â€ The early movers demonstrate that, when integrated thoughtfully, physical AI systems deliver more than task automation, enhancing flexibility and throughput in response to workforce pressures and rising logistical complexity. Download the â€œ State of AI in Retail and CPG: 2026 Trends â€ report for in-depth results and insights. Explore NVIDIAâ€™s AI solutions and enterprise-level AI platforms for retail . Categories: Generative AI | Robotics Tags: Agentic AI | Artificial Intelligence | Open Source | Physical AI | Retail]]></description>
      <author>NVIDIA</author>
      <pubDate>Wed, 07 Jan 2026 14:00:06 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/ai-in-retail-cpg-survey-2026/</guid>
    </item>
    <item>
      <title>NVIDIA RTX Accelerates 4K AI Video Generation on PC With LTX-2 and ComfyUI Upgrades</title>
      <link>https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</link>
      <description><![CDATA[2025 marked a breakout year for AI development on PC. PC-class small language models (SLMs) improved accuracy by nearly 2x over 2024, dramatically closing the gap with frontier cloud-based large language models (LLMs). AI PC developer tools including Ollama, ComfyUI, llama.cpp and Unsloth have matured, their popularity has doubled year over year and the number of users downloading PC-class models grew tenfold from 2024. These developments are paving the way for generative AI to gain widespread adoption among everyday PC creators, gamers and productivity users this year. At CES this week, NVIDIA is announcing announcing a wave of AI upgrades for GeForce RTX, NVIDIA RTX PRO and NVIDIA DGX Spark devices that unlock the performance and memory needed for developers to deploy generative AI on PC, including: Up to 3x performance and 60% reduction in VRAM for video and image generative AI via PyTorch-CUDA optimizations and native NVFP4/FP8 precision support in ComfyUI . RTX Video Super Resolution integration in ComfyUI, accelerating 4K video generation. NVIDIA NVFP8 optimizations for the open weights release of Lightricksâ€™ state-of-the-art LTX-2 audio-video generation model . A new video generation pipeline for generating 4K AI video using a 3D scene in Blender to precisely control outputs. Up to 35% faster inference performance for SLMs via Ollama and llama.cpp. RTX acceleration for Nexa.ai â€™s Hyperlink new video search capability. These advancements will allow users to seamlessly run advanced video, image and language AI workflows with the privacy, security and low latency offered by local RTX AI PCs. Generate Videos 3x Faster and in 4K on RTX PCs Generative AI can make amazing videos, but online tools can be difficult to control with just prompts. And trying to generate 4K videos is near impossible, as most models are too large to fit on PC VRAM. Today, NVIDIA is introducing an RTX-powered video generation pipeline that enables artists to gain accurate control over their generations while generating videos 3x faster and upscaling them to 4K â€” only using a fraction of the VRAM. This video pipeline allows emerging artists to create a storyboard, turn it into photorealistic keyframes and then turn these keyframes into a high-quality, 4K video. The pipeline is split into three blueprints that artists can mix and match or modify to their needs: A 3D object generator that creates assets for scenes. A 3D-guided image generator that allows users to set their scene in Blender and generate photorealistic keyframes from it. A video generator that follows a userâ€™s start and end key frames to animate their video, and uses NVIDIA RTX Video technology to upscale it to 4K This pipeline is possible by the groundbreaking release of the new LTX-2 model from Lightricks, available for download today. A major milestone for local AI video creation, LTX-2 delivers results that stand toe-to-toe with leading cloud-based models while generating up to 20 seconds of 4K video with impressive visual fidelity. The model features built-in audio, multi-keyframe support and advanced conditioning capabilities enhanced with controllability low-rank adaptations â€” giving creators cinematic-level quality and control without relying on cloud dependencies. Under the hood, the pipeline is powered by ComfyUI. Over the past few months, NVIDIA has worked closely with ComfyUI to optimize performance by 40% on NVIDIA GPUs, and the latest update adds support for the NVFP4 and NVFP8 data formats. All combined, performance is 3x faster and VRAM is reduced by 60% with RTX 50 Seriesâ€™ NVFP4 format, and performance is 2x faster and VRAM is reduced by 40% with NVFP8. NVFP4 and NVFP8 checkpoints are now available for some of the top models directly in ComfyUI. These models include LTX-2 from Lightricks, FLUX.1 and FLUX.2 from Black Forest Labs, and Qwen-Image and Z-Image from Alibaba. Download them directly in ComfyUI, with additional model support coming soon. Once a video clip is generated, videos are upscaled to 4K in just seconds using the new RTX Video node in ComfyUI. This upscaler works in real time, sharpens edges and cleans up compression artifacts for a clear final image. RTX Video will be available in ComfyUI next month. To help users push beyond the limits of GPU memory, NVIDIA has collaborated with ComfyUI to improve its memory offload feature, known as weight streaming. With weight streaming enabled, ComfyUI can use system RAM when it runs out of VRAM, enabling larger models and more complex multistage node graphs on mid-range RTX GPUs. The video generation workflow will be available for download next month, with the newly released open weights of the LTX-2 Video Model and ComfyUI RTX updates available now. A New Way to Search PC Files and Videos File searching on PCs has been the same for decades. It still mostly relies on file names and spotty metadata, which makes tracking down that one document from last year way harder than it should be. Hyperlink â€” Nexa.aiâ€™s local search agent â€” turns RTX PCs into a searchable knowledge base that can answer questions in natural language with inline citations. It can scan and index documents, slides, PDFs and images, so searches can be driven by ideas and content instead of file name guesswork. All data is processed locally and stays on the userâ€™s PC for privacy and security. Plus, itâ€™s RTX-accelerated, taking 30 seconds per gigabyte to index text and image files and three seconds for a response on a RTX 5090 GPU, compared with an hour per gigabyte to index files and 90 seconds for a response on CPUs. At CES, Nexa.ai is unveiling a new beta version of Hyperlink that adds support for video content, enabling users to search through their videos for objects, actions and speech. This is ideal for users ranging from video artists looking for B-roll to gamers who want to find that time they won a battle royale match to share with their friends. For those interested in trying the Hyperlink private beta, sign up for access on this webpage . Access will roll out starting this month. Small Language Models Get 35% Faster NVIDIA has collaborated with the openâ€‘source community to deliver major performance gains for SLMs on RTX GPUs and the NVIDIA DGX Spark desktop supercomputer using Llama.cpp and Ollama. The latest changes are especially beneficial for mixture-of-experts models, including the new NVIDIA Nemotron 3 family of open models . SLM inference performance has improved by 35% and 30% for llama.cpp and Ollama, respectively, over the past four months. These updates are available now, and a quality-of-life upgrade for llama.cpp also speeds up LLM loading times. These speedups will be available in the next update of LM Studio, and will be coming soon to agentic apps like the new MSI AI Robot app. The MSI AI Robot app, which also takes advantage of the Llama.cpp optimizations, lets users control their MSI device settings and will incorporate the latest updates in an upcoming release. NVIDIA Broadcast 2.1 Brings Virtual Key Light to More PC Users The NVIDIA Broadcast app improves the quality of a userâ€™s PC microphone and webcam with AI effects, ideal for livestreaming and video conferencing. Version 2.1 updates the Virtual Key Light effect to improve performance â€” making it available to RTX 3060 desktop GPUs and higher â€” handle more lighting conditions, offer broader color temperature control and use an updated HDRi base map for a twoâ€‘keyâ€‘light style often seen in professional streams. Download the NVIDIA Broadcast update today. Transform an At-Home Creative Studio Into an AI Powerhouse With DGX Spark As new and increasingly capable AI models arrive on PC each month, developer interest in more powerful and flexible local AI setups continues to grow. DGX Spark â€” a compact AI supercomputer that fits on usersâ€™ desks and pairs seamlessly with a primary desktop or laptop â€” enables experimenting, prototyping and running advanced AI workloads alongside an existing PC. Spark is ideal for those interested in testing out LLMs or prototyping agentic workflows, or for artists who want to generate assets in parallel to their workflow so that their main PC is still available for editing. At CES, NVIDIA is unveiling major AI performance updates to Spark, delivering up to 2.6x faster performance since it launched just under three months ago. New DGX Spark playbooks are also available, including one for speculative decoding and another to fine-tune models with two DGX Spark modules. Plug in to NVIDIA AI PC on Facebook , Instagram , TikTok and X â€” and stay informed by subscribing to the RTX AI PC newsletter . Follow NVIDIA Workstation on LinkedIn and X . See notice regarding software product information. Categories: Generative AI Tags: Artificial Intelligence | CES 2026 | Creators | GeForce RTX | NVIDIA RTX | Rendering | RTX AI Garage]]></description>
      <author>NVIDIA</author>
      <pubDate>Tue, 06 Jan 2026 05:30:18 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/rtx-ai-garage-ces-2026-open-models-video-generation/</guid>
    </item>
    <item>
      <title>NVIDIA Unveils New Open Models, Data and Tools to Advance AI Across Every Industry</title>
      <link>https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</link>
      <description><![CDATA[Expanding the open model universe, NVIDIA today released new open models, data and tools to advance AI across every industry. These models â€” spanning the NVIDIA Nemotron family for agentic AI, the NVIDIA Cosmos platform for physical AI , the new NVIDIA Alpamayo family for autonomous vehicle development, NVIDIA Isaac GR00T for robotics and NVIDIA Clara for biomedical â€” will empower companies with the tools to develop real-world AI systems. NVIDIA contributes open-source training frameworks and one of the worldâ€™s largest collections of open multimodal data, including 10 trillion language training tokens, 500,000 robotics trajectories, 455,000 protein structures and 100 terabytes of vehicle sensor data. This is an unprecedented scale of diverse open resources to accelerate innovation in language, robots, scientific research and autonomous vehicles. Leading technology companies â€” including Bosch, CodeRabbit, CrowdStrike, Cohesity, Fortinet, Franka Robotics, Humanoid, Palantir, Salesforce, ServiceNow, Hitachi and Uber â€” are adopting and building on NVIDIAâ€™s open model technologies. NVIDIA Nemotron Brings Speech, Multimodal Intelligence and Safety to AI Agents Building on the recently released NVIDIA Nemotron 3 family of open models and data, NVIDIA is releasing Nemotron models for speech, multimodal retrieval-augmented generation (RAG) and safety. Nemotron Speech comprises leaderboard-topping open models, including a new ASR model , that deliver real-time, low-latency speech recognition for live captions and speech AI applications. Daily and Modal benchmarks show that the model delivers 10x faster performance than other models in its class. Nemotron RAG comprises new embed and rerank vision language models (VLMs) that provide highly accurate multilingual and multimodal data insights to enhance document search and information retrieval. Nemotron Safety models, which strengthen the safety and trustworthiness of AI applications, now include the Llama Nemotron Content Safety model, featuring expanded language support, and Nemotron PII , which detects sensitive data with high accuracy. Bosch is adopting Nemotron Speech to enable drivers to interact with their vehicles. ServiceNow trains its Apriel model family on open datasets, including Nemotron for cost-efficient multimodal performance. Cadence and IBM are piloting NVIDIA Nemotron RAG models to improve search and reasoning across complex technical documents. CrowdStrike, Cohesity and Fortinet are adopting NVIDIA Nemotron Safety models to strengthen the trustworthiness of their AI applications. Palantir is integrating Nemotron models into its Ontology framework to build a first-of-its-kind, integrated technology stack for specialized AI agents. CodeRabbit is using Nemotron models to power and scale its AI code reviews, improving speed and cost efficiency while maintaining high review accuracy. NVIDIA is also releasing open-source datasets, training resources and blueprints to developers, including the dataset and training code for the Llama Embed Nemotron 8B model, featured on the MMTEB leaderboard . This is in addition to the updated LLM Router that shows developers how to automatically direct AI requests to the best model for the job, and the dataset used to build the new Nemotron Speech ASR model. New Models for Every Type of Physical AI and Robot Developing physical AI for robots and autonomous systems requires large, diverse datasets and models that can perceive, reason and act in complex, real-world environments. On Hugging Face, robotics is the fastest-growing segment, with NVIDIAâ€™s open robotics models and datasets leading the platformâ€™s downloads . NVIDIA is releasing NVIDIA Cosmos open world foundation models that bring humanlike reasoning and world generation to accelerate physical AI development and validation. Cosmos Reason 2 is a new, leaderboard-topping reasoning VLM that helps robots and AI agents see, understand and interact with higher accuracy in the physical world. Cosmos Transfer 2.5 and Cosmos Predict 2.5 are leading models that generate large-scale synthetic videos across diverse environments and conditions. NVIDIA has also released open models and blueprints for each physical AI embodiment, built on Cosmos: Isaac GR00T N1.6 is an open reasoning vision language action (VLA) model, purpose-built for humanoid robots, that unlocks full body control and uses NVIDIA Cosmos Reason for better reasoning and contextual understanding. The NVIDIA Blueprint for video search and summarization , part of the NVIDIA Metropolis platform, is a reference workflow for building vision AI agents that can analyze large volumes of recorded and live video to improve operational efficiency and public safety. Salesforce , Milestone , Hitachi, Uber, VAST Data and Encord are using Cosmos Reason for traffic and workplace productivity AI agents. Franka Robotics, Humanoid and NEURA Robotics are using Isaac GR00T to simulate, train and validate new behaviors for robots before scaling to production. NVIDIA Alpamayo for Reasoning-Based Autonomous Vehicles Developing safe, scalable autonomous driving depends on AI that can perceive, reason and act in complex real-world environments and scenarios, with development workflows that support rapid training, testing and improvement at scale. NVIDIA is releasing NVIDIA Alpamayo, a new family of open models, simulation tools and large datasets to advance reasoning-based autonomous vehicle development. It includes: Alpamayo 1 , the first open, large-scale reasoning VLA model for autonomous vehicles (AVs) that enables vehicles to understand their surroundings, as well as explain their actions.â€‹ AlpaSim , an open-source simulation framework that enables closed-loop training and evaluation of reasoning-based AV models across diverse environments and edge cases. NVIDIA is also releasing Physical AI Open Datasets , including over 1,700 hours of driving data collected across the widest range of geographies and conditions, covering rare and complex real-world edge cases essential for advancing reasoning architectures. NVIDIA Clara for Healthcare and Life Sciences To lower costs and deliver treatments faster, NVIDIA is launching new Clara AI models that bridge the gap between digital discovery and real-world medicine. Helping researchers design treatments that are safer, more effective and easier to produce, these models include: La-Proteina enables the design of large, atom-level-precise proteins for research and drug candidate development, giving scientists new tools to study diseases previously considered untreatable. ReaSyn v2 ensures AI-designed drugs are practical to synthesize by incorporating a manufacturing blueprint into the discovery process. KERMT provides high-accuracy, computational safety testing early in development by predicting how a potential drug will interact with the human body. RNAPro unlocks the potential of personalized medicine by predicting the complex 3D shapes of RNA molecules. In addition, an NVIDIA dataset of 455,000 synthetic protein structures helps AI researchers build more accurate AI models. Get Started With NVIDIA Open Models and Technologies NVIDIA open models, data and frameworks are now available on GitHub and Hugging Face and from a range of cloud, inference and AI infrastructure platforms, as well as build.nvidia.com , giving developers flexible access to supporting resources. Many of these models are also available as NVIDIA NIM microservices for secure, scalable deployment on any NVIDIA-accelerated infrastructure, from the edge to the cloud. Learn more by watching NVIDIA Live at CES . Categories: Driving | Generative AI | Robotics Tags: Agentic AI | CES 2026 | Cosmos | Healthcare and Life Sciences | Nemotron | NVIDIA Clara | NVIDIA NIM | Open Source | Physical AI]]></description>
      <author>NVIDIA</author>
      <pubDate>Mon, 05 Jan 2026 21:50:50 GMT</pubDate>
      <guid isPermaLink="true">https://blogs.nvidia.com/blog/open-models-data-tools-accelerate-ai/</guid>
    </item>
  </channel>
</rss>
