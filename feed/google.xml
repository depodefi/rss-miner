<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 28 Jan 2026 09:47:00 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Google AI Plus is now available everywhere our AI plans are available, including the U.S.</title>
      <link>https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/</link>
      <description><![CDATA[Today, we’re launching Google AI Plus in 35 new countries and territories , including the U.S., making it available everywhere Google AI plans are available and helping people do more with Google AI for less. Google AI Plus opens up access to powerful AI models and tools to level up your productivity and creativity, all at an accessible price. This includes Gemini 3 Pro and Nano Banana Pro in the Gemini app, AI filmmaking tools in Flow, research and writing assistance in NotebookLM and more. It also includes 200GB of storage, and you can share all of the benefits with up to five other family members. Existing Google One Premium 2TB subscribers in these countries will also automatically get access to all the benefits of Google AI Plus in the next few days. In the U.S., Google AI Plus is available for $7.99/month 1 . For a limited time, new subscribers can get 50% off for the first 2 months of their subscription.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 27 Jan 2026 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/google-one/google-ai-plus-availability/</guid>
    </item>
    <item>
      <title>Just ask anything: a seamless new Search experience</title>
      <link>https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X People come to Search for an incredibly wide range of questions — sometimes to find information quickly, like a sports score or the weather, where a simple result is all you need. But for complex questions or tasks where you need to explore a topic deeply, you should be able to seamlessly tap into a powerful conversational AI experience. Today we’re rolling out two upgrades that bring us closer to this vision for Search: the ability to ask whatever’s on your mind — no matter how long or complex — and find exactly what you need. First, we’re making Gemini 3 the new default model for AI Overviews globally, so you get a best-in-class AI response right on the search results page, for questions where it’s helpful. Second, we’re making the transition to a conversation even more seamless. Now, you can easily ask a follow-up question right from an AI Overview, and jump into a conversational back and forth with AI Mode. In our testing, we’ve found that people prefer an experience that flows naturally into a conversation — and that asking follow-up questions while keeping the context from AI Overviews makes Search more helpful. It’s one fluid experience with prominent links to continue exploring: a quick snapshot when you need it, and deeper conversation when you want it. So next time you have a question, find your nearest Google search bar, and just ask anything. You can now jump into AI Mode conversations directly from AI Overviews on mobile, globally. POSTED IN: Search AI Gemini models]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 27 Jan 2026 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/search/ai-mode-ai-overviews-updates/</guid>
    </item>
    <item>
      <title>In our latest podcast, hear how the “Smokejumpers” team brings Gemini to billions of people.</title>
      <link>https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/</link>
      <description><![CDATA[The latest episode of the Google AI: Release Notes podcast focuses on how the Gemini team built one of the world’s leading AI coding models. Host Logan Kilpatrick chats with Connie Fan and Danny Tarlow, the product and research leads for Gemini’s coding capabilities. They discuss the early goals that shaped Gemini’s approach to code, the rise of "vibe coding" and the future of programming languages in the age of AI. Hear the full conversation below, or listen to the Google AI: Release Notes podcast on Apple Podcasts or Spotify .]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 27 Jan 2026 10:28:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/gemini/release-notes-podcast-smokejumpers/</guid>
    </item>
    <item>
      <title>How animators and AI researchers made ‘Dear Upstairs Neighbors’</title>
      <link>https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, our animated short film, “Dear Upstairs Neighbors,” previews at the Sundance Film Festival. The film will be showcased at the Sundance Institute’s Story Forum, a space focused on artist-first tools and technologies supporting visual storytelling. “Dear Upstairs Neighbors” is the story of a young woman, Ada, who is desperate for a good night’s sleep but kept awake by her exceedingly noisy neighbors. As she struggles to imagine what could be causing the cacophony upstairs, reality drifts into fantasy, and an epic battle for peace and sanity ensues. The film is a collaboration between animation veterans , including director and Pixar alum Connie He, and researchers at Google DeepMind, united by a shared goal of exploring how generative tools might fit in with artists' creative processes. Director Connie He developed the story based on her personal experience with noisy neighbors. In her storyboards she envisioned a series of hallucinations that get more unhinged and ridiculous as the night progresses. For our main character, Ada, production designer Yingzong Xin created a design that’s quirky and unique, with pushed proportions and an angular shape language. Ada’s face is extremely expressive. Character model sheet by Yingzong Xin. Ada’s bedroom is rendered in cool colors, conveying a sense of calm, comfort and sanctuary. Set design by Yingzong Xin. Ada’s hallucinations have a rough style and neon palette that distinguishes them from the “real world” of her bedroom. Concept art by Yingzong Xin. The painterly style changes from moment to moment, expressing Ada’s changing emotions through color and texture. Concept art by Yingzong Xin. In the most intense moments, the abstract expressionist style grows to dominate the entire scene. Concept art by Yingzong Xin. From the start, the team aspired to empower animation artists to benefit from the creative potential of generative AI without sacrificing artistic control to its inherent unpredictability. To define her vision for this film, Connie developed the storyboards, and enlisted award-winning production designer Yingzong Xin to create concept art and character designs. We committed to staying faithful to this artistic vision throughout shot production. The expressionistic visual styles are central to the storytelling — and extremely difficult to achieve in traditional animation. We expected that AI could help fill the gap, but soon found that these styles were so unique, and our design choices so specific, that our researchers would have to develop new capabilities to provide the customization and control that we needed to bring the film to life. Tune for new visual styles Our first challenge was to produce shots consistent with Ada’s character design and the painterly styles that defined each scene. To achieve high quality and consistency, our researchers built tools that allowed our artists to fine-tune custom Veo and Imagen models on their artwork, teaching the models new visual concepts from just a few example images. Images of Ada generated by Imagen after fine-tuning. The fine-tuned model helped the whole team explore Ada as a character. Left: paintings by Yingzong Xin. Right: stylized animated video generated by Veo after fine-tuning. What Veo learned from our concept art surprised us: not just superficial details like color and texture, but deep artistic concepts like two-point perspective. Top: Ada’s character design follows strictly two-dimensional rules: her characteristic hair poof and messy bun must always be part of her silhouette, never obscuring her face. Bottom left: a 3D sculpture of Ada’s hair can’t possibly look correct from every angle, because the solid form violates those 2D rules. Bottom right: Veo, after fine-tuning on images of Ada, seamlessly resolves the conflict, smoothly adapting the shapes to keep the silhouette correct as the head turns. Show, don’t type Another challenge was precisely controlling the content and motion of each shot. We knew that text prompting alone would never let us control the rhythm of Ada’s sleepy fingers typing, the comedic timing of her facial expressions, or the exact framing of a camera reveal. We needed a way to communicate that level of nuance and specificity to our AI models. Our researchers drew inspiration from how our animators communicate visually, by drawing, painting or acting out scenes. We developed novel video-to-video workflows, which allowed our animators to convey their intentions visually by creating rough animation in their tool of choice. Our models then transformed that animation into fully stylized videos that follow the input motion, with an adjustable balance between tight control and creative improvisation. Using text-to-video with the fine-tuned Veo model produced scenes that looked like Ada, but their movement was random, uncontrolled, and often bizarre. Text alone can’t convey the nuance and specificity needed for narrative animated filmmaking. To create a nuanced performance strong enough to carry the story, our animators used traditional methods. Animator Ben Knight created rough 3D animation for this scene in Maya, and researcher Andy Coenen used fine-tuned Veo models to transform it into the final look. The video-to-video approach allowed each artist to work in their comfort zone, using their favorite animation tools. Animator Mattias Breitholtz created this rough 2D animation using TV Paint, and researcher Forrester Cole transformed it into the final look frame by frame, using fine-tuned versions of Imagen in a custom ComfyUI workflow. Animator Steven Chao animated Ada and created dynamic low-poly effects in Maya, and researcher Ellen Jiang and director Connie He used fine-tuned Veo and Imagen models to transform these elements into the expressionist look. The staccato rhythm of the changing paint texture adds to the intensity of the action. Iterate toward perfection Even with the control provided by fine-tuning and video-to-video workflows, none of our final shots were created in a single “one-click” generation. Just as in any film production, we critiqued each shot in our “dailies” reviews, going through several rounds of feedback to get every detail right. To iterate on a shot without re-generating from scratch every time, we built tools for localized refinement, allowing us to edit specific regions of a video with an adjustable level of control. To create Ada’s hallucination of a howling dog, we started with a concept painting by Yingzong Xin, and used Veo image-to-video to bring it to life. Veo’s first pass (without fine-tuning) was too photorealistic for our film; so we used the fine-tuned version of Veo to bring the shot closer to our intended visual style. The video-to-video workflow allowed us to switch freely between Veo and traditional tools like Premiere. Using fine-tuned Veo with video-to-video workflows allowed us to iterate on the design of both the dog and the painterly effects around it, exploring stylistic variations with unprecedented freedom and control. Supervising animator Cassidy Curtis created rough 3D animation for this shot in Maya, and researcher Erika Lu fine-tuned a Veo model to transform it into the final look. To improve the silhouette of Ada’s hair, Lu added a rough mask to indicate the region where more hair was needed, and used Veo to improvise an extra tuft of hair there that fits seamlessly into the rest of the shot. Finally, to prepare our film for the big screen, we used Veo's upscaling capability to bring our final shots to 4K resolution. Guided by our artists' critique, our researchers carefully tuned the model's behavior to add rich detail that preserved every nuance of the artistic style. The Veo 4K upscaling model is available in Flow and coming to Google AI Studio and Vertex AI later this month to meet the real-world needs of filmmakers. Each shot presented unique challenges, and over the course of production, our multi-disciplinary team developed several workflows combining the precise control of hand-crafted animation with the stylistic flexibility and scalability of generative AI. Not only did our AI models produce hilarious bloopers, they often surprised us with unexpectedly beautiful and creative solutions. We learned valuable lessons from coming together every day to produce each shot with fine-grained artistic intention and care. Our artists found new creative powers through direct access to experimental research, and used their craft and perspective to help shape its development. Our researchers gained hands-on experience as technical artists, rapidly prototyping solutions to break through artistic and technological barriers. We’re excited to continue our mission to build generative AI with and for professional artists and filmmakers. POSTED IN: Google DeepMind AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 26 Jan 2026 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors/</guid>
    </item>
    <item>
      <title>Personal Intelligence in AI Mode in Search: Help that's uniquely yours</title>
      <link>https://blog.google/products-and-platforms/products/search/personal-intelligence-ai-mode-search/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X With Google Search, you should be able to ask any question and find precisely what you need. Accessing the world’s information is the foundation, but the most helpful search experience brings together that global knowledge with insights that are uniquely relevant to you . To make this vision possible, today we’re expanding Personal Intelligence to AI Mode in Google Search. Personal Intelligence transforms Search into an experience that feels uniquely yours by connecting the dots across your Google apps. Starting today, Google AI Pro and AI Ultra subscribers can opt-in to securely connect Gmail and Google Photos to AI Mode. With this new experience, you can tap into your own personal context and insights to unlock even more helpful Search responses that are tailored to you. Get a more tailored Search, just for you With Personal Intelligence, recommendations don’t just match your interests — they fit seamlessly into your life. You don’t have to constantly explain your preferences or existing plans, it selects recommendations just for you, right from the start. In testing Personal Intelligence with Gmail and Google Photos enabled, I’ve found new things that I wouldn’t have ever considered or discovered before. One recent example: I was looking for a new pair of sneakers, and AI Mode noticed a brand I’d just bought and suggested a new style I hadn’t seen yet. The recommendation was spot on — I bought them instantly! Here are a few more examples where Personal Intelligence in Search can be helpful for you: Say you’re looking for things to do and places to eat that the whole family would enjoy ahead of your upcoming getaway. With Personal Intelligence, AI Mode can reference your hotel booking in Gmail and travel memories in Google Photos, to suggest an itinerary with something for everyone. You’ll see tailored recommendations like an interactive museum perfect for the kids or an old-timey ice cream parlor, because it recalls the many ice-cream selfies captured in your pictures. It’s not just a generic list of restaurants and activities; it’s a personalized starting point for your next great weekend. Personal Intelligence can also be particularly helpful for shopping, because AI Mode considers the types of items you buy and where you shop. If you need a new coat for your upcoming trip, AI Mode could automatically take into account the brands you prefer, as well as your flight confirmation in Gmail to identify the destination and timing (Chicago in March). You’ll get suggestions for windproof, versatile coats that fit the weather and your preferred look. It’s like a personal shopper who already knows your itinerary and the vibe you’re going for. You can even ask fun questions that you never imagined searching for — like “if my life were a movie, what would the title and movie genre be,” or “describe my perfect day.” You’re always in control We’ve designed Personal Intelligence with transparency, choice and control at its core. Connecting your Gmail and Google Photos is strictly opt-in, meaning you choose if and when you want to connect these apps to Search and you can always turn those connections on or off . Built with privacy in mind, AI Mode uses our most intelligent model, Gemini 3, and doesn’t train directly on your Gmail inbox or Google Photos library. Training is contained to limited info, like specific prompts in AI Mode and the model’s responses, to improve functionality over time. Learn more here . In our internal testing, we know that Personal Intelligence in Search can be incredibly helpful, but mistakes can happen. Our systems might incorrectly make connections between unrelated topics or not fully understand the context. If a recommendation feels a bit off, you can correct it and clarify what you were looking for with a follow-up response in AI Mode. You can also provide feedback by giving the response a “thumbs down.” Learn more about our approach and how we’re addressing limitations here . The ability to connect AI Mode to your Gmail and Google Photos is rolling out as a Labs feature. Eligible Google AI Pro and Ultra subscribers in English in the U.S. will automatically have access to the feature as it becomes available. This experimental feature is available for personal Google accounts and not for Workspace business, enterprise or education users. As this rolls out over the next few days, AI Pro and Ultra subscribers should see an invitation to try this experience out in AI Mode, but if not, you can also turn it on in your settings, simply: Open Search and tap your profile Click on Search personalization Select Connected Content Apps Connect Workspace and Google Photos We’re looking forward to hearing how you make Search your own. POSTED IN: Search AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 22 Jan 2026 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/search/personal-intelligence-ai-mode-search/</guid>
    </item>
    <item>
      <title>Building a community-led future for AI in film with Sundance Institute</title>
      <link>https://blog.google/company-news/outreach-and-initiatives/google-org/sundance-institute-ai-education/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X The future of filmmaking is being defined by a new kind of creative partnership: artists experimenting with AI, iterating, and bringing ambitious visions to life. For the past year, Google has worked side-by-side with filmmakers to explore ways to advance and enhance artistic processes with generative AI. But we’ve also heard a clear message: for AI to truly empower, it must be community-driven leadership and supported by accessible education. Currently, the majority of media companies feel overwhelmed by the pace of AI change and only 25% of media companies are currently investing in training. To help, Google.org is providing $2 million in funding to the Sundance Institute to build a community-led ecosystem for AI education and empowerment. The funding will help bridge the AI skills gap by training over 100,000 artists in foundational AI skills and democratize access to AI learning for filmmakers. This effort is a part of Google.org’s AI Opportunity Fund , an initiative that helps Americans develop essential AI skills by funding best-in-class workforce development and education organizations across critical segments of society. A trusted ecosystem for AI filmmaking education By supporting Sundance Institute's AI training efforts, Google.org is enabling a community-led ecosystem that focuses on: Building the storytelling hubs of the future: Sundance Institute will establish an AI Literacy Alliance initiative in collaboration with The Gotham and Film Independent . This initiative will empower artist communities by providing training and support of the establishment of values and ethics that protect human creativity, artists and the creative industry at-large. Turning big ideas into technical skills: Sundance Institute and alliance partners will develop a free online curriculum to help bridge the gap between creative curiosity and effective technical use. This will include scholarships for Google courses like AI Essentials. Advancing artist learning and developing standards: Sundance Institute will launch an AI Creators Fellowship for technical experimentation and host community conversations to develop shared case studies, reports and industry-led standards. A year of collaborative innovation For the past year, we’ve invited filmmakers to our labs to co-create new tools and inform the technical requirements of our generative models based on the practical, rigorous standards of the filmmaking craft. Today’s announcement from Google.org is the natural evolution of our "collaboration-first" approach, building on recent initiatives such as: Flow: Built with creatives, for creatives: We gave storytellers early access to Flow , our AI filmmaking tool. Their hands-on feedback helped us shape an interface that, today, serves as a place where artists can explore and iterate on cinematic ideas. Through our program Flow Sessions , we continue to work closely with a group of creatives — providing them with mentorship, AI education and unlimited access to the tool as they work to create short films of all types. AI on Screen : In partnership with Range Media Partners, we launched this short film program to explore our evolving relationship with technology through the creation of films about AI, not made with AI. The program’s first film, Sweetwater , examines the poignant concept of digitally preserving a loved one’s memory. Primordial Soup: Our partnership with Darren Aronofsky’s Primordial Soup on the film Ancestra forced our generative models to solve real-world production hurdles. To meet director Eliza McNitt’s vision, we developed advanced capabilities like personalized video for character consistency and motion matching to replicate complex 3D camera paths. Today’s independent filmmakers are at the center of a foundational shift. But tools do nothing on their own; it is human imagination that gives them purpose. The goal is no longer just to learn a new tool, but to realize the creative potential that AI unlocks for their specific vision. Across our ongoing collaborations and investment in community-led education, we are committed to ensuring that the future of film remains firmly in the hands of the storytellers. We’re now heading to Park City, if you’re attending, come check out our deep dive session into Flow or the Sundance Institute Story Forum where we’ll preview “Dear Upstairs Neighbors,” demonstrating how custom Google DeepMind models help artists transform hand-crafted art into "living paintings" while maintaining total creative control. POSTED IN: Google.org AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 20 Jan 2026 20:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/outreach-and-initiatives/google-org/sundance-institute-ai-education/</guid>
    </item>
    <item>
      <title>How Nano Banana got its name</title>
      <link>https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X You already know it for its viral editing power . But how did one of Google DeepMind’s most popular models end up with the name Nano Banana ? The answer lies in a late-night scramble, a pair of personal nicknames and embracing the unexpected. With Nano Banana Pro, the newest version of our latest image generation and editing model, you can prompt Gemini with text, images or a combination of both to create, edit and iterate on even better visuals than before — including with accurate, legible text. Back in late July, the team was hard at work preparing the first version of the model for launch, squashing pesky bugs and running evaluations. They’d already locked in the technical name — Gemini 2.5 Flash Image — but one critical detail was still missing: a public codename for LMArena. LMArena is a public platform designed to evaluate AI models through anonymous, crowd-sourced pairwise comparisons. Users submit a prompt and get responses from two unidentified models. They vote for the better response, and then the platform reveals which models were used. While LMArena showcases released models, it’s also a powerful testing ground. Teams often submit models still in development to gather early performance signals and real-world, human feedback. Because these models are still being refined, using a codename is crucial. “We pushed the codename conversation until the last minute,” Product Manager Naina Raisinghani says. “So at 2:30 a.m., one of the PMs messaged me saying we needed to submit it, and I said, ‘OK, how about something funny like ‘Nano Banana’?’ And they're like, ‘Yeah, sure. That's completely nonsensical.’” The reason that idea came to Naina? It’s a variation of her own nickname. “Some of my friends call me Naina Banana, and others call me Nano because I’m short and I like computers. So I just smushed my two nicknames together,” Naina says. “And it fit because it was a Flash model.” The team introduced Nano Banana on LMArena in early August — and the model was ripe for virality. Users were stunned by its powerful editing capabilities, like its ability to maintain a person’s likeness and expertly edit multiple images together. Then, they saw the name. And social media went bananas. “People responded really well. They were so impressed with it, and then they found the name funny, and that kind of grew discourse,” Naina says. After a few weeks of speculation, the team teased Google was behind the Nano Banana model with posts on X. It was an early sign that they had a hit on their hands. And when the model officially launched , it didn’t slip, Nano Banana became the top-rated image editing model in the world. People everywhere found creative ways to use it to try on different looks, remix and restore photos, make specific edits, create custom apps and countless more use cases. “One reason we were successful is the model was available everywhere from day one — it didn’t matter what country you were in, or whether you were a developer or a consumer, you had it on the same day,” Naina says. “And then culturally relevant prompts went viral everywhere, like the popular figurine trend, which started in Thailand, or the saree trend in India.” While the technical name was still officially called Gemini 2.5 Flash Image, the Nano Banana brand name stuck. The team ran with it, turning the run button for Nano Banana in AI Studio yellow, adding a banana emoji to the “Create image” chip in the Gemini app and even creating some limited edition banana-themed swag. And now that Gemini 3 Pro Image is here, its brand name also got an upgrade: Nano Banana Pro. “We leaned into the silliness of it all. We've embraced the banana emoji as one of us. The team is split on the banana puns of it all,” Naina says. “But we're glad people find the model appealing.” POSTED IN: Gemini AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 15 Jan 2026 16:06:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/gemini/how-nano-banana-got-its-name/</guid>
    </item>
    <item>
      <title>Learners and educators are AI’s new “super users”</title>
      <link>https://blog.google/products-and-platforms/products/education/our-life-with-ai-2025/</link>
      <description><![CDATA[For the past three years, we’ve conducted the “Our Life with AI” study with Ipsos to measure how people around the world use AI. Our most recent survey found something fascinating: for the first time, the top motivation for using AI is learning. The survey, conducted late last year across 21 countries and 21,000 participants, reveals two firsts: that the majority of people in nearly every country surveyed say they use AI chatbots and, critically, that their motivation for using AI changed from curiosity to core utility. Before this year, the top motivation for using AI was entertainment, but now 74% of users report using it to “learn something new” or “understand a complex topic.” People are moving from experimentation to real-world learning, and view AI for education with a different perspective. Here’s a look at what our survey found. Students, teachers and parents are using AI 85% of students 18+ are using AI. Students use it to help with schoolwork (83%), understand complex topics (78%), manage day-to-day life tasks like trips, meals or workouts (54%) and make decisions (42%). 81% of teachers report using AI, far surpassing the global average (66% of the global public reports using an AI tool). Some of educators’ top uses for AI are learning something new or understanding a complex topic (77% of teacher users) and saving time (75%). We observed the time-saving potential of AI first-hand during a six-month pilot program in Northern Ireland where teachers reported saving an average of 10 hours per week with the help of Gemini. 76% of parents say they use AI, especially to learn something new (77%) or to assist with work (73%). Nearly half (49%) of parents report using AI to explore changing careers, making more money or starting a new business. Most importantly, rather than fearing cognitive decline, teachers, students (18+) and parents believe AI is having a positive impact on how we learn. A majority of teachers also believe AI will improve teaching quality (67%) and student outcomes (63%). Outside of the United States, Canada and Europe, the global public echoes student and teachers’ positive attitudes about AI for education: In emerging markets, people believe AI is more likely to improve student outcomes by supporting personalized learning (63%) than worsen student outcomes by eroding critical thinking (37%). In South Korea, Japan and Singapore — where students scored strong 500+ PISA scores (a globally recognized benchmark for 15-year-olds' real-world skills in reading, math and science) — attitudes are equally positive about AI’s role in education (63% improve, 37% worsen). Google’s tools are transforming learning using AI AI is changing how we learn, but, as the survey shows, the situation is complex. With increasing usage, it’s more important than ever that we continue to build AI tools with appropriate guardrails . Another study in Europe similarly found that students are seeking guidance on how to use these tools – and it’s up to all of us to ensure we match that need with appropriate safeguards for younger users. Simultaneously, we are mindful of the "5% problem": the risk that these benefits only reach the most privileged or those most motivated to learn. According to the survey, the public expects tech companies and governments to work together to ensure these tools serve the public interest. Our " North Star " for AI in education is to improve learning outcomes for everyone and help teachers focus on teaching, so they can maintain the human connection at the heart of learning. Our tools like Gemini’s Guided Learning Mode , Gemini for Education , Google AI Pro for Education and NotebookLM are helping transform learning by supporting personalized instruction and easing administrative burdens. By grounding our products in learning science and deep partnership with educators, students and experts, we’re committed to innovating responsibly so that AI helps every person reach their full potential. POSTED IN: Learning & Education AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 15 Jan 2026 11:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/products/education/our-life-with-ai-2025/</guid>
    </item>
    <item>
      <title>Introducing Community Benchmarks on Kaggle</title>
      <link>https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, Kaggle is launching Community Benchmarks , which lets the global AI community design, run and share their own custom benchmarks for evaluating AI models. This is the next step after we launched Kaggle Benchmarks last year, to provide trustworthy and transparent access to evaluations from top-tier research groups like Meta’s MultiLoKo and Google’s FACTS suite . Why community-driven evaluation matters AI capabilities have evolved so rapidly that it’s become difficult to evaluate model performance. Not long ago, a single accuracy score on a static dataset was enough to determine model quality. But today, as LLMs evolve into reasoning agents that collaborate, write code and use tools, those static metrics and simple evaluations are no longer sufficient. Kaggle Community Benchmarks provide developers with a transparent way to validate their specific use cases and bridge the gap between experimental code and production-ready applications. These real-world use cases demand a more flexible and transparent evaluation framework. Kaggle’s Community Benchmarks provide a more dynamic, rigorous and continuously evolving approach to AI model evaluation — one shaped by the users building and deploying these systems everyday. How to build your own benchmarks on Kaggle Benchmarks start with building tasks, which can range from evaluating multi-step reasoning and code generation to testing tool use or image recognition. Once you have tasks, you can add them to a benchmark to evaluate and rank selected models by how they perform across the tasks in the benchmark. Here’s how you can get started: Create a task: Tasks test an AI model’s performance on a specific problem. They allow you to run reproducible tests across different models to compare their accuracy and capabilities. Create a benchmark: Once you have created one or more tasks, you can group them into a Benchmark. A benchmark allows you to run tasks across a suite of leading AI models and generate a leaderboard to track and compare their performance. Once you build your benchmark, here’s what benefits you’ll see: Broad model access: Free access (within quota limits) to state-of-the-art models from labs like Google, Anthropic, DeepSeek and more. Reproducibility: Benchmarks capture exact outputs and model interactions so results can be audited and verified. Complex interactions: They support testing for multi-modal inputs, code execution, tool use and multi-turn conversations. Rapid prototyping: They allow you to quickly design and iterate on creative new tasks. These powerful capabilities are powered by the new kaggle-benchmarks SDK . Here are a few resources for getting started: Benchmarks Cookbook: A guide to advanced features and use cases. Example tasks: Get inspired with a variety of pre-built tasks. Getting started : How to create your first task & benchmark How we’re shaping the future of AI evaluation The future of AI progress depends on how models are evaluated. With Kaggle Community Benchmarks, Kagglers are no longer just testing models, they’re helping shape the next generation of intelligence. Ready to build? Try Community Benchmarks today. POSTED IN: Developer tools AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 14 Jan 2026 14:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/developers-tools/kaggle-community-benchmarks/</guid>
    </item>
    <item>
      <title>Announcing the winner of the Global AI Film Award</title>
      <link>https://blog.google/company-news/inside-google/around-the-globe/google-middle-east/winner-of-the-global-ai-film-award/</link>
      <description><![CDATA[Over the past year, we’ve witnessed how creators globally have been using our AI models and tools to share their stories with the world. That’s why we launched the AI Film Award , last September, with a simple belief — that AI is here to become a powerful partner in the creative process. The response was overwhelming. We received 3,500 entries from every corner of the globe, each showcasing the incredible potential of AI in filmmaking. Just last week, we were happy to announce the winner of this special competition at the 1 Billion Followers Summit, one of the largest annual creator events globally. The winner received $1M, offered by our partner, the 1 Billion Followers Summit ! Meet the winner: ‘Lily’ The top honor went to Zoubeir ElJlassi, a visionary graphic designer and filmmaker from Tunisia. His film , Lily, tells the story of a lonely archivist whose life is upended by a doll found at the scene of a hit-and-run. The film shows that objects are silent witnesses to our secrets, eventually pushing the main character to confess and make things right. The jury — which included creative leaders from tech, content creation & filmmaking — was captivated by how ElJlassi blended human emotion with technology. He didn't just showcase what the tools could do; he used them to serve a deeply moving narrative. Creativity powered by Google AI When we launched this award, we introduced creators to a suite of tools designed to bring their visions to life. ElJlassi’s winning submission is a testament to how these models can be used in a professional filmmaking workflow: Cinematic vision with Veo : Using our state-of-the-art video generation model, Zoubeir was able to create the film’s distinct, gloomy aesthetic — generating consistent, high-quality video that set the atmospheric tone of the piece. Precision control with Flow : To ensure the characters conveyed the necessary feeling of empathy, Flow allowed for sophisticated control over scenes and styles. Visuals with Gemini : From storyboarding to identifying characters’ look & feel, Gemini played a crucial role in the creative pipeline. A global celebration of storytelling While Lily took home the grand prize, the shortlist highlighted the incredible diversity of AI cinema. The finalists tackled our two core themes — "Rewrite Tomorrow" and "The Secret Life of Everything" — with unique perspectives from around the world: The Translator (USA): A story of connection with nature in a dying world. Portrait No. 72 (Philippines): A heartwarming bond between an elderly photographer and a child. Cats Like Warmth (South Korea): A robot discovering the meaning of emotional warmth. HEAL (Egypt): A futuristic journey into memory and healing. What’s next? The AI Film Award has proven that when you put powerful tools in the hands of talented storytellers, the boundaries of creativity expand. We want to thank every creator who submitted a film, and the 1 Billion Followers Summit for hosting this landmark event and for their partnership. Congratulations to Zoubeir ElJlassi on this historic win. We can’t wait to see what he creates next. POSTED IN: Google in the Middle East AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 14 Jan 2026 10:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/inside-google/around-the-globe/google-middle-east/winner-of-the-global-ai-film-award/</guid>
    </item>
    <item>
      <title>Veo 3.1 Ingredients to Video: More consistency, creativity and control</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/veo-3-1-ingredients-to-video/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, Veo is getting more expressive, with improvements that help you create more fun, creative, high-quality videos based on ingredient images, built directly for the mobile format. We’re excited to bring new creative possibilities for everyone from casual storytellers to professional filmmakers. We’re releasing: Improvements to Veo 3.1 Ingredients to Video, our capability that lets you create videos based on reference images. This update makes videos more expressive and creative, even with simple prompts Native vertical outputs for Ingredients to Video (portrait mode) to power mobile-first, short-form video creation State-of-the-art upscaling to 1080p and 4K resolution 1 for high-fidelity production workflows Whether you are looking for livelier movement, better control over visual elements or broadcast-ready resolution, these updates give you the tools to bring your vision to life. These updates are launching in the Gemini app, YouTube, Flow, Google Vids, the Gemini API and Vertex AI. Improvements to Veo 3.1 Ingredients to Video Turn ingredient images into fun, shareable clips Even with short prompts, you can generate dynamic and engaging videos based on ingredient images. You’ll now see richer dialogue and storytelling, making your videos feel more alive and expressive. Maintain identity consistency for your characters Identity consistency is better than ever with Veo 3.1 Ingredients to Video. Keep your characters looking the same even as the setting changes, making it easier to tell a full narrative by having the same character appear across multiple scenes. Achieve background and object consistency Control the scene by maintaining the integrity of your setting and the objects within it. You can also reuse an object, backgrounds or textures across scenes. Seamlessly blend textures, characters and objects Combine disparate elements — like characters, objects, textures and stylized backgrounds — into a cohesive, high-impact clip. Pro tip: use the new Nano Banana Pro (Gemini 3 Pro Image) in the Gemini app or Flow to create your ingredient images, which you can then use to create stunning videos with Veo 3.1 Ingredients to Video. Create high-fidelity visuals with upgraded capabilities With Veo 3.1’s new capabilities, we are introducing mobile-optimized outputs and professional-grade quality options. Native vertical outputs for Ingredients to Video For the first time, "Ingredients to Video" supports generating videos in a native 9:16 aspect ratio. Whether you are creating for YouTube Shorts or other platforms, you can now produce high-quality, full-screen vertical storytelling without cropping or quality loss. State-of-the-art upscaling to 1080p and 4K resolution Generate videos 1080p and 4K with state-of-the-art upscaling. Our improved 1080p resolution offers a sharper, cleaner video perfect for editing. For even more detail, choose 4K to capture rich textures and stunning clarity — ideal for high-end productions and large screens. Try these updates today Across our products and services, you can now access these new capabilities tailored to your workflow: Consumers and creators: We are bringing Veo 3.1 Ingredients to Video directly to YouTube Shorts and the YouTube Create app for the first time. You can also try the enhanced Veo 3.1 Ingredients to Video and portrait mode for Veo in the Gemini app starting today. Professional and enterprise workflows: The enhanced Veo 3.1 Ingredients to Video and native vertical format support are rolling out to Flow , the Gemini API , Vertex AI , and Google Vids , with 1080p and 4K resolution options also available on Flow, the API, and Vertex AI. Verify videos in the Gemini app We’re committed to providing tools to make it easier to determine if content is AI-generated. This is why videos generated by Google’s tools are embedded with our imperceptible SynthID digital watermark. In December we expanded our powerful verification tool in the Gemini app to include video. You can now upload a video and simply ask if it was generated with Google AI. This builds on our existing image verification tools, helping to foster a more transparent ecosystem for everyone. You can find out more about how we’re increasing transparency in AI content with SynthID in our blog post . POSTED IN: AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 13 Jan 2026 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/veo-3-1-ingredients-to-video/</guid>
    </item>
    <item>
      <title>2025 at Google</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/look-back-2025/</link>
      <description><![CDATA[It’s (almost) a wrap on 2025! As we prepare for a great new year, let’s take a quick rewind and remember some of this past year’s most exciting launches, biggest moments and more.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/look-back-2025/</guid>
    </item>
    <item>
      <title>The Google guide for holiday help</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/holiday-planning-ai-gemini-tips/</link>
      <description><![CDATA[Check out our tips, trends and more for tackling any seasonal stress. Offload tedious tasks to Gemini, get insider recommendations from Google Maps and be sure you’re getting the best price when shopping on Google. Get the help you need — and then get back to enjoying the holiday festivities.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/holiday-planning-ai-gemini-tips/</guid>
    </item>
    <item>
      <title>Investing in America 2025</title>
      <link>https://blog.google/company-news/inside-google/company-announcements/investing-in-america-2025/</link>
      <description><![CDATA[Google's investments across the U.S are helping enable this extraordinary time for American innovation. Through major investments in technical infrastructure, research and development — along with expanded energy capacity for an AI-driven economy and workforce development and education programs — we will help the U.S. continue to lead the world in AI. These investments also unlock substantial economic opportunity for American businesses — advancing scientific breakthroughs, fortifying cybersecurity for the U.S., and creating new career opportunities for millions of Americans. In recent months, we’ve been bringing this new era of American innovation to life in communities across the country, with more to come.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 17 Nov 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/inside-google/company-announcements/investing-in-america-2025/</guid>
    </item>
    <item>
      <title>I/O 2025</title>
      <link>https://blog.google/innovation-and-ai/technology/developers-tools/google-io-2025-collection/</link>
      <description><![CDATA[At Google I/O 2025, our annual developer conference, we shared how we’re using our cutting-edge technology to build products that are intelligent and personalized — and that can take action for you. From advancements in our Gemini 2.5 models to rolling out AI Mode to Search for everyone in the U.S., we’re bringing innovative AI to our products to make them even more helpful. Read on to find out what’s new.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 20 May 2025 17:45:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/developers-tools/google-io-2025-collection/</guid>
    </item>
    <item>
      <title>Google Cloud Next 25</title>
      <link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/next-2025/</link>
      <description><![CDATA[AI is transforming the way we work — from boosting productivity and creativity to delivering real business transformation and impact. And at Google Cloud, we’re bringing the best of Google AI to people, organizations and businesses across the globe. Today, there are over four million developers building with the power of Gemini, our most advanced AI model family. This rapid adoption of Gemini, Imagen (our groundbreaking image generation model), and Veo (our industry-leading video generation model) has resulted in a 20x increase in Vertex AI usage in the past year alone. And within Google Workspace, over two billion AI assists are provided monthly to business users, reshaping how work gets done. We’re continuing to push what Google AI can do, making it more powerful, easier to use and more affordable. This week at Google Cloud Next 25, we shared exciting updates about how we’re doing just that: From introducing the most powerful chip we’ve ever built, to providing support for even more generative media models, to helping organizations create and manage AI agents, to changing how teams work with new capabilities in Google Workspace and Google Agentspace. We also shared over 500 examples of how organizations are using Google AI and seeing real impact. Here are some of the highlights of what we announced: - Ironwood, our 7th-generation TPU built for inference, will be available later this year. Compared to the prior generation, Ironwood offers five times more peak compute capacity and six times the high-bandwidth memory capacity. - With the addition of Lyria to Vertex AI, we are now the only platform with generative media models for video, image, speech and music. - New updates and tools for Gemini in Workspace bring even more helpful AI capabilities into tools people use every day — Docs, Sheets, Meet, Chat and more. - Updates to Agentspace make it easier for customers to discover, create and adopt AI agents. We're also growing the AI Agent Marketplace , a dedicated section within Google Cloud Marketplace where customers can easily browse and purchase AI agents from partners. . - And we unveiled more tools to build helpful agents including Agent Development Kit (ADK), an open-source framework for building agents while maintaining control over agent behavior; and Agent2Agent (A2A), new open protocol that gives your agents a common language to collaborate no matter what framework or vendor they are built on. - Gemini 2.5 Flash, our workhorse model with low latency and cost efficiency, will soon be available in Vertex AI. - Google Unified Security brings our best-in-class security products for threat intelligence, security operations, cloud security and secure enterprise browsing into a new, single AI-powered security solution. - With Cloud Wide Area Network (Cloud WAN), we’re making our high-speed, low-latency network — the same one that connects billions of users to services like Gmail, Photos and Search — available to organizations around the world.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 09 Apr 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/next-2025/</guid>
    </item>
    <item>
      <title>The Check Up with Google</title>
      <link>https://blog.google/innovation-and-ai/technology/health/google-health-check-up-2025/</link>
      <description><![CDATA[AI can lead to scientific progress and cutting-edge products that help improve health outcomes for people all around the world. At Google’s annual health event, The Check Up, we shared how our products, research and partnerships are making the most of AI with the goal of helping everyone, everywhere live healthier lives. Dr. Karen DeSalvo Chief Health Officer, Google]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 18 Mar 2025 13:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/health/google-health-check-up-2025/</guid>
    </item>
    <item>
      <title>New AI features and more for Android and Pixel</title>
      <link>https://blog.google/products-and-platforms/devices/pixel/pixel-android-ai-updates-december-2024/</link>
      <description><![CDATA[Our latest updates for Android and Pixel are packed with tons of AI-powered features — including Expressive Captions, an all-new feature that brings more feeling to captions, Gemini’s saved info, which remembers important information for you, and updates for Call Screen so it’s even easier to respond. Check out all the updates below.]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 05 Dec 2024 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/devices/pixel/pixel-android-ai-updates-december-2024/</guid>
    </item>
    <item>
      <title>The AI for Science Forum: A new era of discovery</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/ai-science-forum-2024/</link>
      <description><![CDATA[AI is revolutionizing the landscape of scientific research, enabling advancements at a pace that was once unimaginable — from accelerating drug discovery to designing new materials for clean energy technologies. The AI for Science Forum — co-hosted by Google DeepMind and the Royal Society — brought together the scientific community, policymakers, and industry leaders to explore the transformative potential of AI to drive scientific breakthroughs, address the world's most pressing challenges, and lead to a new era of discovery.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 18 Nov 2024 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/ai-science-forum-2024/</guid>
    </item>
    <item>
      <title>2023 at Google</title>
      <link>https://blog.google/company-news/inside-google/google-2023-recaps-highlights/</link>
      <description><![CDATA[From launching Gemini to celebrating our 25th birthday, 2023 was a busy and exciting year at Google. Explore the stories below to learn more about some of the biggest launches, moments and highlights of the year. Cheers to 2023 — we're ready for you, 2024.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 20 Dec 2023 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/inside-google/google-2023-recaps-highlights/</guid>
    </item>
  </channel>
</rss>
