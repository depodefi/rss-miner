<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 17 Dec 2025 10:04:50 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>We’re publishing an AI playbook to help others with sustainability reporting.</title>
      <link>https://blog.google/outreach-initiatives/sustainability/ai-playbook-sustainability-reporting/</link>
      <description><![CDATA[We’re sharing a practical playbook to help organizations streamline and enhance sustainability reporting with AI. Corporate transparency is essential, but navigating fragmented data and labor-intensive processes often leaves little bandwidth for the strategic work that actually drives progress forward. After two years of integrating AI into our own environmental reporting process, we’re open-sourcing our learnings to help others break through these bottlenecks. The AI Playbook for Sustainability Reporting is a hands-on toolkit designed to move from hype to implementation. It includes a systematic framework to audit processes, "starter pack" prompt templates for common tasks, and real-world examples of how teams can use tools like Gemini and NotebookLM to help validate claims and answer inquiries. Dive into the playbook today, and let us know if you have any feedback .]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 15 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/sustainability/ai-playbook-sustainability-reporting/</guid>
    </item>
    <item>
      <title>You can now have more fluid and expressive conversations when you go Live with Search.</title>
      <link>https://blog.google/products/search/live-audio-gemini-model-update/</link>
      <description><![CDATA[When you go Live with Search , you can have a back-and-forth voice conversation in AI Mode to get real-time help and quickly find relevant sites across the web. And now, thanks to our latest Gemini model for native audio , the responses on Search Live will be more fluid and expressive than ever before. This update means that Search can respond with a more natural sounding voice or at a certain speed – perfect for DIY help or learning more about a new topic, like geology. Just open the Google app ( Android & iOS ), tap the Live icon under the search bar and ask your question out loud for a helpful audio response. The updated Gemini model for native audio will roll out over the next week to all Search Live users in the U.S. We hope you’ll try it out soon to enjoy smoother, more natural conversations.]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 12 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/live-audio-gemini-model-update/</guid>
    </item>
    <item>
      <title>Bringing state-of-the-art Gemini translation capabilities to Google Translate</title>
      <link>https://blog.google/products/search/gemini-capabilities-translation-upgrades/</link>
      <description><![CDATA[True understanding comes from not just what someone says, but also the nuance of how they say it. Today, Google Translate is getting better at both. We’re introducing state-of-the-art text translation quality 1 in Search and the Translate app, built with Gemini. This means you’ll get much smarter, more natural and accurate text translations wherever you’re searching. We’re also rolling out a new beta version of live translation, that brings real-time, natural-sounding translations right to your headphones with the help of Gemini’s new, native speech-to-speech translation capabilities . Plus, we’re adding more languages for you to practice in the Translate app. Get smarter, more natural translations, built with Gemini Starting today, Google Translate uses advanced Gemini capabilities to better improve translations on phrases with more nuanced meanings like idioms, local expressions or slang. Say you’re trying to translate an English idiom like “ stealing my thunder .” Now, it's easier than ever to get a more natural, accurate translation, instead of a literal word-for-word translation. Gemini parses the context to give you a helpful translation that captures what the idiom really means. This update is rolling out starting today in the U.S. and India translating between English and nearly 20 languages, including Spanish, Hindi, Chinese, Japanese and German, in the Translate app ( Android and iOS ) and on the web . Hear and understand the world in real-time Building on Gemini’s new live speech-to-speech translation capabilities , we are rolling out a beta experience enabling you to hear real-time translations in your headphones. This new experience works to preserve the tone, emphasis and cadence of each speaker to create more natural translations and make it easier to follow along with who said what. Whether you're trying to have a conversation in a different language, listen to a speech or lecture while abroad, or watch a TV show or film in another language, you can now put in your headphones, open the Translate app, tap “Live translate” and hear a real-time translation in your preferred language. Following positive feedback in early testing, we're excited to make this beta more broadly available to collect even more feedback as we work to refine the model and experience. Starting today the beta is rolling out in the Translate app on Android in the U.S., Mexico, and India, works with any pair of headphones, and supports more than 70 languages . And we’ll be bringing it to iOS and more countries in 2026. Try it out in the Android app and let us know your thoughts via in-app feedback! Practice and master even more languages in Translate Starting today, we’re also expanding our language learning tools in the Translate app with improved feedback so you can get helpful tips based on your speaking practice. We’re also introducing a way for you to challenge yourself and reach your learning goals by tracking how many days in a row you’ve been learning, so you can clearly see your progress and consistency over time. And we’re expanding the capability to nearly 20 new countries including Germany, India, Sweden, and Taiwan, so even more people can continue to refine their language skills, including: English to German and Portuguese Bengali, Mandarin Chinese (Simplified), Dutch, German, Hindi, Italian, Romanian, and Swedish to English We’ve heard from users that they appreciate the customized learning experiences that reflect real-life scenarios. We'll continue to add new ways for people to make progress on their language learning goals. With even more advanced AI models and expanded language learning capabilities in Translate, we’re helping you capture not just the words, but the meaning behind them. We’re excited to hear what you think. POSTED IN: Translate Search AI]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 12 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-capabilities-translation-upgrades/</guid>
    </item>
    <item>
      <title>Gradient Canvas: Celebrating over a decade of artistic collaborations with AI</title>
      <link>https://blog.google/technology/ai/google-gradient-canvas-ai-art/</link>
      <description><![CDATA[To celebrate over a decade of artistic experimentation with AI, today we are revealing Gradient Canvas , an exhibition of 13 newly commissioned artworks. Inspired by the Bay Area’s local ecology, Gradient Canvas explores the idea that artificial intelligence can act as a bridge — connecting human perception, machine vision and the natural world around us. The featured global artists, a group including long-time collaborators with Google, used Google tools and AI to expand how we sense and interact with our surroundings. Their new works probe the co-evolving relationships between people, nature and machines, offering unique perspectives on how different forms of intelligence make sense of the world. Hear directly from the multidisciplinary group of artists about what inspired their work and the unique approaches to their work with AI: Installation view of Pollinator Pathmaker : 6nvKvSPnBEEFa6vTqwXJaZ in ‘Human Vision’ by Alexandra Daisy Ginsberg at Google Gradient Canopy in Mountain View, California, 2025. Photo: Henrik Kam. In Silico , by Casey Reas Photo: Henrik Kam. The Recombinant Room , by Certain Measures Photo: Henrik Kam. A Google Tree , by Clement Vallas Photo: Henrik Kam. The Garden Eternal: California , by Linda Dounia Photo: Henrik Kam. EP Flow , by Michael Joo Photo: Henrik Kam. Somatic Landscapes , by Rashaad Newsome Photo: Henrik Kam. Machine Dreams: Biophilia , by Refik Anadol Photo: Henrik Kam. California Terrain , by Sarah Rosalena Photo: Henrik Kam. Clouds , by Trevor Paglen Photo: Henrik Kam. DEAR DATA , by Sasha Stiles Photo: Henrik Kam. Over a decade of creative collaboration Rooted in early and long term efforts like the Google Arts & Culture Artist-in-Residence programs , our commitment to supporting artists has a long history. Ten years ago, Google researcher Alex Mordvintsev developed DeepDream, a computer-vision program that signaled the creative potential of artists and artificial intelligence and sparked a public fascination with AI-generated visuals. This excitement directly paved the way for our groundbreaking 2016 art exhibition with Gray Area and initiatives like Artists + Machine Intelligence (AMI), accelerating partnership between artists and AI that continues today. Human imagination is what gives technology purpose. We believe that when artists work with powerful tools, they don't just create; they help us all build a more thoughtful future. That’s why our focus for the next decade is on expanding this canvas by supporting the next generation of visionary voices and making cutting-edge AI tools more accessible for everyone. The exhibition includes a physical installation at our Gradient Canopy office in Mountain View, explore the artworks online on Google Arts & Culture . POSTED IN: AI Arts & Culture]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 11 Dec 2025 21:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-gradient-canvas-ai-art/</guid>
    </item>
    <item>
      <title>4 highlights from Google Beam in 2025</title>
      <link>https://blog.google/technology/research/google-beam-2025-moments/</link>
      <description><![CDATA[In 2021, we introduced Project Starline with the ambitious goal of making remote communication feel like you’d instantly traveled across distances to be together. This year, our vision became reality with the announcement of Google Beam — our first true-to-life 3D video-communication platform, powered by Google AI. Here’s a look at how Beam progressed this year: 1. Google Beam took center stage at I/O. At I/O in May, CEO Sundar Pichai unveiled the evolution of Project Starline into a new AI-powered video communication platform, Google Beam . He showed how Google is bringing its AI technology to Google Beam through breakthrough AI-powered video models that transform 2D video streams to 3D experiences, and how we’re paving the way for future features like real-time speech translation . 2. Beam began working with industry leaders in video. At InfoComm 2025, we announced HP Dimension with Google Beam , the first product specifically for businesses developed in partnership with our partner, HP. Our technology earned a " Best of Show " win at InfoComm, recognized for its broad impact and remote conversation experience that’s far closer to being in the same room. HP Dimension with Google Beam was later named an Honoree in Fast Company's Next Big Things in Tech Awards 2025 . Google Beam was also showcased at Zoomtopia, Zoom's annual user conference, highlighting a partnership to integrate the Google Beam platform with their video conferencing software. To further expand Beam's reach, we collaborated with HP to establish a network of distribution partners like Diversified, AVI-SPL and more. Zoom CEO, Eric Yuan In October, we co-hosted with industry experts at a WORKTECH event where founder and CEO of UnWork and WORKTECH Philip Ross called Beam “an AI game-changer that connects people and not just pixels, changing the way we enable presence and collaboration.” 3. Workplaces all over the world became early users — including Google. Organizations globally began using Beam's true-to-life capabilities firsthand in their offices. Workplaces like Schwarz Digits and Huntington Bank were among them. The list is growing, too: Bain, Duolingo, Salesforce, Citadel, NEC, Hackensack Meridian Health and more are actively embracing Beam. Here at Google, Googlers testing Beam said they prefer it over typical video conferencing and 90% agreed that using Beam makes it feel like they're in the same space as their meeting partner. Through testing, we’ve identified five key use cases where Beam truly shines: job interviews and recruiting; mentoring and talent development; productivity and collaboration; critical conversations and connecting distributed teams. 4. We announced a pilot program with the USO. We began work to place Beam devices in select USO centers worldwide, enabling deployed active duty service members to connect with their families. Learn more about Google Beam and connect with our sales team at beam.google . POSTED IN: Research AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 11 Dec 2025 18:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/research/google-beam-2025-moments/</guid>
    </item>
    <item>
      <title>These developers are changing lives with Gemma 3n</title>
      <link>https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X When Gemma 3n was released, we hoped developers would use its on-device, multimodal capabilities to make a difference in people’s lives. With more than 600 projects submitted to the Gemma 3n Impact Challenge on Kaggle, the community delivered on that promise. Today, we’re excited to introduce the winners: First Place: Gemma Vision Gemma Vision is an AI assistant designed for visually impaired people. The developer’s brother, who is blind, played a vital role in ensuring features were genuinely helpful for the blind community. Because holding a phone can be impractical while using a cane, the system was designed to process visuals from a phone camera strapped to the user’s chest. Functions can be triggered using a 8BitDo Micro controller or voice commands, allowing users to perform actions without navigating touchscreen menus. This project also won the Special Technology Prize for Google AI Edge , a platform for deploying models on-device. It deployed Gemma 3n using the MediaPipe LLM Inference API and leveraged features like streamed responses in the flutter_gemma package to make the experience fluid. Second Place: Vite Vere Offline Vite Vere helps foster autonomy for people with cognitive disabilities. Originally developed using the Gemini API, this project leveraged Gemma 3n to make the digital companion work offline. By transforming images to simple instructions that can then be read aloud using the local device’s text-to-speech engine, the app enables users to navigate daily tasks. Third Place: 3VA For decades, Eva, a brilliant graphic designer with cerebral palsy, was limited to simple commands like “want food now.” This project fine-tuned Gemma 3n to translate pictograms into rich expressions that better reflect Eva’s voice. The team trained the model locally using Apple’s MLX framework, demonstrating a cost-effective way to develop personalized Augmentative and Alternative Communication (AAC) technology. Fourth Place: Sixth Sense for Security Guards Unlike traditional video monitoring systems that just detect motion, this project used Gemma 3n to provide human-level context and distinguish benign events from genuine threats. By integrating a lightweight YOLO-NAS model to detect initial movement and send it to Gemma 3n for processing, the system can handle high-bandwidth video feeds (up to 360fps and 16 cameras) in real time. The Unsloth Prize: Dream Assistant Voice assistants frequently fail users with speech impairments. This project used Unsloth , a library for efficient fine-tuning, to train Gemma 3n on an individual’s audio recordings. The result is a custom AI assistant that understands the user’s unique speech patterns and enables voice control over device functions. The Ollama Prize: LENTERA This project demonstrates how to bring AI to disconnected regions by transforming affordable hardware into offline microservers. Lentera broadcasts a local WiFi hotspot, allowing users to connect their devices to an educational hub running Gemma 3n via Ollama , a platform for local model deployment. The LeRobot Prize: Graph-based Cost Learning and Gemma 3n for Sensing Robotic exploration is often bottlenecked by the time spent sensing rather than moving. To solve this, the team built a novel “scanning-time-first” pipeline on top of LeRobot , a robotics framework developed by Hugging Face. This project used Gemma 3n to create plans while an inductive graph-based matrix completion (IGMC) model predicted latencies, demonstrating the viability of embodied AI at the edge. The NVIDIA Jetson Prize: My (Jetson) Gemma Integrating AI into our physical environment requires systems that are both responsive and energy-efficient. This project used a smart CPU-GPU hybrid processing strategy to deploy a context-aware voice interface on an NVIDIA Jetson Orin , demonstrating how helpful AI can move beyond screens to assist users in the real world. From accessibility to crisis response, these projects show what's possible with Gemma 3n. Many others deserve recognition, so join us as we highlight a developer story every day on @googleaidevs over the coming month. POSTED IN: Developers AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 10 Dec 2025 17:15:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</guid>
    </item>
    <item>
      <title>Learn more about AI in the workplace in our new research report.</title>
      <link>https://blog.google/products/workspace/gemini-ai-workplace-research-report/</link>
      <description><![CDATA[A new global survey of executives, decision makers and knowledge workers reveals that organizations truly transforming with AI are seeing real results that move their businesses and allow employees to focus on meaningful work. The biggest gains from adopting AI aren’t about saving time — they’re about expanding potential. Highly transformed organizations surveyed report that AI: Increases innovation by 57% Decreases time spent on mundane tasks by 39% Improves work creativity by 65% Read the full research report, “ Beyond AI Optimism: Five ways to move your business from saving time to sparking innovation ,” or visit the Workspace blog for key takeaways.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/gemini-ai-workplace-research-report/</guid>
    </item>
    <item>
      <title>2025 at Google</title>
      <link>https://blog.google/technology/ai/look-back-2025/</link>
      <description><![CDATA[It’s (almost) a wrap on 2025! As we prepare for a great new year, let’s take a quick rewind and remember some of this past year’s most exciting launches, biggest moments and more.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/look-back-2025/</guid>
    </item>
    <item>
      <title>How we’re supporting the next generation of innovators</title>
      <link>https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</link>
      <description><![CDATA[For decades, Google has supported Computer Science Education Week (CSEdWeek) to demystify coding and computational thinking. This year during CSEdWeek, we’re taking part in the global Hour of AI alongside our partners Code.org and the Computer Science Teachers Association (CSTA), to help educators and students better understand, use and create with AI. Towards this goal, we’re launching a new quest in our gamified experience, AI Quests , bringing AI literacy to classrooms with a hands-on approach, and we’re announcing over $5 million in Google.org funding for computer science teaching. Launching a new quest Hundreds of Googler volunteers around the world are visiting classrooms this week to lead AI Quests , our game-based learning series developed with the Stanford Accelerator for Learning . Today, we’re releasing a new quest where students step into the shoes of researchers and use an AI model to detect eye disease and prevent blindness. This quest is inspired by our real-world research on diabetic retinopathy . It can be accessed for free along with our existing flood forecasting quest and accompanying resources for teachers. We’re also working closely with partners to bring AI Quests to more classrooms in 2026. One collaboration is with the Raspberry Pi Foundation and Google DeepMind, which has expanded their Experience AI program — recognized by UNESCO for promoting responsible AI education — to millions of students with funding from Google.org and integrated AI Quests into their curriculum. Investing in educators We’re announcing over $5 million in new Google.org funding to bolster computer science teaching in the age of AI. This builds on our recent $30 million global commitment to learning and foundational research, as well as the over $240 million we have provided to advance computer science education globally. The new funding will help organizations like California State University, Dominguez Hills to prepare teachers to deliver foundational computer science and AI curriculum to K-12 students and CSTA to publish the revised K–12 Computer Science Standards through a modern, accessible web-presence. While coding tasks may change in the AI era, the foundational principles of computer science remain more vital than ever. POSTED IN: Learning & Education Google.org AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</guid>
    </item>
    <item>
      <title>Transforming Nordic classrooms through responsible AI partnerships</title>
      <link>https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</link>
      <description><![CDATA[As AI transforms the classroom, school districts in Northern Europe are setting a global standard for innovation. Beyond mere adoption, districts across Iceland, Norway, and Sweden are prioritizing responsible use. By partnering with educators and administrators, they are ensuring Google’s AI tools are integrated thoughtfully, securely, and with real value for students. Providing personalized learning in Icelandic schools Building on the widespread adoption of Google Classroom in Iceland, we partnered with the Ministry of Education to launch a pilot with 300 teachers. By integrating Gemini for Education and NotebookLM, the program aims to enhance personalized learning and boost AI literacy. Beyond developing fluency, the pilot identifies the resources teachers need for ethical use, while upskilling them to streamline administrative tasks and pinpoint student learning gaps. Saving teachers time while building AI literacy in Sweden Through a partnership with school districts across Sweden to bring Gemini for Education to nearly 30,000 students and faculty members, schools are embracing AI’s potential to transform learning. Teachers are using Gemini to create high-quality, tailored materials — a shift one educator describes as "revolutionary" because it allows them to invest more time with their students. According to Johan Kellén, teacher & ICT Coordinator at Linköping Municipality, it takes so long to produce good teaching material — you usually have to construct it yourself and ensure it is up to date and current, and adapted to each class and sometimes each student. This is where Gemini is able to help. Districts hosted workshops to empower educators to use Gemini and foster dialogue with older students about utilizing Guided Learning mode as a study aid. They believe AI literacy is a shared responsibility. Leading the charge for safe, secure digital learning in Norway In a landmark move for digital privacy, Norway completed a national Data Protection Impact Assessment (DPIA), greenlighting the use of Google Workspace for Education and ChromeOS in schools. This collaborative achievement between Google Cloud and the Norwegian Association of Local and Regional Authorities (KS) is a prime example of public sector efficiency. By conducting a centrally driven DPIA, KS eliminated the need for every single municipality to conduct their own complex assessments, highlighting how Google tools meet stringent GDPR requirements and ensuring that local resources focus on innovation over administrative compliance. By prioritizing trust and partnering with Google, Norway has secured a safe, innovative learning environment for students while saving IT administrators significant time and resources. POSTED IN: Google in Europe Learning & Education AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 10:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</guid>
    </item>
    <item>
      <title>The latest AI news we announced in November</title>
      <link>https://blog.google/technology/ai/google-ai-updates-november-2025/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X For more than 20 years, we’ve invested in machine learning and AI research, tools and infrastructure to build products that make everyday life better for more people. Teams across Google are working on ways to unlock AI’s benefits in fields as wide-ranging as healthcare, crisis response and education. To keep you posted on our progress, we're doing a regular roundup of Google's most recent AI news. Here’s a look back at some of our AI announcements from November. November ushered in a new era of intelligence: with the launch of Gemini 3, we're improving everyone's ability to learn, plan or just get things done. From "vibe coding" with our powerful models, to generating professional grade visuals with Nano Banana Pro, to the autonomous workflows in our new Google Antigravity platform, the distance between a spark of imagination and reality has never been shorter. So, whether you’re a developer building complex agents or a traveler planning for the upcoming holidays with Canvas in AI Mode, these updates turn AI into a proactive partner ready to help you get things done, all season long. We released Gemini 3, AI for a new era of intelligence . Gemini 3 is built to bring any idea to life. It represents the next step in our work to push the frontiers of intelligence, agentic experiences and personalization — so that AI is truly helpful for everyone. Gemini 3 is the best model in the world for multimodal understanding and it's our most powerful agentic and vibe coding model to date. For developers, Gemini 3 Pro outperforms previous versions across major AI benchmarks. Gemini 3’s upgraded smarts and new capabilities are now available in the Gemini app , and you can review our hub with all the Gemini 3 announcements . We made Gemini 3 available in Google Search for our most intelligent search yet . Gemini 3’s state-of-the-art reasoning is now available in Google Search, starting with AI Mode — marking the first time we brought a Gemini model to Search on day one. Gemini 3 grasps depth and nuance, and unlocks new experiences in Search with dynamic visual layouts, interactive tools and simulations tailored specifically for your query. Google AI Pro and Ultra subscribers in nearly 120 countries and territories in English can use Gemini 3 Pro by selecting “Thinking with 3 Pro” from the model drop-down menu in AI Mode. We unveiled Nano Banana Pro, built on Gemini 3 . Nano Banana Pro, our newest image generation and editing model, is built on Gemini 3 and moves beyond spontaneous art into an era of high-fidelity, studio-quality visuals. You now have the choice between the original Nano Banana for fun, fast editing, or Pro for an even more powerful creative partner capable of handling complex tasks demanding the highest quality. To help get you started, we shared seven tips to get the most out of Nano Banana Pro . We introduced Google Antigravity, a new agentic development platform . Antigravity is a platform designed to give developers an AI-powered coding experience that goes beyond simple editing. It delivers a new agent-first interface for deploying agents that autonomously plan, execute and verify complex tasks. Our vision for Antigravity is to enable anyone with an idea to experience liftoff and build that idea into reality. You can try Antigravity for yourself, available today in public preview. We announced that Google Maps is getting smarter with Gemini . With the help of Gemini, you will soon have the first, hands-free, conversational driving experience in Google Maps that allows you to find places, report traffic, ask for suggestions along your route and more using just your voice. Plus, new landmark-based navigation will give you clear directions, so in addition to hearing “turn right in 500 feet,” you’ll also get directions based on helpful landmarks like “turn right after the Thai Siam Restaurant.” Landmark-based navigation is rolling out now on Android and iOS in the U.S, and Gemini in navigation on Google Maps is rolling out everywhere Gemini is available . We announced that Gemini has started rolling out in Android Auto . Android Auto is already available in over 250 million cars on the road, and Gemini is now coming along for the ride to make life on the road even better. You’ll be able to use natural language to add stops, send messages, access emails, create playlists and even brainstorm ideas while driving. Just make sure you have the Gemini app on your phone and look for the tooltip on your car display. We introduced SIMA 2, a significant step toward Artificial General Intelligence (AGI) . SIMA 2 is a major milestone in our work to create general and helpful AI agents. By integrating the advanced capabilities of Gemini into SIMA 2, the model is evolving from an instruction-follower into an interactive gaming companion. Now, SIMA 2 can follow human-language instructions in virtual worlds and also think about its goals, converse with users and improve itself over time, marking an important step in the direction of robotics and AI-embodiment in general. We released WeatherNext 2: our most advanced weather forecasting model . WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour, and we’re already using this breakthrough technology to support weather agencies in making decisions. We marked the 5-year anniversary of AlphaFold cracking the protein folding problem, and spotlighted its ongoing impact . Five years ago AlphaFold 2 solved the protein structure prediction problem. The profound scientific and societal value of this work was recognized in 2024 with the Nobel Prize in Chemistry. In November, we looked back at how AlphaFold has unlocked new avenues of biological research and provided our first major proof point that AI can be a powerful tool to advance science. We announced new ways to plan travel with AI in Search . Our new AI features in Search help you build the perfect itinerary, from snagging a great deal to turning your plans into actual bookings. To get started with planning, try the Canvas tool in AI Mode: Describe the kind of trip you want and what recommendations you need. Then hit "Create Canvas," and watch your custom travel plan come together. We added new AI shopping features in Search and Gemini to help with the holidays . Our biggest upgrade to shopping means you can now use conversational AI and agentic AI to take the hard work out of your holiday shopping. Using AI Mode in Search to shop, you can now describe what you’re looking for and get an intelligently organized response that brings together rich visuals, price, reviews, inventory info, and more. Plus, new AI agents can now call stores to check stock and use agentic checkout to buy items automatically from eligible merchants when the price is right. We announced new commitments to AI learning and education . At our AI for Learning Forum in London, we announced $30 million in new funding for learning. The event brought together experts in education and technology and continued our recent work to develop AI in a way that improves learning outcomes. We also released popular new features to help with daily studying, including the ability to create flashcards and quizzes right in the NotebookLM app . We shared essential tips for using Gemini Live . Gemini Live’s latest updates allow you to have more natural, two-way conversations with AI. Our tips show how to make the most of these updates by tailoring your learning, especially for complex subjects. You can now adjust Gemini's speech speed to learn at your own pace and improve accessibility. You can also practice a new language, rehearse for a job interview or even liven your conversation up with a fun accent. We announced a new $40 billion investment in Texas for AI and cloud infrastructure . CEO of Google and Alphabet Sundar Pichai and Texas Governor Greg Abbott made the announcement at an event in Midlothian, TX. This latest announcement represented the capstone of our 2025 push to make AI investments that unlock economic opportunity, advance scientific breakthroughs and create opportunities that benefit everyone. It includes major investments across America , as well as Europe , Africa and the Asia-Pacific region — alongside a critical US workforce initiative to train 100,000 electrical workers and create 30,000 new apprentices. POSTED IN: AI Gemini Developers Search Google DeepMind Maps Android Shopping Learning & Education Google Labs Company announcements Google in Europe Google.org]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 19:45:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-ai-updates-november-2025/</guid>
    </item>
    <item>
      <title>New research from Google Workspace reveals how young leaders are using AI at work.</title>
      <link>https://blog.google/products/workspace/young-leaders-survey-ai/</link>
      <description><![CDATA[Google Workspace has released findings from our second survey that looks at how people aged 22-39 are using AI at work. Commissioned by Workspace in partnership with the Harris Poll, the findings reveal three key themes: Young leaders want AI that offers more personalized responses — think outputs that aren’t just generic, but use your preferred tone and style. They are taking a very hands-on approach to using this technology, becoming AI architects for their own workflows. AI is helping young leaders feel more confident at work: They’re increasingly relying on it as a tool for professional development. Check out the press release for the full data, as well as insights from Google Workspace’s VP of Product, Yulie Kwon Kim.]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/young-leaders-survey-ai/</guid>
    </item>
    <item>
      <title>Gemini 3 Pro: the frontier of vision AI</title>
      <link>https://blog.google/technology/developers/gemini-3-pro-vision/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Gemini 3 Pro represents a generational leap from simple recognition to true visual and spatial reasoning. It is our most capable multimodal model ever, delivering state-of-the-art performance across document, spatial, screen and video understanding. This model sets new highs on vision benchmarks such as MMMU Pro and Video MMMU for complex visual reasoning, as well as use-case-specific benchmarks across document, spatial, screen and long video understanding. 1. Document understanding Real-world documents are messy, unstructured, and difficult to parse — often filled with interleaved images, illegible handwritten text, nested tables, complex mathematical notation and non-linear layouts. Gemini 3 Pro represents a major leap forward in this domain, excelling across the entire document processing pipeline — from highly accurate Optical Character Recognition (OCR) to complex visual reasoning. Intelligent perception To truly understand a document, a model must accurately detect and recognize text, tables, math formulas, figures and charts regardless of noise or format. A fundamental capability is "derendering" — the ability to reverse-engineer a visual document back into structured code (HTML, LaTeX, Markdown) that would recreate it. As illustrated below, Gemini 3 demonstrates accurate perception across diverse modalities including converting an 18th-century merchant log into a complex table, or transforming a raw image with mathematical annotation into precise LaTeX code. Example 1: Handwritten Complex Table from 18th century Albany Merchant’s Handbook Example 2: Reconstructing equations from an image Example 3: Reconstructing Florence Nightingale's original Polar Area Diagram into an interactive chart (with a toggle!) Sophisticated reasoning Users can rely on Gemini 3 to perform complex, multi-step reasoning across tables and charts — even in long reports. In fact, the model notably outperforms the human baseline on the CharXiv Reasoning benchmark (80.5%). To illustrate this, imagine a user analyzing the 62-page U.S. Census Bureau " Income in the United States: 2022 " report with the following prompt: “Compare the 2021–2022 percent change in the Gini index for "Money Income" versus "Post-Tax Income", and what caused the divergence in the post-tax measure, and in terms of "Money Income", does it show the lowest quintile's share rising or falling?” Swipe through the images below to see the model's step-by-step reasoning. Visual Extraction: To answer the Gini Index Comparison question, Gemini located and cross-referenced this info in Figure 3 about “Money Income decreased by 1.2 percent” and in Table B-3 about “Post-Tax Income increased by 3.2 percent” Causal Logic: Crucially, Gemini 3 does not stop at the numbers; it correlates this gap with the text’s policy analysis, correctly identifying Lapse of ARPA Policies and the end of Stimulus Payments are the main causes. Numerical Comparison: To compare the lowest quantile’s share rising or falling, Gemini3 looked at table A-3, and compared the number of 2.9 and 3.0, and concluded that “the share of aggregate household income held by the lowest quintile was rising.” Final Model Answer 2. Spatial understanding Gemini 3 Pro is our strongest spatial understanding model so far. Combined with its strong reasoning, this enables the model to make sense of the physical world. Pointing capability: Gemini 3 has the ability to point at specific locations in images by outputting pixel-precise coordinates. Sequences of 2D points can be strung together to perform complex tasks, such as estimating human poses or reflecting trajectories over time. Open vocabulary references: Gemini 3 identifies objects and their intent using an open vocabulary. The most direct application is robotics: the user can ask a robot to generate spatially grounded plans like, “Given this messy table, come up with a plan on how to sort the trash.” This also extends to AR/XR devices, where the user can request an AI assistant to “Point to the screw according to the user manual.” 3. Screen understanding Gemini 3.0 Pro’s spatial understanding really shines through its screen understanding of desktop and mobile OS screens. This reliability helps make computer use agents robust enough to automate repetitive tasks. UI understanding capabilities can also enable tasks like QA testing, user onboarding and UX analytics. The following computer use demo shows the model perceiving and clicking with high precision. 4. Video understanding Gemini 3 Pro takes a massive leap forward in how AI understands video, the most complex data format we interact with. It is dense, dynamic, multimodal and rich with context. High frame rate understanding: We have optimized the model to be much stronger at understanding fast-paced actions when sampling at >1 frames-per-second. Gemini 3 Pro can capture rapid details — vital for tasks like analyzing golf swing mechanics. By processing video at 10 FPS—10x the default speed—Gemini 3 Pro catches every swing and shift in weight, unlocking deep insights into player mechanics. 2. Video reasoning with “thinking” mode: We upgraded "thinking" mode to go beyond object recognition toward true video reasoning. The model can now better trace complex cause-and-effect relationships over time. Instead of just identifying what is happening, it understands why it is happening. 3. Turning long videos into action: Gemini 3 Pro bridges the gap between video and code. It can extract knowledge from long-form content and immediately translate it into functioning apps or structured code. 5. Real-world applications Here are a few ways we think various fields will benefit from Gemini 3’s capabilities. Education Gemini 3.0 Pro’s enhanced vision capabilities drive significant gains in the education field, particularly for diagram-heavy questions central to math and science. It successfully tackles the full spectrum of multimodal reasoning problems found from middle school through post-secondary curriculums. This includes visual reasoning puzzles (like Math Kangaroo ) and complex chemistry and physics diagrams. Gemini 3’s visual intelligence also powers the generative capabilities of Nano Banana Pro . By combining advanced reasoning with precise generation, the model, for example, can help users identify exactly where they went wrong in a homework problem. Prompt: “Here is a photo of my homework attempt. Please check my steps and tell me where I went wrong. Instead of explaining in text, show me visually on my image.” (Note: Student work is shown in blue; model corrections are shown in red). [ See prompt in Google AI Studio ] Medical and biomedical imaging Gemini 3 Pro 1 stands as our most capable general model for medical and biomedical imagery understanding, achieving state-of-the-art performance across major public benchmarks in MedXpertQA-MM (a difficult expert-level medical reasoning exam), VQA-RAD (radiology imagery Q&A) and MicroVQA (multimodal reasoning benchmarks for microscopy based biological research). Input image from MicroVQA - a benchmark for microscopy-based biological research Law and finance Gemini 3 Pro’s enhanced document understanding helps professionals in finance and law tackle highly complex workflows. Finance platforms can seamlessly analyze dense reports filled with charts and tables, while legal platforms benefit from the model's sophisticated document reasoning. 6. Media resolution control Gemini 3 Pro improves the way it processes visual inputs by preserving the native aspect ratio of images. This drives significant quality improvements across the board. Additionally, developers gain granular control over performance and cost via the new media_resolution parameter. This allows you to tune visual token usage to balance fidelity against consumption: High resolution: Maximizes fidelity for tasks requiring fine detail, such as dense OCR or complex document understanding. Low resolution: Optimizes for cost and latency on simpler tasks, such as general scene recognition or long-context tasks. For specific recommendations, refer to our Gemini 3.0 Documentation Guide . Build with Gemini 3 Pro We are excited to see what you build with these new capabilities. To get started, check out our developer documentation or play with the model in Google AI Studio today. POSTED IN: Developers AI Gemini Models]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/gemini-3-pro-vision/</guid>
    </item>
    <item>
      <title>Look back on your 2025 with Google Photos Recap</title>
      <link>https://blog.google/products/photos/google-photos-2025-recap/</link>
      <description><![CDATA[Another year is almost in the books, and it’s time to look back at the moments that made it memorable. Last year , we introduced Google Photos Recap to help you rediscover what made your year special. Now, Recap is back for 2025, turning your photos and videos from the past year into a highlight reel you can enjoy and share. This year, we’ve added new ways for you to personalize, create and share your Recap. Look back at the moments that made your year Your Recap highlights memorable photos and moments from your year, paired with fun graphics and cinematic effects. You’ll see familiar photo stats from last year, like your top people and total photo count — and this year we also added a selfie count. And for those in the U.S. with Gemini features in Photos enabled, your Recap will also showcase your standout hobbies and top highlights from the year. We also heard your feedback and added the option to hide specific people or photos. Once you do, simply regenerate for an updated version. Create and share with your favorite apps You can get even more creative with a new integration: Just click the “Edit with CapCut” button at the end of your Recap to easily export it to CapCut, where you can choose from exclusive Google Photos templates so you can make a version that’s uniquely yours. Sharing your favorite moments from your Recap is also easier. At the end of your Recap, you’ll find a new carousel of short videos and collages from your lookback that are made for sharing on social or to the group chat. There’s also a new option to share your Recap directly to your WhatsApp Status. Relive your Recap all month long You can find your Recap in the Memories carousel starting this week. If it’s not there, you may see an option at the top of the app where you can request for Photos to create it. After you’ve watched your Recap, it will remain in the back of your Memories carousel and will be pinned in your Collections tab for all of December. You’ll also see a series of related highlights focused on 2025 in your Memories carousel throughout the month of December, featuring some more of your top moments from the year. Open the Google Photos app to explore, customize and share your 2025 Recap. We're already seeing some of the year's biggest stars giving it a try — check it out! POSTED IN: Photos AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/photos/google-photos-2025-recap/</guid>
    </item>
    <item>
      <title>We’re announcing new health AI funding, while a new report signals a turning point for health in Europe.</title>
      <link>https://blog.google/technology/health/ai-health-fund-eu/</link>
      <description><![CDATA[At the European Health Summit in Brussels, Greg Corrado, Distinguished Scientist at Google, released a new report authored by Implement Consulting Group and commissioned by Google revealing that AI is reversing the long-term trend of slowing scientific productivity, providing a turning point for a European healthcare system grappling with rising costs and workforce shortages. The report highlights how AI is already giving practitioners "time back," such as cutting emergency room wait times by over an hour. Building on this momentum, Google announced at the Summit $5 million in funding from Google.org for Bayes Impact to launch "Impulse Healthcare," a new EU health initiative. The project will empower frontline nurses, doctors, and administrators to build and experiment their own AI solutions on Bayes Impact's open-source platform. The end goal is to scale these practitioner-led innovations across the EU, freeing up precious clinical time to improve patient care.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/health/ai-health-fund-eu/</guid>
    </item>
    <item>
      <title>We’re celebrating Geoffrey Hinton’s Nobel-winning legacy with the University of Toronto.</title>
      <link>https://blog.google/technology/ai/hinton-chair-toronto/</link>
      <description><![CDATA[Today, we are celebrating the extraordinary impact of Nobel Prize-winner Geoffrey Hinton by investing in the future of the field he helped build. Google is proud to support the University of Toronto with $10 million CAD to establish the Hinton Chair in Artificial Intelligence. Geoff’s work on neural networks — spanning his time in academia and his decade here at Google — laid the foundation for modern AI. This chair honors his legacy and will help the university recruit visionary scholars dedicated to the same kind of curiosity-driven, fundamental research that Geoff championed. We are proud to support the next generation of breakthrough innovations and research at University of Toronto — a global leader in AI research.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/hinton-chair-toronto/</guid>
    </item>
    <item>
      <title>Use Circle to Search and Google Lens to spot scam messages.</title>
      <link>https://blog.google/products/search/scam-detection-circle-to-search-lens/</link>
      <description><![CDATA[One trending tactic among scammers involves sending fraudulent text messages, either directly to your phone or through messaging apps and social media sites. These messages often solicit or demand money and link out to scammy sites. To help you spot these scams, we’ve now added new capabilities to Circle to Search and Lens that will help you see the telltale signs so you can avoid getting deceived. Here’s how to use it: To use this with Circle to Search : Simply long press the home button or navigation bar of your Android device Circle the suspicious text This capability is also available through Lens , via the Google app ( Android and iOS ) with three short steps: Take a screenshot of the message Open Lens in the Google app Tap the screenshot After you follow these steps on your device, our systems will use AI and information from the web to assess whether the message is likely a scam. You’ll see an overview with guidance and insights including suggested next steps. This capability is available globally in Circle to Search and Lens, and will appear when our systems have high confidence in the quality of the response.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 02 Dec 2025 19:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/scam-detection-circle-to-search-lens/</guid>
    </item>
    <item>
      <title>Gemini 3 and Nano Banana Pro in Search are coming to more countries around the world.</title>
      <link>https://blog.google/products/search/gemini-3-ai-mode-more-countries/</link>
      <description><![CDATA[We're bringing our most intelligent model, Gemini 3 , to AI Mode in Google Search in nearly 120 countries and territories in English. Starting today, Google AI Pro and Ultra subscribers can begin using Gemini 3 Pro by tapping “Thinking with 3 Pro” in the model drop-down in AI Mode. Thanks to Gemini 3’s state-of-the-art reasoning capabilities, AI Mode can grasp the nuance of your most complex queries. And Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities are unlocking new generative user interfaces – so you can get dynamic visual layouts, interactive tools and custom simulations — all generated on the fly, specifically for your query. We're also bringing our latest generative imagery model, Nano Banana Pro , to AI Mode in more countries in English, starting today with Google AI Pro and Ultra subscribers. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s advanced reasoning and connects to Search's vast knowledge base to help you visualize infographics and more. With this expansion, it's now easier to ask anything and instantly get a richer, more helpful understanding.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 01 Dec 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-3-ai-mode-more-countries/</guid>
    </item>
    <item>
      <title>Here’s how researchers in Asia-Pacific are using AlphaFold</title>
      <link>https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</link>
      <description><![CDATA[Proteins are the microscopic machines that drive nearly every biological process, from how our bodies fight disease to how life itself evolves. Their function is defined by their unique 3D shape, but for decades, mapping that shape was a slow and expensive process. Five years ago, we introduced AlphaFold, an AI system that accurately predicts protein structures based solely on their amino acid sequences, and made it freely available to the world. Today, AlphaFold is used by over three million researchers, with more than a third of them in the Asia-Pacific region alone. From tackling deadly diseases to discovering entirely new forms of life, here are five ways researchers in this region have used AlphaFold to make breakthroughs in medicine and science: 1. Fighting a ‘silent killer’ in Malaysia Melioidosis, a disease caused by the bacterium Burkholderia pseudomallei — most commonly contracted through contaminated soil or water — claims nearly 90,000 lives each year. At the National University of Malaysia, Dr. Su Datt Lam’s team is using AlphaFold to understand how the bacterium’s proteins help it survive and spread , accelerating the development of new drugs to combat this "silent killer." 2. Uncovering new clues to Parkinson’s disease in Singapore Researchers Jackwee Lim and Yinxia Chao at Singapore’s Agency for Science, Technology and Research (A*STAR) and National Neuroscience Institute (NNI) used AlphaFold to create a 3D visualization of a protein linked to Parkinson's, a neurodegenerative disease. This revealed how the body's own immune system can disrupt the protein’s function, opening new paths for earlier diagnosis and targeted therapies. 3. Seeing the invisible drivers of disease in Korea At Korea’s Advanced Institute of Science & Technology (KAIST), Professor Ji-Joon Song’s team studies how disruptions in the way DNA is organized can lead to cancer and other diseases. AlphaFold allowed his team to map previously unseen regions of a key protein, uncovering a hidden interaction site. As he puts it, "AlphaFold is like the internet for structural biology." 4. Discovering a new frontier of protein shapes in Taiwan Dr. Danny Hsu’s team at Academia Sinica used AlphaFold to study a protein of unknown structure, and predicted a "71-torus knot" — an exceptionally complex protein fold. The team later confirmed this prediction in the lab, proving AlphaFold can help scientists discover entirely new phenomena. 5. Finding new life in Japan’s hot springs While studying microbes in the hot springs of Japan, Dr. Syun-ichi Urayama’s team discovered unusual viruses . Using AlphaFold to predict their protein structures, the team confirmed they belonged to a widespread, previously unknown family of life, revealing a new branch of molecular evolution. These five stories from the Asia-Pacific region — where we have over 13,000 research papers citing AlphaFold — represent just a fraction of what scientists have discovered since we introduced the technology. From tackling neglected diseases to revealing evolutionary history, AlphaFold has fundamentally changed biological research, empowering a global community of scientists to accelerate their work and tackle humanity's greatest challenges. Learn more about AlphaFold — including the co-award of the 2024 Nobel Prize in Chemistry to Demis Hassabis and John Jumper for its development — on the Google Deepmind blog . POSTED IN: Google in Asia AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 26 Nov 2025 23:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</guid>
    </item>
    <item>
      <title>Get an in-depth look at Gemini 3 with CEO Sundar Pichai.</title>
      <link>https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</link>
      <description><![CDATA[For the latest episode of the Google AI: Release Notes podcast, host Logan Kilpatrick sat down with CEO Sundar Pichai last week to discuss the extraordinary progress — and future — of AI at Google. Sundar shared the thinking behind going “AI-first” all the way back in 2016, setting up key investments that led the company to this moment. And beyond current releases like Gemini 3 and Nano Banana Pro, he also shared his excitement about long-term bets for the next decade, like quantum computing. "I think in about five years we'll be having breathless excitement about quantum, hopefully, like we are having with AI today,” he said. Watch the full conversation below, or listen to the Google AI: Release Notes podcast on YouTube , Apple Podcasts or Spotify .]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 23:39:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</guid>
    </item>
  </channel>
</rss>
