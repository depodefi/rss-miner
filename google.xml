<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Fri, 09 Jan 2026 21:19:08 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>2025 at Google</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/look-back-2025/</link>
      <description><![CDATA[It’s (almost) a wrap on 2025! As we prepare for a great new year, let’s take a quick rewind and remember some of this past year’s most exciting launches, biggest moments and more.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/look-back-2025/</guid>
    </item>
    <item>
      <title>The Google guide for holiday help</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/holiday-planning-ai-gemini-tips/</link>
      <description><![CDATA[Check out our tips, trends and more for tackling any seasonal stress. Offload tedious tasks to Gemini, get insider recommendations from Google Maps and be sure you’re getting the best price when shopping on Google. Get the help you need — and then get back to enjoying the holiday festivities.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/holiday-planning-ai-gemini-tips/</guid>
    </item>
    <item>
      <title>Investing in America 2025</title>
      <link>https://blog.google/company-news/inside-google/company-announcements/investing-in-america-2025/</link>
      <description><![CDATA[Google's investments across the U.S are helping enable this extraordinary time for American innovation. Through major investments in technical infrastructure, research and development — along with expanded energy capacity for an AI-driven economy and workforce development and education programs — we will help the U.S. continue to lead the world in AI. These investments also unlock substantial economic opportunity for American businesses — advancing scientific breakthroughs, fortifying cybersecurity for the U.S., and creating new career opportunities for millions of Americans. In recent months, we’ve been bringing this new era of American innovation to life in communities across the country, with more to come.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 17 Nov 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/inside-google/company-announcements/investing-in-america-2025/</guid>
    </item>
    <item>
      <title>I/O 2025</title>
      <link>https://blog.google/innovation-and-ai/technology/developers-tools/google-io-2025-collection/</link>
      <description><![CDATA[At Google I/O 2025, our annual developer conference, we shared how we’re using our cutting-edge technology to build products that are intelligent and personalized — and that can take action for you. From advancements in our Gemini 2.5 models to rolling out AI Mode to Search for everyone in the U.S., we’re bringing innovative AI to our products to make them even more helpful. Read on to find out what’s new.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 20 May 2025 17:45:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/developers-tools/google-io-2025-collection/</guid>
    </item>
    <item>
      <title>Google Cloud Next 25</title>
      <link>https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/next-2025/</link>
      <description><![CDATA[AI is transforming the way we work — from boosting productivity and creativity to delivering real business transformation and impact. And at Google Cloud, we’re bringing the best of Google AI to people, organizations and businesses across the globe. Today, there are over four million developers building with the power of Gemini, our most advanced AI model family. This rapid adoption of Gemini, Imagen (our groundbreaking image generation model), and Veo (our industry-leading video generation model) has resulted in a 20x increase in Vertex AI usage in the past year alone. And within Google Workspace, over two billion AI assists are provided monthly to business users, reshaping how work gets done. We’re continuing to push what Google AI can do, making it more powerful, easier to use and more affordable. This week at Google Cloud Next 25, we shared exciting updates about how we’re doing just that: From introducing the most powerful chip we’ve ever built, to providing support for even more generative media models, to helping organizations create and manage AI agents, to changing how teams work with new capabilities in Google Workspace and Google Agentspace. We also shared over 500 examples of how organizations are using Google AI and seeing real impact. Here are some of the highlights of what we announced: - Ironwood, our 7th-generation TPU built for inference, will be available later this year. Compared to the prior generation, Ironwood offers five times more peak compute capacity and six times the high-bandwidth memory capacity. - With the addition of Lyria to Vertex AI, we are now the only platform with generative media models for video, image, speech and music. - New updates and tools for Gemini in Workspace bring even more helpful AI capabilities into tools people use every day — Docs, Sheets, Meet, Chat and more. - Updates to Agentspace make it easier for customers to discover, create and adopt AI agents. We're also growing the AI Agent Marketplace , a dedicated section within Google Cloud Marketplace where customers can easily browse and purchase AI agents from partners. . - And we unveiled more tools to build helpful agents including Agent Development Kit (ADK), an open-source framework for building agents while maintaining control over agent behavior; and Agent2Agent (A2A), new open protocol that gives your agents a common language to collaborate no matter what framework or vendor they are built on. - Gemini 2.5 Flash, our workhorse model with low latency and cost efficiency, will soon be available in Vertex AI. - Google Unified Security brings our best-in-class security products for threat intelligence, security operations, cloud security and secure enterprise browsing into a new, single AI-powered security solution. - With Cloud Wide Area Network (Cloud WAN), we’re making our high-speed, low-latency network — the same one that connects billions of users to services like Gmail, Photos and Search — available to organizations around the world.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 09 Apr 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/next-2025/</guid>
    </item>
    <item>
      <title>The Check Up with Google</title>
      <link>https://blog.google/innovation-and-ai/technology/health/google-health-check-up-2025/</link>
      <description><![CDATA[AI can lead to scientific progress and cutting-edge products that help improve health outcomes for people all around the world. At Google’s annual health event, The Check Up, we shared how our products, research and partnerships are making the most of AI with the goal of helping everyone, everywhere live healthier lives. Dr. Karen DeSalvo Chief Health Officer, Google]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 18 Mar 2025 13:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/health/google-health-check-up-2025/</guid>
    </item>
    <item>
      <title>New AI features and more for Android and Pixel</title>
      <link>https://blog.google/products-and-platforms/devices/pixel/pixel-android-ai-updates-december-2024/</link>
      <description><![CDATA[Our latest updates for Android and Pixel are packed with tons of AI-powered features — including Expressive Captions, an all-new feature that brings more feeling to captions, Gemini’s saved info, which remembers important information for you, and updates for Call Screen so it’s even easier to respond. Check out all the updates below.]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 05 Dec 2024 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products-and-platforms/devices/pixel/pixel-android-ai-updates-december-2024/</guid>
    </item>
    <item>
      <title>The AI for Science Forum: A new era of discovery</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/ai-science-forum-2024/</link>
      <description><![CDATA[AI is revolutionizing the landscape of scientific research, enabling advancements at a pace that was once unimaginable — from accelerating drug discovery to designing new materials for clean energy technologies. The AI for Science Forum — co-hosted by Google DeepMind and the Royal Society — brought together the scientific community, policymakers, and industry leaders to explore the transformative potential of AI to drive scientific breakthroughs, address the world's most pressing challenges, and lead to a new era of discovery.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 18 Nov 2024 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/ai-science-forum-2024/</guid>
    </item>
    <item>
      <title>2023 at Google</title>
      <link>https://blog.google/company-news/inside-google/google-2023-recaps-highlights/</link>
      <description><![CDATA[From launching Gemini to celebrating our 25th birthday, 2023 was a busy and exciting year at Google. Explore the stories below to learn more about some of the biggest launches, moments and highlights of the year. Cheers to 2023 — we're ready for you, 2024.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 20 Dec 2023 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/company-news/inside-google/google-2023-recaps-highlights/</guid>
    </item>
    <item>
      <title>Learn more about Gemini, our most capable AI model</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/gemini-collection/</link>
      <description><![CDATA[Today we introduced Gemini , our largest and most capable AI model — and the next step on our journey toward making AI helpful for everyone. Built from the ground up to be multimodal , Gemini can generalize and seamlessly understand, operate across and combine different types of information, including text, images, audio, video and code. This means it has sophisticated multimodal reasoning and advanced coding capabilities. And with three different sizes — Ultra, Pro and Nano — Gemini has the flexibility to run on everything from data centers to mobile devices. We trained Gemini at scale on our AI-optimized infrastructure using Google's Tensor Processing Units (TPUs) v4 and v5e. Today, we also announced our most powerful and scalable TPU system to date, Cloud TPU v5p . Gemini is available in some of our core products starting today: Bard is using a fine-tuned version of Gemini Pro for more advanced reasoning, planning, understanding and more. Pixel 8 Pro is the first smartphone engineered for Gemini Nano, using it in features like Summarize in Recorder and Smart Reply in Gboard. And we’re already starting to experiment with Gemini in Search, where it's making our Search Generative Experience (SGE) faster. Early next year, we’ll bring Gemini Ultra to a new Bard Advanced experience. And in the coming months, Gemini will power features in more of our products and services like Ads, Chrome and Duet AI. Android developers who want to build Gemini-powered apps on-device can now sign up for an early preview of Gemini Nano, our most efficient model, via Android AICore. Starting December 13, developers and enterprise customers will be able to access Gemini Pro via the Gemini API in Vertex AI or Google AI Studio, our free web-based developer tool. And as we continue to refine Gemini Ultra, including completing extensive trust and safety checks, we’ll make it available to select groups before opening it up broadly to developers and enterprise customers early next year. Explore the collection to learn more about our newest model, and the start of the Gemini era.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 06 Dec 2023 15:15:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/gemini-collection/</guid>
    </item>
    <item>
      <title>What our quantum computing milestone means</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/what-our-quantum-computing-milestone-means/</link>
      <description><![CDATA[Today, Nature published its 150th anniversary issue with the news that Google’s team of researchers have achieved a big breakthrough in quantum computing known as quantum supremacy. It’s a term of art that means we’ve used a quantum computer to solve a problem that would take a classical computer an impractically long amount of time. This moment represents a distinct milestone in our effort to harness the principles of quantum mechanics to solve computational problems. While we’re excited for what’s ahead, we are also very humbled by the journey it took to get here. And we’re mindful of the wisdom left to us by the great Nobel Laureate Richard Feynman: “If you think you understand quantum mechanics, you don't understand quantum mechanics.” In many ways, the exercise of building a quantum computer is one long lesson in everything we don’t yet understand about the world around us. While the universe operates fundamentally at a quantum level, human beings don’t experience it that way. In fact many principles of quantum mechanics directly contradict our surface level observations about nature. Yet the properties of quantum mechanics hold enormous potential for computing. A bit in a classical computer can store information as a 0 or 1. A quantum bit—or qubit—can be both 0 and 1 at the same time, a property called superposition. So if you have two quantum bits, there are four possible states that you can put in superposition, and those grow exponentially. With 333 qubits there are 2^333, or 1.7x10^100—a Googol—computational states you can put in superposition, allowing a quantum computer to simultaneously explore a rich space of many possible solutions to a problem. As we scale up the computational possibilities, we unlock new computations. To demonstrate supremacy, our quantum machine successfully performed a test computation in just 200 seconds that would have taken the best known algorithms in the most powerful supercomputers thousands of years to accomplish. We are able to achieve these enormous speeds only because of the quality of control we have over the qubits. Quantum computers are prone to errors, yet our experiment showed the ability to perform a computation with few enough errors at a large enough scale to outperform a classical computer. For those of us working in science and technology, it’s the “hello world” moment we’ve been waiting for—the most meaningful milestone to date in the quest to make quantum computing a reality. But we have a long way to go between today’s lab experiments and tomorrow’s practical applications; it will be many years before we can implement a broader set of real-world applications. We can think about today’s news in the context of building the first rocket that successfully left Earth’s gravity to touch the edge of space. At the time, some asked: Why go into space without getting anywhere useful? But it was a big first for science because it allowed humans to envision a totally different realm of travel … to the moon, to Mars, to galaxies beyond our own. It showed us what was possible and nudged the seemingly impossible into frame. That’s what this milestone represents for the world of quantum computing: a moment of possibility. It’s been a 13-year journey for Google to get here. In 2006, Google scientist Hartmut Neven started exploring the idea of how quantum computing might help our efforts to accelerate machine learning. This work led to the founding of our Google AI Quantum team, and in 2014, John Martinis and his team at the University of California at Santa Barbara joined us in our efforts to build a quantum computer. Two years later, Sergio Boixo published a paper that focused our efforts around the well-defined computational task of quantum supremacy, and now the team has built the world’s first quantum system that exceeds the capabilities of supercomputers for this particular computation. We made these early bets because we believed—and still do—that quantum computing can accelerate solutions for some of the world's most pressing problems, from climate change to disease. Given that nature behaves quantum mechanically, quantum computing gives us the best possible chance of understanding and simulating the natural world at the molecular level. With this breakthrough we’re now one step closer to applying quantum computing to—for example—design more efficient batteries, create fertilizer using less energy, and figure out what molecules might make effective medicines. Those applications are still many years away and we are committed to building the error-corrected quantum computer that will power these discoveries. We’ve always known that it would be a marathon, not a sprint. The thing about building something that hasn’t been proven yet is that there is no playbook. If the team needed a part, they had to invent it and build it themselves. And if it didn’t work—and often, it didn’t—they had to redesign and build it again. One turning point came in October 2018, when the wildfires were raging in Southern California. I got a message that they would need to close down the Santa Barbara laboratory for a few days out of an abundance of caution. What I didn’t know was that the team had been experiencing one of those periods where progress had slowed to a crawl. The few days of forced vacation helped the team to reset and think about things differently, and a few months later, they made this breakthrough. As with any advanced technology, quantum computing raises its own anxieties and questions. In thinking through these issues, we’re following a set of AI principles that we developed to help guide responsible innovation of advanced technology. For example, for many years the security community, with contributions from Google, has been working on post-quantum cryptography, and we’re optimistic we are ahead of the curve when it comes to future encryption concerns. We will continue to publish research and help the broader community develop quantum encryption algorithms using our open source framework Cirq . We’ve appreciated the National Science Foundation’s support for our researchers, and we’ve collaborated with NASA Ames and Oak Ridge National Laboratory on this latest result. As was the case with the Internet and machine learning, government support of basic research remains critical to long-term scientific and technological achievement. I am excited about what quantum computing means for the future of Google and the world. Part of that optimism comes from the nature of the technology itself. You can trace the progress from the mega-computers of the 1950s to advances we’re making in artificial intelligence today to help people in their everyday lives. Quantum computing will be a great complement to the work we do (and will continue to do) on classical computers. In many ways quantum brings computing full circle, giving us another way to speak the language of the universe and understand the world and humanity not just in 1s and 0s but in all of its states: beautiful, complex, and with limitless possibility. POSTED IN: AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 23 Oct 2019 09:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/what-our-quantum-computing-milestone-means/</guid>
    </item>
    <item>
      <title>How to make AI that’s good for people</title>
      <link>https://blog.google/innovation-and-ai/technology/ai/how-make-ai-good-for-people/</link>
      <description><![CDATA[For a field that was not well known outside of academia a decade ago, artificial intelligence has grown dizzyingly fast. Tech companies from Silicon Valley to Beijing are betting everything on it, venture capitalists are pouring billions into research and development, and start-ups are being created on what seems like a daily basis. If our era is the next Industrial Revolution, as many claim, AI is surely one of its driving forces. It is an especially exciting time for a researcher like me. When I was a graduate student in computer science in the early 2000s, computers were barely able to detect sharp edges in photographs, let alone recognize something as loosely defined as a human face. But thanks to the growth of big data, advances in algorithms like neural networks and an abundance of powerful computer hardware, something momentous has occurred: AI has gone from an academic niche to the leading differentiator in a wide range of industries, including manufacturing, health care, transportation and retail. I worry, however, that enthusiasm for AI is preventing us from reckoning with its looming effects on society. Despite its name, there is nothing “artificial” about this technology—it is made by humans, intended to behave like humans and affects humans. So if we want it to play a positive role in tomorrow’s world, it must be guided by human concerns. I call this approach “human-centered AI.” It consists of three goals that can help responsibly guide the development of intelligent machines. First, AI needs to reflect more of the depth that characterizes our own intelligence. Consider the richness of human visual perception. It’s complex and deeply contextual, and naturally balances our awareness of the obvious with a sensitivity to nuance. By comparison, machine perception remains strikingly narrow. Sometimes this difference is trivial. For instance, in my lab, an image-captioning algorithm once fairly summarized a photo as “a man riding a horse” but failed to note the fact that both were bronze sculptures. Other times, the difference is more profound, as when the same algorithm described an image of zebras grazing on a savanna beneath a rainbow. While the summary was technically correct, it was entirely devoid of aesthetic awareness, failing to detect any of the vibrancy or depth a human would naturally appreciate. That may seem like a subjective or inconsequential critique, but it points to a major aspect of human perception beyond the grasp of our algorithms. How can we expect machines to anticipate our needs—much less contribute to our well-being—without insight into these “fuzzier” dimensions of our experience? Making AI more sensitive to the full scope of human thought is no simple task. The solutions are likely to require insights derived from fields beyond computer science, which means programmers will have to learn to collaborate more often with experts in other domains. Such collaboration would represent a return to the roots of our field, not a departure from it. Younger AI enthusiasts may be surprised to learn that the principles of today’s deep-learning algorithms stretch back more than 60 years to the neuroscientific researchers David Hubel and Torsten Wiesel, who discovered how the hierarchy of neurons in a cat’s visual cortex responds to stimuli. Likewise, ImageNet, a data set of millions of training photographs that helped to advance computer vision, is based on a project called WordNet, created in 1995 by the cognitive scientist and linguist George Miller. WordNet was intended to organize the semantic concepts of English. Reconnecting AI with fields like cognitive science, psychology and even sociology will give us a far richer foundation on which to base the development of machine intelligence. And we can expect the resulting technology to collaborate and communicate more naturally, which will help us approach the second goal of human-centered AI: enhancing us, not replacing us. Imagine the role that AI might play during surgery. The goal need not be to automate the process entirely. Instead, a combination of smart software and specialized hardware could help surgeons focus on their strengths—traits like dexterity and adaptability—while keeping tabs on more mundane tasks and protecting against human error, fatigue and distraction. Or consider senior care. Robots may never be the ideal custodians of the elderly, but intelligent sensors are already showing promise in helping human caretakers focus more on their relationships with those they provide care for by automatically monitoring drug dosages and going through safety checklists. These are examples of a trend toward automating those elements of jobs that are repetitive, error-prone and even dangerous. What’s left are the creative, intellectual and emotional roles for which humans are still best suited. No amount of ingenuity, however, will fully eliminate the threat of job displacement. Addressing this concern is the third goal of human-centered AI: ensuring that the development of this technology is guided, at each step, by concern for its effect on humans. Today’s anxieties over labor are just the start. Additional pitfalls include bias against underrepresented communities in machine learning, the tension between AI’s appetite for data and the privacy rights of individuals and the geopolitical implications of a global intelligence race. Adequately facing these challenges will require commitments from many of our largest institutions. Universities are uniquely positioned to foster connections between computer science and traditionally unrelated departments like the social sciences and even humanities, through interdisciplinary projects, courses and seminars. Governments can make a greater effort to encourage computer science education, especially among young girls, racial minorities and other groups whose perspectives have been underrepresented in AI. And corporations should combine their aggressive investment in intelligent algorithms with ethical AI policies that temper ambition with responsibility. No technology is more reflective of its creators than AI. It has been said that there are no “machine” values at all, in fact; machine values are human values. A human-centered approach to AI means these machines don’t have to be our competitors, but partners in securing our well-being. However autonomous our technology becomes, its impact on the world—for better or worse—will always be our responsibility. This article was originally published in the New York Times . POSTED IN: AI]]></description>
      <author>Google AI</author>
      <pubDate>Sat, 07 Apr 2018 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/innovation-and-ai/technology/ai/how-make-ai-good-for-people/</guid>
    </item>
  </channel>
</rss>
