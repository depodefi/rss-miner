<?xml version="1.0" ?>
<?xml-stylesheet type="text/xsl" href="style.xsl"?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Wed, 24 Dec 2025 10:04:28 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Google's year in review: 8 areas with research breakthroughs in 2025</title>
      <link>https://blog.google/technology/ai/2025-research-breakthroughs/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X 2025 has been a year of extraordinary progress in research. With artificial intelligence, we can see its trajectory shifting from a tool to a utility: from something people use to something they can put to work. If 2024 was about laying the multimodal foundations for this era, 2025 was the year AI began to really think, act and explore the world alongside us. With quantum computing, we made progress towards real-world applications. And across the board, we helped turn research into reality, with more capable and useful products and tools making a positive impact on people's lives today. Here’s a look back at some of the breakthroughs, products and scientific milestones that defined the work of Google, Google DeepMind and Google Research in a year of relentless progress . Delivering breakthroughs on world-class models This year, we significantly advanced our model capabilities with breakthroughs on reasoning, multimodal understanding, model efficiency, and generative capabilities, beginning with the release of Gemini 2.5 in March and culminating in the November launch of Gemini 3 and the December launch of Gemini 3 Flash. Built on a foundation of state-of-the-art reasoning, Gemini 3 Pro is our most powerful model to date, designed to help you bring any idea to life. It topped the LMArena Leaderboard and redefined multimodal reasoning with breakthrough scores on benchmarks like Humanity’s Last Exam — a fiendishly hard test for AI models to see if AI can truly think and reason like humans — and GPQA Diamond. It also set a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex. We followed shortly with Gemini 3 Flash, which combines Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost, making it the most performant model for its size. Gemini 3 Flash's quality surpasses our previous Gemini 2.5 Pro-scale model's capabilities at a fraction of the price and substantially better latency, continuing our Gemini-era trend of 'the next generation's Flash model is better than the previous generation's Pro model'. Learn more about our progress on our world-class AI models this year: Gemini 3 Flash: frontier intelligence built for speed (Dec 2025) A new era of intelligence with Gemini 3 (Nov 2025) Introducing Nano Banana Pro (Nov 2025) Introducing Veo 3.1 and new creative capabilities in the Gemini API (Nov 2025) Gemini 2.5: Our most intelligent AI model (March 2025) Gemini 3 Flash price & benchmark table. We’re committed to making useful AI technology accessible, with state-of-the-art open models. We built our Gemma family of models to be lightweight and open for public use; this year we were able to introduce multimodal capabilities, significantly increase the context window, expand multilingual capabilities, and improve efficiency and performance. Learn more about this year’s advances in Gemma models: Introducing Gemma 3: The most capable model you can run on a single GPU or TPU (March 2025) Introducing Gemma 3 270M: The compact model for hyper-efficient AI (Aug 2025) Innovating and transforming our products with AI Throughout 2025, we continued to advance the trajectory of AI from tool to utility, transforming our portfolio of products with new, powerful agentic capabilities. We reimagined software development by moving beyond tools that assist coding to introducing powerful, agentic systems that collaborate with developers. Key advances, such as the impressive coding capabilities in Gemini 3 and the launch of Google Antigravity , mark a new era in AI-assisted software development. Learn more about this year’s advances building developer tools: Start building with Gemini 3 (Nov 2025) Introducing Google Antigravity, a New Era in AI-Assisted Software Development (Nov 2025) This evolution was also clear across our core products, from AI-enabled features on the Pixel 10 and updates to AI Mode in Search, to AI-first innovations like the Gemini app and NotebookLM, which gained advanced features like Deep Research. Learn more about how we’ve transformed our products with AI: 9 ways AI makes Pixel 10 our most helpful phone yet (Aug 2025) Expanding AI Overviews and introducing AI Mode (March 2025) Gemini 3 brings upgraded smarts and new capabilities to the Gemini app (Nov 2025) NotebookLM adds Deep Research and support for more source types (Nov 2025) Empowering creativity and co-creating with AI 2025 was a transformative year for generative media, giving people new and unprecedented capabilities to realize their creative ambitions. Generative media models and tools for video, images, audio and worlds became more effective and broadly used, with breakouts Nano Banana and Nano Banana Pro offering unprecedented capabilities for native image generation and editing. We worked with people in creative industries to develop tools like Flow and Music AI Sandbox, making them more helpful for creative workflows, and we expanded creative possibilities for people with new, AI-powered experiences in the Google Arts & Culture lab, major upgrades to image editing within the Gemini app, and the introduction of powerful new generative media models like Veo 3.1, Imagen 4 and Flow. Learn more about how we’re building AI to enhance creativity: Art, science, travel: 3 new AI-powered experiences this holiday season (Nov 2025) Introducing Veo 3.1 and advanced capabilities in Flow (Oct 2025) Nano Banana: Image editing in Gemini just got a major upgrade (Aug 2025) Veo 3, Imagen 4, and Flow: Fuel your creativity with new generative media models and tools (May 2025) Music AI Sandbox, now with new features and broader access (April 2025) As research breakthroughs continue to expand AI’s capabilities, Google Labs is where we share AI experiments as we develop them – hearing from users and evolving as we learn. Some of this year’s most engaging experiments from Labs: Pomelli, an AI experiment for on-brand marketing content; Stitch, which introduced a way to turn prompt and image inputs into complex UI designs and frontend code in minutes; Jules, an asynchronous coding agent that acts as a collaborative partner for developers; and Google Beam, a 3D video communications platform that used AI to advance the possibilities of remote presence. Learn more about how we’re experimenting in Labs: Create on-brand marketing content for your business with Pomelli (Oct 2025) Google Beam: Our AI-first 3D video communication platform (May 2025) From idea to app: Introducing Stitch, a new way to design UIs (May 2025) Build with Jules, your asynchronous coding agent (May 2025) Advancing science and mathematics 2025 was also a banner year for scientific advances with AI, marked by breakthroughs in life sciences, health, natural sciences, and mathematics. In the space of a year, we made progress in building AI resources and tools that empower researchers and help them understand, identify, and develop treatments in healthcare. In genomics, where we’ve been applying advanced technology to research for 10 years, we moved beyond sequencing, using AI to interpret the most complex data. We also marked the 5-year anniversary of AlphaFold, the Nobel-winning AI system that solved the 50-year-old protein folding problem. AlphaFold has been used by over 3 million researchers in more than 190 countries, including over 1 million users in low- and middle-income countries. Learn more about how we’re using AI to advance life sciences and health: AlphaFold: Five years of impact (Nov 2025) Using AI to identify genetic variants in tumors with DeepSomatic (Oct 2025) AI as a research partner: Advancing theoretical computer science with AlphaEvolve (Sept 2025) AlphaGenome: AI for better understanding the genome (June 2025) Accelerating scientific breakthroughs with an AI co-scientist (Feb 2025) Gemini’s advanced thinking capabilities, including Deep Think, also enabled historic progress in mathematics and coding. Deep Think was able to solve problems that require deep abstract reasoning – achieving gold medal-standard in two international contests. Learn more about how we’re advancing natural sciences and mathematics: Gemini achieves gold-medal level at the International Collegiate Programming Contest World Finals (Sept 2025) Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad (July 2025) Shaping innovations in computing and the physical world We’re also leading major discoveries and shaping the future of science in areas like quantum computing, energy and moonshots. Research in this area drew new levels of public attention, with progress towards real-world applications of quantum computing as demonstrated by Quantum Echoes and, notably, Googler Michel Devoret becoming a 2025 Physics Nobel Laureate along with former Googler John Martinis and UC Berkeley’s John Clarke, for their foundational 1980s quantum research. Learn more about our work on space infrastructure and quantum computing: Project Suncatcher: Exploring a space-based, scalable AI infrastructure system design (Nov 2025) Googler Michel Devoret awarded the Nobel Prize in Physics (Oct 2025) Our Quantum Echoes algorithm is a big step toward real-world applications for quantum computing (Oct 2025) In 2025, we continued to advance the core infrastructure that powers our AI, focusing on breakthroughs in hardware design and improving energy efficiency. This included the introduction of Ironwood, a new TPU built for the age of inference, which was designed using a method called AlphaChip , alongside a commitment to measuring the environmental impact of our technology. Learn more about how we’re using AI to develop chips, infrastructure and improve energy efficiency: 3 things to know about Ironwood, our latest TPU (Nov 2025) How much energy does Google’s AI use? We did the math (Aug 2025) Ironwood: The first Google TPU for the age of inference (April 2025) Our work in robotics and visual understanding brought AI agents into both the physical and virtual worlds, with advancements like the foundational Gemini Robotics models, the more sophisticated Gemini Robotics 1.5, and the introduction of Genie 3 as a new frontier for general-purpose world models. Learn more about our work with world models and robotics: Gemini Robotics 1.5 brings AI agents into the physical world (Sept 2025) Genie 3: A new frontier for world models (Aug 2025) Gemini Robotics brings AI into the physical world (March 2025) Tackling global challenges and opportunities at scale Our work throughout 2025 demonstrates how AI-enabled scientific progress is being directly applied to address the world's most critical and pervasive challenges. By leveraging state-of-the-art foundational models and agentic reasoning, we are significantly increasing our understanding of the planet and its systems, while also delivering impactful solutions in areas vital to human flourishing, including climate resilience, public health and education. For example, we are using state-of-the-art foundational models and agentic reasoning to help increase our understanding of the planet, helping enable work that is making a difference in people’s lives now from weather predictions to urban planning to public health. For example, our flood forecasting information now covers more than two billion people in 150 countries for severe riverine floods. And our most advanced and efficient forecasting model, WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour. Using this technology, we’ve supported weather agencies in making decisions based on a range of scenarios through our experimental cyclone predictions. Learn more about our work in weather, mapping and wildfires: WeatherNext 2: Our most advanced weather forecasting model (Nov 2025) New updates and more access to Google Earth AI (Oct 2025) Google Earth AI: Our state-of-the-art geospatial AI models (July 2025) AlphaEarth Foundations helps map our planet in unprecedented detail (July 2025) How we're supporting better tropical cyclone prediction with AI (June 2025) Inside the launch of FireSat, a system to find wildfires earlier (March 2025) We are working with partners to apply AI-enabled scientific progress closer to patients, opening up new avenues for disease management and therapeutic discovery. Learn more about our health-related work: Cell2Sentence-Scale 27B: How a Gemma model helped discover a new potential cancer therapy pathway (Oct 2025) From diagnosis to treatment: Advancing AMIE for longitudinal disease management (March 2025) AI is proving to be a powerful tool in education, enabling new forms of understanding and expanding curiosity through initiatives like LearnLM and Guided Learning in Gemini. We brought Gemini’s most powerful translation capabilities to Google Translate, enabling much smarter, more natural and accurate translations and piloting new speech to speech translation capabilities. Learn more about how we’re using AI to enable learning: Bringing state-of-the-art Gemini translation capabilities to Google Translate (Dec 2025) Guided Learning in Gemini: From answers to understanding (Aug 2025) How generative AI expands curiosity and understanding with LearnLM (May 2025) Prioritizing responsibility and safety We couple our research breakthroughs with rigorous and forward-looking work on responsibility and safety. As our models grow more capable, we’re continuing to advance and evolve our tools, resources and safety frameworks to anticipate and mitigate risk. Gemini 3 demonstrated this approach in action: it's our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. And we’re looking further ahead, exploring a responsible path to AGI, prioritizing readiness, proactive risk assessment, and collaboration with the wider AI community. Learn more about our responsibility and safety work: You can now verify Google AI-generated videos in the Gemini app (Dec 2025) How we’re bringing AI image verification to the Gemini app (Nov 2025) Strengthening our Frontier Safety Framework (September 2025) Taking a responsible path to AGI (April 2025) Evaluating potential cybersecurity threats of advanced AI (April 2025) Leading frontier collaborations with industry, academia and civil society Advancing the frontier of AI responsibly demands collaboration across all parts of society. In 2025, we worked with leading AI labs to help to form the Agentic AI Foundation and support open standards to ensure a responsible and interoperable future for agentic AI. In education, we’ve partnered with school districts like Miami Dade County and education groups like Raspberry Pi to equip students with AI skills. Our research partnerships with universities like UC Berkeley, Yale, the University of Chicago and many more have been instrumental to some of this year’s most exciting frontier research, and we’re working with the US Department of Energy’s 17 national laboratories to transform how scientific research is conducted. And we’re working with filmmakers and other creative visionaries to put the best AI tools in their hands and explore storytelling in the age of AI. Learn more about our work on frontier collaboration: Google DeepMind supports U.S. Department of Energy on Genesis: a national mission to accelerate innovation and scientific discovery (Dec 2025) Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md (Dec 2025) Announcing Model Context Protocol (MCP) support for Google services (Dec 2025) Our latest commitments in AI and learning (Nov 2025) Partnering to power Miami’s AI-ready future (Oct 2025) AI on Screen premiere: “Sweetwater” short film explores new AI narratives (Sept 2025) Behind “ANCESTRA”: combining Veo with live-action filmmaking (Jun 2025) How Indian music legend Shankar Mahadevan experiments with Music AI Sandbox (April 2025) Looking ahead As we look towards 2026, we’re looking forward to continuing to advance the frontier, safely and responsibly, for the benefit of humanity. POSTED IN: AI Research Google DeepMind]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 23 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/2025-research-breakthroughs/</guid>
    </item>
    <item>
      <title>60 of our biggest AI announcements in 2025</title>
      <link>https://blog.google/technology/ai/google-ai-news-recap-2025/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X As 2025 comes to a close, it’s only natural to look back on our biggest AI moments of the year. And what a year it’s been — we shared hundreds of AI announcements about products and features meant to make people’s lives easier in ways big and small. Take a look back at some of our biggest AI news this year (and check out our favorite AI tips we shared this year, too.) January New year, new AI. We kicked off 2025 with a bunch of updates to Gemini, Android, Google TV, educational tools like NotebookLM and more — all to bring the benefits of AI right into your hands. Here were some of the top Google AI news stories of the month: A more powerful Android assistant with Gemini All the Android updates coming to the Samsung Galaxy S25 series and more New Google education tools for 2025 Here’s what’s new with Google TV at CES Try Gemini 2.0 Flash in the Gemini app February In February, our CEO Sundar Pichai shared that he believes AI is the most profound shift in our lifetimes. To that end, the month’s news showed how AI is making a real difference, from helping people find new jobs to helping scientists create novel hypotheses and research plans. Here were some of the top Google AI news stories of the month: Gemini 2.0 is now available to everyone Get coding help from Gemini Code Assist — now for free A new experiment to help people explore more career possibilities We’re launching a new AI system for scientists Use Lens to search your screen while you browse on iOS March AI Mode in Search debuted in March, and it’s been evolving ever since to help you find answers to your toughest questions. Other heavy AI hitters this month included the introduction of Gemini 2.5 and Gemma 3, along with a slew of Gemini updates for creativity and collaboration. Here were some of the top Google AI news stories of the month: Gemini 2.5: Our most intelligent AI model Introducing Gemma 3: The most capable model you can run on a single GPU or TPU Expanding AI Overviews and introducing AI Mode New Gemini app features, available to try at no cost New ways to collaborate and get creative with Gemini April Our April AI announcements ran the gamut: On the infrastructure side, we unveiled Ironwood, our latest TPU. And to foster creativity, we brought our video generation model Veo 2 to new products. Some language-related news — about both humans and dolphins! — put an extra spring in our April AI step. Here were some of the top Google AI news stories of the month: Generate videos in Gemini and Whisk with Veo 2 Little Language Lessons uses generative AI to make practicing languages more personal DolphinGemma: How Google AI is helping decode dolphin communication Ironwood: The first Google TPU for the age of inference 6 highlights from Google Cloud Next 25 May The return of Google I/O meant that many of the month’s biggest announcements stemmed from our annual developer conference. We explained how we’re making AI more helpful with Gemini, welcomed our AI filmmaking tool, Flow, to the stage and shared plenty more exciting updates across a ton of products. Here were some of the top Google AI news stories of the month (and as a bonus, here are 100 things we announced at I/O 2025 ). Google I/O 2025: From research to reality Meet Flow: AI-powered filmmaking with Veo 3 Introducing Google AI Ultra: The best of Google AI in one subscription Shop with AI Mode, use AI to buy and try clothes on yourself virtually AI in Search: Going beyond information to intelligence June In June, we dove into updates to help people more easily build and create with Gemini, find information with Search Live and use technology more naturally day-to-day with Android. Here were some of the top Google AI news stories of the month: Gemini CLI: your open-source AI agent Android 16 is here Search Live: Talk, listen and explore in real time with AI Mode We’re expanding our Gemini 2.5 family of models Advanced audio dialog and generation with Gemini 2.5 July July hinted at the many different ways Googlers are bringing AI to products and tools meant to make people’s lives easier and more productive. Animating images with a new photo-to-video capability in Gemini? Sure! Unsubscribing from emails with Gmail's new "Manage subscriptions" feature? Yes, please. Here were some of the top Google AI news stories of the month: Turn your photos into videos in Gemini More advanced AI capabilities are coming to Search New ways to learn and explore with AI Mode in Search Declutter your inbox with Gmail’s newest feature We've got a surprise Pixel Drop for you August August tends to be a big month for hardware at Google, and this year was no exception thanks to our latest Pixel lineup. We also officially unveiled Nano Banana, our viral image editing model from Google DeepMind. Here were some of the top Google AI news stories of the month: Powerful and proactive: Pixel 10 phones are here Image editing in Gemini just got a major upgrade Bringing the best of AI to college students for free Try Deep Think in the Gemini app Flight Deals is our new, AI-powered flight search tool September Chrome took center stage in September as we made many new updates to help you get things done faster, easier and more safely than ever with new AI features. We also shared Pixel and Android drops, plus the official launch of Search Live. Here were some of the top Google AI news stories of the month: Go behind the browser with Chrome’s new AI features Chrome: The browser you love, reimagined with AI 5 ways to get real-time help by going Live with Search New on Android: Write smarter, share quicker and remix emoji Material 3 Expressive updates and more in the latest Pixel Drop October In October, we announced a specialized model built on Gemini 2.5 Pro’s capabilities to power agents that can interact with user interfaces. We also shared updates to generative AI tools, Google Home and Nano Banana. Here were some of the top Google AI news stories of the month: Introducing the Gemini 2.5 Computer Use model Introducing Veo 3.1 and advanced capabilities in Flow Our newest Google Home devices are built for Gemini How a Gemma model helped discover a new potential cancer therapy pathway Nano Banana is coming to Google Search, NotebookLM and Photos November With the announcement of Gemini 3, November marked one of our biggest news moments of the year. Although many of the month’s top announcements were Gemini 3-related, we were hard at work elsewhere, too — like Google Maps. Here were some of the top Google AI news stories of the month: A new era of intelligence with Gemini 3 Introducing Nano Banana Pro Start building with Gemini 3 Gemini 3 brings upgraded smarts and new capabilities to the Gemini app Google Maps navigation gets a powerful boost with Gemini December We didn’t let up as we rounded the bend of 2025. Instead, we squeezed even more AI news into an already jam-packed year with the introduction of Gemini 3 Flash, our annual Year in Search release and still more announcements to round us out. Here were some of the top Google AI news stories of the month: Gemini 3 Flash: frontier intelligence built for speed Gemini 3 Flash is rolling out globally in Google Search Year in Search 2025: What and how we searched this year Stay organized and express yourself with Android 16’s new updates Bringing state-of-the-art Gemini translation capabilities to Google Translate 2025 was a banner year for AI at Google, with launches and updates spanning a ton of products. Even though the topics ran the gamut, our theme remained making AI helpful for everyone — and there’s even more to come in 2026. POSTED IN: AI Gemini Search Pixel Android Shopping Maps Google Workspace Google Labs Google Nest Google Cloud Chrome]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 22 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-ai-news-recap-2025/</guid>
    </item>
    <item>
      <title>40 of our most helpful AI tips from 2025</title>
      <link>https://blog.google/technology/ai/ai-tips-2025/</link>
      <description><![CDATA[This year was full of AI milestones at Google: multiple Gemini models, new Search capabilities, photo and video creation tools, research and learning apps and so much more. That means we also shared plenty of tips on how to use all of these products and features. Here are our favorite AI tips from 2025: Dive deeper into what interests you. Break down technical or scientific topics with visualizations. Gemini 3 is our newest and most intelligent model, and you can access it in the Gemini app, AI Mode in Search , AI Studio and more. It’s great at breaking down complicated topics: It can take any kind of input you give it — from text to video to code and beyond — and help you learn in ways that make sense for you, like with an interactive guide based on a dense research paper. 2. Get interactive simulations and more in Search with Gemini 3. If you're researching something like mortgage loans, Gemini 3 in AI Mode can make you a custom-built interactive loan calculator, creating bespoke generative user interfaces directly in the response so you can compare two different options and see which offers the most long-term savings. This is made possible by Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities. 3. Choose your research style with NotebookLM. You have options when you’re working in NotebookLM : Choose the “Fast Research” option when you need a quick search. It rapidly scans for information, letting you immediately review and import sources. Choose “Deep Research” when you need a full briefing. It performs an in-depth analysis to find high-quality sources. This runs in the background so you can continue your work. 4. Get homework help with Guided Learning. Guided Learning in the Gemini app is an interactive study partner that helps you build a deeper understanding of any topic. It can generate study guides from uploaded course material, walk you through debugging problematic code or simply explain something, like how gravity works on the moon, with helpful videos and images. It can even do your homework with you (but not for you!). 5. Test your knowledge. NotebookLM can make flashcards and quizzes based on your sources for even more help learning any topic. 6. Understand complex topics while you’re browsing. Simplify is a Google app feature for iOS that uses AI to make dense text on the web easier to understand — without leaving a web page. To use it, select any complex text on a webpage you’re visiting in the Google app. Tap the “Simplify” icon that appears, and you’ll see a new, simpler version of the text. Explore the world easily. 7. Get help from Gemini while you’re behind the wheel. You can get help with multi-step questions and tasks , like: “Is there a budget-friendly restaurant with vegan options along my route, something within a couple miles? … What's parking like there?” Followed by, “OK, let’s go there.” You can even ask “Oh, by the way, can you also add a calendar event for soccer practice tomorrow for 5 p.m.?” You can ask for something more specific, like “What dishes are popular there?” or even catch up on last night's game or news, all without taking your hands off the wheel. 8. Find a great deal for your next vacation. Instead of playing with different dates, destinations and filters to uncover the best deals, use Flight Deals to describe when, where and how you’d like to travel — as though you’re talking to a friend — and Flight Deals will take care of the rest. Search for something like “I want to take a week off in February. Looking for a deal, I want to fly nonstop to a city with great food and warm weather.” You’ll see the best bargains available that match your search, including destinations you may not have previously considered. 9. Build travel plans with Canvas in AI Mode. Canvas is a tool in AI Mode that gives you a space to organize plans and projects over time. And now, you can use Canvas to build travel plans that are customized for your specific needs. Head to AI Mode in Search and describe what type of trip you’re planning and the recommendations you’re looking for, then select the option to “Create Canvas.” Just make sure you’re enrolled in the AI Mode Labs experiment for cutting-edge features. 10. Save screenshots of places to make Google Maps lists. If you ever have trouble keeping track of all of the screenshots you take of travel blogs, news articles or social media posts when you’re researching places to go for an upcoming trip, you’ll want to try out this Google Maps feature. It uses Gemini capabilities to find names of places in your screenshots and helps save them to a list for you, making travel planning a breeze. 11. Get instant info with AI Mode in Circle to Search. Explore complex topics and dig into your initial searches with follow-up questions — all without switching apps. Simply long press the home button or navigation bar, then circle, tap or gesture on what you want to search. When our systems determine an AI response to be most helpful, an AI Overview will appear in your results. From there, you can ask follow up questions in AI Mode by using the bottom search bar and explore content across the web that’s relevant to your search. Tap into your creative side. 12. Edit images in Google Photos just by asking. You can use natural language to describe the edits you want like “erase the fence” or “restore this old photo.” You can even ask for personalized fixes like removing sunglasses and hats, fixing a smile and more . 13. Get creative with your photos using AI: Want to quickly transform your photos into shareable creations? Head to the Create tab to try AI templates , Remix, Photo to video and more right in Google Photos. 14. Edit images with Nano Banana in Search. Just open Lens in the Google app for Android and iOS , and tap the new Create mode (look for the yellow banana !). Try a suggested prompt or snap a picture and describe the edits you’re looking for. 15. Experiment with vibe coding. Try this Googler’s tips — like what AI tools to use and how to refine prompts. 16. Prompt like a pro to generate and edit images with Gemini. You can get great results with Gemini from simple one or two-sentence inputs. But for truly incredible images and more nuanced creative control, check out our guide on how to get the absolute most out of its capabilities. 17. Find your style with a selfie. If you want to use our virtual try on tool but don’t have a full body photo, you can now simply upload a selfie and Nano Banana will generate a full body digital version for you. 18. Give your photos some life and animate them. Gemini’s photo-to-video capabilities can take a static image and animate it for you — even adding audio! 19. Recap your 2025 with Google Photos. Don’t forget to look back on this year! Google Photos Recap turns your photos and videos from the past year into a highlight reel you can enjoy and share — plus this year, it has new tools so you can customize it. 20. Create an illustrated storybook with Gemini. Simply describe any story you can imagine, and Gemini generates a unique 10-page book with custom art and audio. 21. Master movie-making with Flow. Try our tips for animating anything with Flow , our AI filmmaking tool that uses our AI video generation model, Veo. 22. Give meta prompting a try. Work with Gemini on your prompts for creative tools like Veo to refine your instructions and see where it can take you. 23. Turn photos into a recipe book. Simply snap a few pictures of your holiday baking and have NotebookLM convert it into a delightful recipe book. You can just use Gemini’s world understanding to describe the recipes, or upload your notes and NotebookLM will take care of the rest . Tackle your to-dos. 24. Use one-handed gestures — Double pinch or Wrist turn — to control your Pixel Watch 4. Scroll through notifications, dismiss notifications, snooze your alarm effortlessly, manage timers and more without ever touching your screen . 25. Ask agentic AI in Search to call local businesses for you. Our new agentic AI feature can just ask you a few questions and then dial local businesses to ask if items are in stock, if there are any discounts available and more. 26. Access Gemini hands-free on your Pixel Watch 4 with Raise to Talk. All you need to do once the feature is enabled is bring your watch a few inches from your mouth, and start chatting . You don’t even need to say “Hey Google” — you’ll still see a blue light subtly flash at the bottom of your watch’s display so you know Gemini is listening. 27. Ask Pixel 10 to take a message for you. Take a Message on the latest Pixel 10 phones is a smarter way to deal with missed or declined calls. It uses spam detection models to see if a call is spam and then will also create a real-time transcript for you, as well as an AI-generated list of next steps. 28. Create Google Home automations more easily. With Ask Home, you can make automations using natural language thanks to the new Gemini for Home 1 — for example, simply say, “Create an automation to turn on the porch lights and lock the front door every day at sunset,” and it’s done. 29. Schedule tasks with Gemini. With scheduled actions , you can streamline routine tasks or receive personalized updates directly from Gemini. In your conversation, ask Gemini to perform a task at a specific time, or transform a prompt you're already using into a recurring action. 30. Troubleshoot with Gemini Live in real time. Have a squeaky chair? A glitching record player? With Gemini Live’s camera input, you can show Gemini what you’re seeing in real time and get quick input . Just point your camera at the issue and chat with Gemini to figure out a plan. 31. Use Gemini in Chrome to tackle your tab problem. Gemini in Chrome can now work across multiple tabs, so you can quickly compare and summarize information across multiple websites to find what you need. Planning your flight, hotel and vacation activities across multiple tabs? Gemini in Chrome can help you consolidate that information into a single itinerary to take the stress out of your travel plans. 32. Create and share custom Gems. Gems help you tailor Gemini to your specific needs, making tasks simpler and saving you time on repetitive prompting. Maybe you’ve made one for a workflow or a family vacation: To share, open your Gem manager on the web and click “Share” next to any Gem you’ve created, turning your favorite Gems into a shared resource so you can prompt less and create more. 33. Turn on Pixel’s Magic Cue to get relevant info when you need it. Magic Cue uses Gemini models to proactively surface relevant information and suggest helpful actions across different apps. That means it can find reservation details in your Gmail, reference events to your Calendar and preview the forecast for upcoming trips in the Weather app. 34. Shop conversationally and explore visually with AI Mode in Search. Rather than setting filters and sifting through items to find something that’s just right, you can say something like “barrel jeans that aren’t too baggy,” and AI Mode will intelligently provide a relevant set of shoppable options to help you get inspired. If you want to refine your options further, just search what you’re thinking, like “I want more ankle length.” 35. Translate as you scroll with Circle to Search. This year we updated translation capabilities in Circle to Search so that the experience is continuous: Simply long-press the home button or navigation bar to start Circle to Search, tap the “Translate” icon and press “scroll and translate.” As you scroll down the page, or even switch apps, the text will continue to translate, so there’s no interruption. Go pro at work. 36. Create beautiful slide presentations with Nano Banana Pro in Mixboard. Mixboard is an AI experiment from Google Labs designed to help you explore, expand and refine ideas. It now uses Nano Banana Pro — the newest version of our viral image generation model — to transform the content from a board into a compelling presentation. 37. Turn your notes and documents into narrated videos. NotebookLM’s Video Overviews use Nano Banana so you have more options for how Video Overviews look and feel. Video Overviews use one of six visual styles to choose from: Watercolor, Papercraft, Anime, Whiteboard, Retro Print and Heritage. And you can dictate whether you want a detailed explainer or a shorter brief on a topic. 38. Find time on your calendar. When you're responding to an email about finding a time to meet, Gmail detects the intent and shows a Help me schedule button in the toolbar. Select it, and Gemini suggests meeting times based on your Google Calendar and the context of the email. The time suggestions are added directly to your reply, so the other person can choose what works best. 39. Ask Gemini to help you with your Google Sheets. For instance, you can use the AI function to categorize user feedback, summarize product reviews or write personalized copy. Simply type =AI in any cell, along with your instruction for Gemini. Gemini will then carry out your request across the specific range. 40. Connect Deep Research to your Google Workspace account. Gemini Deep Research can draw on context from your Gmail, Drive and Chat and work it directly into your research. This means you can create even more comprehensive reports by pulling in information directly from your Gmail, Drive (including Docs, Slides, Sheets and PDFs) and Google Chat, alongside a variety of sources from the web. Phew — as you can see, it’s been a busy year! And there’s more where that came from: You can expect plenty of new, helpful Google AI launches, updates and tips in the year to come. POSTED IN: AI Gemini]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 19 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/ai-tips-2025/</guid>
    </item>
    <item>
      <title>5 ways AI agents will transform the way we work in 2026</title>
      <link>https://blog.google/products/google-cloud/ai-business-trends-report-2026/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X AI agents can now understand a goal, semi-autonomously develop a multi-step plan, and take actions on your behalf — all under your expert guidance and oversight. We’re moving away from abstract, future-gazing possibilities, and focusing on delivering tangible business value right now. Today, Google Cloud dropped its 2026 AI Agent Trends Report , along with a companion NotebookLM . Here’s a look at the five key trends that are defining where agents will fundamentally reshape business and drive new value in the coming year. 1. AI agents will help everyone be more productive. Employees will now be able to delegate tasks to different AI agents to reach their goals, shifting their daily work from routine execution to higher-level strategic direction. AI is already becoming a core driver of work from the start: More than 57,000 team members at Telus are regularly using AI and saving 40 minutes per AI interaction. Meanwhile, Suzano, the world's largest pulp manufacturer, developed an AI agent with Gemini Pro that translates natural language questions into SQL code — resulting in a 95% reduction in the time required for queries among 50,000 employees. 2. Agentic workflows will become a core part of business processes. Multiple agents in a system can collaborate, coordinate and communicate to automate complex, multi-step processes. This sophisticated automation goes far beyond chatbots to answer questions, so AI can support specific functions with higher business value. In 2026, we’ll see businesses start connecting agents according to their needs and requirements, running entire workflows from start to finish. For example, Salesforce and Google Cloud are building cross-platform AI agents using the Agent2Agent (A2A) protocol — a leap forward in establishing an open, interoperable foundation for agentic enterprises. 3. Agents will bring five-star experiences to life for everyone. The era of scripted chatbots and reactive customer service is coming to an end. In the coming year, we’ll see agents speeding up this shift — establishing hyperpersonalized, “concierge-style” service as the new standard for customer interactions. For example, global manufacturer Danfoss is using AI agents to automate email-based order processing, automating 80% of transactional decisions and reducing the average customer response from 42 hours to near real time. 4. AI agents will supercharge security operations. In a modern security operation center (SOC), human analysts are often overwhelmed by a constant stream of data and alerts. AI agents offer security teams the extra power and coverage they need to identify and respond to threats even faster. Macquarie Bank provides efficient, proactive fraud protection and digital self-service capabilities with Google Cloud AI, directing 38% more users towards self-service and reducing false positive alerts by 40%. We predict 2026 will be the year AI agents take over the most taxing security operations work, automating manual tasks like alert triage and investigation. This will allow human analysts to dedicate their focus to their most critical work: hunting threats and developing next-generation defenses. 5. Companies will double down on training an AI-ready workforce. Adopting the latest AI technology and tools is only the first step. The biggest challenge — and the most critical factor for success — is people. During 2026, organizations will move from simply buying AI to building an AI-ready workforce , transitioning away from one-off training toward developing adaptable, continuous learning plans. These programs provide hands-on practice with real-world scenarios, allowing employees to build the AI skills they need at their own pace and on their own schedule. Check out the full report for more details about AI agent trend predictions in 2026. POSTED IN: Google Cloud AI]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 19 Dec 2025 14:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/google-cloud/ai-business-trends-report-2026/</guid>
    </item>
    <item>
      <title>You can now verify Google AI-generated videos in the Gemini app.</title>
      <link>https://blog.google/technology/ai/verify-google-ai-videos-gemini-app/</link>
      <description><![CDATA[We’re expanding our content transparency tools to help you more easily identify AI-generated content. You can now check if a video was edited or created with Google AI directly in the Gemini app. Simply upload a video and ask something like, "Was this generated using Google AI?" Gemini will scan for the imperceptible SynthID watermark across both the audio and visual tracks and use its own reasoning to return a response that gives you context and specifies which segments contain elements generated using Google AI. For example, it might say: “SynthID detected within the audio between 10-20 secs. No SynthID detected in the visuals.” Uploaded files can be up to 100 MB and 90 seconds long. Both image and video verification are now available in all languages and countries supported by the Gemini app.]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 18 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/verify-google-ai-videos-gemini-app/</guid>
    </item>
    <item>
      <title>Inside Kaggle's AI Agents intensive course with Google</title>
      <link>https://blog.google/technology/developers/ai-agents-intensive-recap/</link>
      <description><![CDATA[After setting a Guinness World Record in April with over 280,000 participants in Kaggle’s 5-Day GenAI intensive course with Google , we thought we’d seen the peak of global interest. Yet the excitement was only growing. Our partnership with Kaggle to host the 5-Day AI Agents Intensive Course surpassed all expectations, welcoming over 1.5 million learners from around the world. Together, we explored the frontier of AI agents moving beyond chatbots to systems that can reason, plan and take action to solve complex, real-world problems. The response was nothing short of inspiring, and confirmed what we’ve long believed: the developer community is ready for what’s next. The content Building on the foundation of our previous GenAI intensive, this course went deeper into the practical realities of AI agents. We examined how AI agents actually work, where they shine and the challenges developers face, when designing and deploying them in production. Learners received hands-on guidance from a lineup of expert speakers, including leaders from Google, alongside industry pioneers from Cohere, Reified and NVIDIA. Sessions emphasized real-world applications, architectural decisions and emerging best practices — equipping participants with skills they could immediately put to use. If you missed attending the intensive in November, all the coursework has now been transformed into a Kaggle Learn Guide . The community We were inspired by the energy and excitement from the Kaggle community. Throughout the week, learners connected, collaborated and shared ideas at an incredible scale: 160,000+ learners joining the conversation on Kaggle’s Discord 2 million views on the course whitepapers, reflecting strong interest in conceptual depth. 3.3 million views on the course notebooks, reflecting a massive appetite for technical depth The capstone projects The capstone projects brought the week together, turning learning into action at a global scale. We received over 11,000 submissions, many from collaborative teams that formed during the course to bring bold ideas to life. The projects demonstrated the creativity, technical skill and imagination of the global AI community. From practical agent-based workflows to ambitious multi-agent systems, the quality and diversity of submissions made it incredibly difficult to narrow down the top entries. You can explore all the winning teams and their capstone projects here . What’s next Whether you participated in a capstone project or followed along with the sessions, your engagement made this course a tremendous success. Learning alongside such a motivated and passionate community has been truly inspiring. We’re already looking forward to bringing back another intensive with Google in 2026. In the meantime, you can continue honing your skills through ongoing Kaggle competitions and stay connected with fellow learners on the Kaggle Discord . For those who missed the November intensive but still want to learn, all course content is now available as a Kaggle Learn guide . Thank you to everyone who joined us — we can’t wait to see what you build next. POSTED IN: Developers Gemini Models Google Cloud AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 18 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/ai-agents-intensive-recap/</guid>
    </item>
    <item>
      <title>Watch a podcast discussion about Gemini 3 and the future of Search.</title>
      <link>https://blog.google/technology/ai/release-notes-podcast-search/</link>
      <description><![CDATA[The latest episode of the Google AI: Release Notes podcast looks at how we are integrating our most capable frontier models directly into Search. Host Logan Kilpatrick talks with Robby Stein and Rhiannon Bell about how Gemini 3 uses advanced reasoning and coding capabilities to solve complex user questions — creating everything from physics simulations to live graphs — and the technical work behind bringing these features to millions of users. Watch the episode below, or listen to the podcast on Apple Podcasts or Spotify .]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 18 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/release-notes-podcast-search/</guid>
    </item>
    <item>
      <title>Gemini 3 Flash: frontier intelligence built for speed</title>
      <link>https://blog.google/products/gemini/gemini-3-flash/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Today, we're expanding the Gemini 3 model family with the release of Gemini 3 Flash, which offers frontier intelligence built for speed at a fraction of the cost. With this release, we’re making Gemini 3’s next-generation intelligence accessible to everyone across Google products. Last month, we kicked off Gemini 3 with Gemini 3 Pro and Gemini 3 Deep Think mode, and the response has been incredible. Since launch day, we have been processing over 1T tokens per day on our API. We’ve seen you use Gemini 3 to vibe code simulations to learn about complex topics, build and design interactive games and understand all types of multimodal content . With Gemini 3, we introduced frontier performance across complex reasoning, multimodal and vision understanding and agentic and vibe coding tasks. Gemini 3 Flash retains this foundation, combining Gemini 3's Pro-grade reasoning with Flash-level latency, efficiency and cost. It not only enables everyday tasks with improved reasoning, but also is our most impressive model for agentic workflows. Starting today, Gemini 3 Flash is rolling out to millions of people globally: For developers in the Gemini API in Google AI Studio , Gemini CLI and our new agentic development platform Google Antigravity For everyone via the Gemini app and in AI Mode in Search For enterprises in Vertex AI and Gemini Enterprise Gemini 3 Flash: frontier intelligence at scale Gemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro. In addition to its frontier-level reasoning and multimodal capabilities, Gemini 3 Flash was built to be highly efficient, pushing the Pareto frontier of quality vs. cost and speed. When processing at the highest thinking level, Gemini 3 Flash is able to modulate how much it thinks. It may think longer for more complex use cases, but it also uses 30% fewer tokens on average than 2.5 Pro, as measured on typical traffic, to accurately complete everyday tasks with higher performance. Gemini 3 Flash pushes the Pareto frontier on performance vs. cost and speed. Performance, here, is measured by LMArena Elo Score. Gemini 3 Flash’s strength lies in its raw speed, building on the Flash series that developers and consumers already love. It outperforms 2.5 Pro while being 3x faster (based on Artificial Analysis benchmarking) at a fraction of the cost. Gemini 3 Flash is priced at $0.50/1M input tokens and $3/1M output tokens (audio input remains at $1/1M input tokens). Gemini 3 Flash outperforms 2.5 Pro in speed and quality . For developers: intelligence that keeps up Gemini 3 Flash is made for iterative development, offering Gemini 3’s Pro-grade coding performance with low latency — it’s able to reason and solve tasks quickly in high-frequency workflows. On SWE-bench Verified, a benchmark for evaluating coding agent capabilities, Gemini 3 Flash achieves a score of 78%, outperforming not only the 2.5 series, but also Gemini 3 Pro. It strikes an ideal balance for agentic coding, production-ready systems and responsive interactive applications. Gemini 3 Flash’s strong performance in reasoning, tool use and multimodal capabilities is ideal for developers looking to do more complex video analysis, data extraction and visual Q&A, which means it can enable more intelligent applications — like in-game assistants or A/B test experiments — that demand both quick answers and deep reasoning. Gemini 3 Flash enables multimodal reasoning in a hand-tracked "ball launching puzzle game" game providing near real-time AI assistance . Gemini 3 Flash builds and A/B tests new loading spinner designs in near real-time , streamlining the design-to-code process. Gemini 3 Flash uses multimodal reasoning to analyze and caption an image with contextual UI overlays in near real-time, ultimately transforming a static image into an interactive experience. Gemini 3 Flash takes a single instruction prompt and codes three unique design variations . We’ve received a tremendous response from companies using Gemini 3 Flash. Companies like JetBrains, Bridgewater Associates, and Figma are already using it to transform their businesses, recognizing how its inference speed, efficiency and reasoning capabilities perform on par with larger models. Gemini 3 Flash is available today to enterprises via Vertex AI and Gemini Enterprise. For everyone: Gemini 3 Flash is rolling out globally Gemini 3 Flash is now the default model in the Gemini app, replacing 2.5 Flash. That means all of our Gemini users globally will get access to the Gemini 3 experience at no cost, giving their everyday tasks a major upgrade. Because of Gemini 3 Flash’s incredible multimodal reasoning capabilities, you can use it to help you see, hear and understand any type of information faster. For example, you can ask Gemini to understand your videos and images and turn that content into a helpful and actionable plan in just a few seconds. Gemini 3 Flash in the Gemini app can analyze short video content and give you a plan , like how to improve your golf swing. As Gemini 3 Flash is optimized for speed, it can see and guess what you’re drawing while you’re still sketching it. You can upload an audio recording and Gemini 3 Flash will identify your knowledge gaps, create a custom quiz , and give you detailed explanations on the answers. Or you can quickly build fun, useful apps from scratch using your voice without prior coding knowledge. Just dictate to Gemini on the go, and it can transform your unstructured thoughts into a functioning app in minutes. Gemini 3 Flash is also starting to roll out as the default model for AI Mode in Search with access to everyone around the world. Building on the reasoning capabilities of Gemini 3 Pro, AI Mode with Gemini 3 Flash is more powerful at parsing the nuances of your question. It considers each aspect of your query to serve thoughtful, comprehensive responses that are visually digestible — pulling real-time local information and helpful links from across the web. The result effectively combines research with immediate action: you get an intelligently organized breakdown alongside specific recommendations — at the speed of Search. This shines when tackling complex goals with multiple considerations like trying to plan a last-minute trip or learning complex educational concepts quickly. Try Gemini 3 Flash today Gemini 3 Flash is available now in preview via the Gemini API in Google AI Studio, Google Antigravity, Vertex AI and Gemini Enterprise . You can also access it through other developer tools like Gemini CLI and Android Studio . It’s also starting to roll out to everyone in the Gemini app and AI Mode in Search, bringing fast access to next-generation intelligence at no cost. We’re looking forward to seeing what you bring to life with this expanded family of models: Gemini 3 Pro, Gemini 3 Deep Think and now, Gemini 3 Flash. POSTED IN: Gemini Models AI Google DeepMind]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 17 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/gemini/gemini-3-flash/</guid>
    </item>
    <item>
      <title>We’re publishing an AI playbook to help others with sustainability reporting.</title>
      <link>https://blog.google/outreach-initiatives/sustainability/ai-playbook-sustainability-reporting/</link>
      <description><![CDATA[We’re sharing a practical playbook to help organizations streamline and enhance sustainability reporting with AI. Corporate transparency is essential, but navigating fragmented data and labor-intensive processes often leaves little bandwidth for the strategic work that actually drives progress forward. After two years of integrating AI into our own environmental reporting process, we’re open-sourcing our learnings to help others break through these bottlenecks. The AI Playbook for Sustainability Reporting is a hands-on toolkit designed to move from hype to implementation. It includes a systematic framework to audit processes, "starter pack" prompt templates for common tasks, and real-world examples of how teams can use tools like Gemini and NotebookLM to help validate claims and answer inquiries. Dive into the playbook today, and let us know if you have any feedback .]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 15 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/sustainability/ai-playbook-sustainability-reporting/</guid>
    </item>
    <item>
      <title>You can now have more fluid and expressive conversations when you go Live with Search.</title>
      <link>https://blog.google/products/search/live-audio-gemini-model-update/</link>
      <description><![CDATA[When you go Live with Search , you can have a back-and-forth voice conversation in AI Mode to get real-time help and quickly find relevant sites across the web. And now, thanks to our latest Gemini model for native audio , the responses on Search Live will be more fluid and expressive than ever before. This update means that Search can respond with a more natural sounding voice or at a certain speed – perfect for DIY help or learning more about a new topic, like geology. Just open the Google app ( Android & iOS ), tap the Live icon under the search bar and ask your question out loud for a helpful audio response. The updated Gemini model for native audio will roll out over the next week to all Search Live users in the U.S. We hope you’ll try it out soon to enjoy smoother, more natural conversations.]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 12 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/live-audio-gemini-model-update/</guid>
    </item>
    <item>
      <title>Bringing state-of-the-art Gemini translation capabilities to Google Translate</title>
      <link>https://blog.google/products/search/gemini-capabilities-translation-upgrades/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X True understanding comes from not just what someone says, but also the nuance of how they say it. Today, Google Translate is getting better at both. We’re introducing state-of-the-art text translation quality 1 in Search and the Translate app, built with Gemini. This means you’ll get much smarter, more natural and accurate text translations wherever you’re searching. We’re also rolling out a new beta version of live translation, that brings real-time, natural-sounding translations right to your headphones with the help of Gemini’s new, native speech-to-speech translation capabilities . Plus, we’re adding more languages for you to practice in the Translate app. Get smarter, more natural translations, built with Gemini Starting today, Google Translate uses advanced Gemini capabilities to better improve translations on phrases with more nuanced meanings like idioms, local expressions or slang. Say you’re trying to translate an English idiom like “ stealing my thunder .” Now, it's easier than ever to get a more natural, accurate translation, instead of a literal word-for-word translation. Gemini parses the context to give you a helpful translation that captures what the idiom really means. This update is rolling out starting today in the U.S. and India translating between English and nearly 20 languages, including Spanish, Hindi, Chinese, Japanese and German, in the Translate app ( Android and iOS ) and on the web . Hear and understand the world in real-time Building on Gemini’s new live speech-to-speech translation capabilities , we are rolling out a beta experience enabling you to hear real-time translations in your headphones. This new experience works to preserve the tone, emphasis and cadence of each speaker to create more natural translations and make it easier to follow along with who said what. Whether you're trying to have a conversation in a different language, listen to a speech or lecture while abroad, or watch a TV show or film in another language, you can now put in your headphones, open the Translate app, tap “Live translate” and hear a real-time translation in your preferred language. Following positive feedback in early testing, we're excited to make this beta more broadly available to collect even more feedback as we work to refine the model and experience. Starting today the beta is rolling out in the Translate app on Android in the U.S., Mexico, and India, works with any pair of headphones, and supports more than 70 languages . And we’ll be bringing it to iOS and more countries in 2026. Try it out in the Android app and let us know your thoughts via in-app feedback! Practice and master even more languages in Translate Starting today, we’re also expanding our language learning tools in the Translate app with improved feedback so you can get helpful tips based on your speaking practice. We’re also introducing a way for you to challenge yourself and reach your learning goals by tracking how many days in a row you’ve been learning, so you can clearly see your progress and consistency over time. And we’re expanding the capability to nearly 20 new countries including Germany, India, Sweden, and Taiwan, so even more people can continue to refine their language skills, including: English to German and Portuguese Bengali, Mandarin Chinese (Simplified), Dutch, German, Hindi, Italian, Romanian, and Swedish to English We’ve heard from users that they appreciate the customized learning experiences that reflect real-life scenarios. We'll continue to add new ways for people to make progress on their language learning goals. With even more advanced AI models and expanded language learning capabilities in Translate, we’re helping you capture not just the words, but the meaning behind them. We’re excited to hear what you think. POSTED IN: Translate Search AI]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 12 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-capabilities-translation-upgrades/</guid>
    </item>
    <item>
      <title>Gradient Canvas: Celebrating over a decade of artistic collaborations with AI</title>
      <link>https://blog.google/technology/ai/google-gradient-canvas-ai-art/</link>
      <description><![CDATA[To celebrate over a decade of artistic experimentation with AI, today we are revealing Gradient Canvas , an exhibition of 13 newly commissioned artworks. Inspired by the Bay Area’s local ecology, Gradient Canvas explores the idea that artificial intelligence can act as a bridge — connecting human perception, machine vision and the natural world around us. The featured global artists, a group including long-time collaborators with Google, used Google tools and AI to expand how we sense and interact with our surroundings. Their new works probe the co-evolving relationships between people, nature and machines, offering unique perspectives on how different forms of intelligence make sense of the world. Hear directly from the multidisciplinary group of artists about what inspired their work and the unique approaches to their work with AI: Installation view of Pollinator Pathmaker : 6nvKvSPnBEEFa6vTqwXJaZ in ‘Human Vision’ by Alexandra Daisy Ginsberg at Google Gradient Canopy in Mountain View, California, 2025. Photo: Henrik Kam. In Silico , by Casey Reas Photo: Henrik Kam. The Recombinant Room , by Certain Measures Photo: Henrik Kam. A Google Tree , by Clement Vallas Photo: Henrik Kam. The Garden Eternal: California , by Linda Dounia Photo: Henrik Kam. EP Flow , by Michael Joo Photo: Henrik Kam. Somatic Landscapes , by Rashaad Newsome Photo: Henrik Kam. Machine Dreams: Biophilia , by Refik Anadol Photo: Henrik Kam. California Terrain , by Sarah Rosalena Photo: Henrik Kam. Clouds , by Trevor Paglen Photo: Henrik Kam. DEAR DATA , by Sasha Stiles Photo: Henrik Kam. Over a decade of creative collaboration Rooted in early and long term efforts like the Google Arts & Culture Artist-in-Residence programs , our commitment to supporting artists has a long history. Ten years ago, Google researcher Alex Mordvintsev developed DeepDream, a computer-vision program that signaled the creative potential of artists and artificial intelligence and sparked a public fascination with AI-generated visuals. This excitement directly paved the way for our groundbreaking 2016 art exhibition with Gray Area and initiatives like Artists + Machine Intelligence (AMI), accelerating partnership between artists and AI that continues today. Human imagination is what gives technology purpose. We believe that when artists work with powerful tools, they don't just create; they help us all build a more thoughtful future. That’s why our focus for the next decade is on expanding this canvas by supporting the next generation of visionary voices and making cutting-edge AI tools more accessible for everyone. The exhibition includes a physical installation at our Gradient Canopy office in Mountain View, explore the artworks online on Google Arts & Culture . POSTED IN: AI Arts & Culture]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 11 Dec 2025 21:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-gradient-canvas-ai-art/</guid>
    </item>
    <item>
      <title>4 highlights from Google Beam in 2025</title>
      <link>https://blog.google/technology/research/google-beam-2025-moments/</link>
      <description><![CDATA[In 2021, we introduced Project Starline with the ambitious goal of making remote communication feel like you’d instantly traveled across distances to be together. This year, our vision became reality with the announcement of Google Beam — our first true-to-life 3D video-communication platform, powered by Google AI. Here’s a look at how Beam progressed this year: 1. Google Beam took center stage at I/O. At I/O in May, CEO Sundar Pichai unveiled the evolution of Project Starline into a new AI-powered video communication platform, Google Beam . He showed how Google is bringing its AI technology to Google Beam through breakthrough AI-powered video models that transform 2D video streams to 3D experiences, and how we’re paving the way for future features like real-time speech translation . 2. Beam began working with industry leaders in video. At InfoComm 2025, we announced HP Dimension with Google Beam , the first product specifically for businesses developed in partnership with our partner, HP. Our technology earned a " Best of Show " win at InfoComm, recognized for its broad impact and remote conversation experience that’s far closer to being in the same room. HP Dimension with Google Beam was later named an Honoree in Fast Company's Next Big Things in Tech Awards 2025 . Google Beam was also showcased at Zoomtopia, Zoom's annual user conference, highlighting a partnership to integrate the Google Beam platform with their video conferencing software. To further expand Beam's reach, we collaborated with HP to establish a network of distribution partners like Diversified, AVI-SPL and more. Zoom CEO, Eric Yuan In October, we co-hosted with industry experts at a WORKTECH event where founder and CEO of UnWork and WORKTECH Philip Ross called Beam “an AI game-changer that connects people and not just pixels, changing the way we enable presence and collaboration.” 3. Workplaces all over the world became early users — including Google. Organizations globally began using Beam's true-to-life capabilities firsthand in their offices. Workplaces like Schwarz Digits and Huntington Bank were among them. The list is growing, too: Bain, Duolingo, Salesforce, Citadel, NEC, Hackensack Meridian Health and more are actively embracing Beam. Here at Google, Googlers testing Beam said they prefer it over typical video conferencing and 90% agreed that using Beam makes it feel like they're in the same space as their meeting partner. Through testing, we’ve identified five key use cases where Beam truly shines: job interviews and recruiting; mentoring and talent development; productivity and collaboration; critical conversations and connecting distributed teams. 4. We announced a pilot program with the USO. We began work to place Beam devices in select USO centers worldwide, enabling deployed active duty service members to connect with their families. Learn more about Google Beam and connect with our sales team at beam.google . POSTED IN: Research AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 11 Dec 2025 18:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/research/google-beam-2025-moments/</guid>
    </item>
    <item>
      <title>These developers are changing lives with Gemma 3n</title>
      <link>https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X When Gemma 3n was released, we hoped developers would use its on-device, multimodal capabilities to make a difference in people’s lives. With more than 600 projects submitted to the Gemma 3n Impact Challenge on Kaggle, the community delivered on that promise. Today, we’re excited to introduce the winners: First Place: Gemma Vision Gemma Vision is an AI assistant designed for visually impaired people. The developer’s brother, who is blind, played a vital role in ensuring features were genuinely helpful for the blind community. Because holding a phone can be impractical while using a cane, the system was designed to process visuals from a phone camera strapped to the user’s chest. Functions can be triggered using a 8BitDo Micro controller or voice commands, allowing users to perform actions without navigating touchscreen menus. This project also won the Special Technology Prize for Google AI Edge , a platform for deploying models on-device. It deployed Gemma 3n using the MediaPipe LLM Inference API and leveraged features like streamed responses in the flutter_gemma package to make the experience fluid. Second Place: Vite Vere Offline Vite Vere helps foster autonomy for people with cognitive disabilities. Originally developed using the Gemini API, this project leveraged Gemma 3n to make the digital companion work offline. By transforming images to simple instructions that can then be read aloud using the local device’s text-to-speech engine, the app enables users to navigate daily tasks. Third Place: 3VA For decades, Eva, a brilliant graphic designer with cerebral palsy, was limited to simple commands like “want food now.” This project fine-tuned Gemma 3n to translate pictograms into rich expressions that better reflect Eva’s voice. The team trained the model locally using Apple’s MLX framework, demonstrating a cost-effective way to develop personalized Augmentative and Alternative Communication (AAC) technology. Fourth Place: Sixth Sense for Security Guards Unlike traditional video monitoring systems that just detect motion, this project used Gemma 3n to provide human-level context and distinguish benign events from genuine threats. By integrating a lightweight YOLO-NAS model to detect initial movement and send it to Gemma 3n for processing, the system can handle high-bandwidth video feeds (up to 360fps and 16 cameras) in real time. The Unsloth Prize: Dream Assistant Voice assistants frequently fail users with speech impairments. This project used Unsloth , a library for efficient fine-tuning, to train Gemma 3n on an individual’s audio recordings. The result is a custom AI assistant that understands the user’s unique speech patterns and enables voice control over device functions. The Ollama Prize: LENTERA This project demonstrates how to bring AI to disconnected regions by transforming affordable hardware into offline microservers. Lentera broadcasts a local WiFi hotspot, allowing users to connect their devices to an educational hub running Gemma 3n via Ollama , a platform for local model deployment. The LeRobot Prize: Graph-based Cost Learning and Gemma 3n for Sensing Robotic exploration is often bottlenecked by the time spent sensing rather than moving. To solve this, the team built a novel “scanning-time-first” pipeline on top of LeRobot , a robotics framework developed by Hugging Face. This project used Gemma 3n to create plans while an inductive graph-based matrix completion (IGMC) model predicted latencies, demonstrating the viability of embodied AI at the edge. The NVIDIA Jetson Prize: My (Jetson) Gemma Integrating AI into our physical environment requires systems that are both responsive and energy-efficient. This project used a smart CPU-GPU hybrid processing strategy to deploy a context-aware voice interface on an NVIDIA Jetson Orin , demonstrating how helpful AI can move beyond screens to assist users in the real world. From accessibility to crisis response, these projects show what's possible with Gemma 3n. Many others deserve recognition, so join us as we highlight a developer story every day on @googleaidevs over the coming month. POSTED IN: Developers AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 10 Dec 2025 17:15:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</guid>
    </item>
    <item>
      <title>Learn more about AI in the workplace in our new research report.</title>
      <link>https://blog.google/products/workspace/gemini-ai-workplace-research-report/</link>
      <description><![CDATA[A new global survey of executives, decision makers and knowledge workers reveals that organizations truly transforming with AI are seeing real results that move their businesses and allow employees to focus on meaningful work. The biggest gains from adopting AI aren’t about saving time — they’re about expanding potential. Highly transformed organizations surveyed report that AI: Increases innovation by 57% Decreases time spent on mundane tasks by 39% Improves work creativity by 65% Read the full research report, “ Beyond AI Optimism: Five ways to move your business from saving time to sparking innovation ,” or visit the Workspace blog for key takeaways.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/gemini-ai-workplace-research-report/</guid>
    </item>
    <item>
      <title>2025 at Google</title>
      <link>https://blog.google/technology/ai/look-back-2025/</link>
      <description><![CDATA[It’s (almost) a wrap on 2025! As we prepare for a great new year, let’s take a quick rewind and remember some of this past year’s most exciting launches, biggest moments and more.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/look-back-2025/</guid>
    </item>
    <item>
      <title>How we’re supporting the next generation of innovators</title>
      <link>https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</link>
      <description><![CDATA[For decades, Google has supported Computer Science Education Week (CSEdWeek) to demystify coding and computational thinking. This year during CSEdWeek, we’re taking part in the global Hour of AI alongside our partners Code.org and the Computer Science Teachers Association (CSTA), to help educators and students better understand, use and create with AI. Towards this goal, we’re launching a new quest in our gamified experience, AI Quests , bringing AI literacy to classrooms with a hands-on approach, and we’re announcing over $5 million in Google.org funding for computer science teaching. Launching a new quest Hundreds of Googler volunteers around the world are visiting classrooms this week to lead AI Quests , our game-based learning series developed with the Stanford Accelerator for Learning . Today, we’re releasing a new quest where students step into the shoes of researchers and use an AI model to detect eye disease and prevent blindness. This quest is inspired by our real-world research on diabetic retinopathy . It can be accessed for free along with our existing flood forecasting quest and accompanying resources for teachers. We’re also working closely with partners to bring AI Quests to more classrooms in 2026. One collaboration is with the Raspberry Pi Foundation and Google DeepMind, which has expanded their Experience AI program — recognized by UNESCO for promoting responsible AI education — to millions of students with funding from Google.org and integrated AI Quests into their curriculum. Investing in educators We’re announcing over $5 million in new Google.org funding to bolster computer science teaching in the age of AI. This builds on our recent $30 million global commitment to learning and foundational research, as well as the over $240 million we have provided to advance computer science education globally. The new funding will help organizations like California State University, Dominguez Hills to prepare teachers to deliver foundational computer science and AI curriculum to K-12 students and CSTA to publish the revised K–12 Computer Science Standards through a modern, accessible web-presence. While coding tasks may change in the AI era, the foundational principles of computer science remain more vital than ever. POSTED IN: Learning & Education Google.org AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</guid>
    </item>
    <item>
      <title>Transforming Nordic classrooms through responsible AI partnerships</title>
      <link>https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</link>
      <description><![CDATA[As AI transforms the classroom, school districts in Northern Europe are setting a global standard for innovation. Beyond mere adoption, districts across Iceland, Norway, and Sweden are prioritizing responsible use. By partnering with educators and administrators, they are ensuring Google’s AI tools are integrated thoughtfully, securely, and with real value for students. Providing personalized learning in Icelandic schools Building on the widespread adoption of Google Classroom in Iceland, we partnered with the Ministry of Education to launch a pilot with 300 teachers. By integrating Gemini for Education and NotebookLM, the program aims to enhance personalized learning and boost AI literacy. Beyond developing fluency, the pilot identifies the resources teachers need for ethical use, while upskilling them to streamline administrative tasks and pinpoint student learning gaps. Saving teachers time while building AI literacy in Sweden Through a partnership with school districts across Sweden to bring Gemini for Education to nearly 30,000 students and faculty members, schools are embracing AI’s potential to transform learning. Teachers are using Gemini to create high-quality, tailored materials — a shift one educator describes as "revolutionary" because it allows them to invest more time with their students. According to Johan Kellén, teacher & ICT Coordinator at Linköping Municipality, it takes so long to produce good teaching material — you usually have to construct it yourself and ensure it is up to date and current, and adapted to each class and sometimes each student. This is where Gemini is able to help. Districts hosted workshops to empower educators to use Gemini and foster dialogue with older students about utilizing Guided Learning mode as a study aid. They believe AI literacy is a shared responsibility. Leading the charge for safe, secure digital learning in Norway In a landmark move for digital privacy, Norway completed a national Data Protection Impact Assessment (DPIA), greenlighting the use of Google Workspace for Education and ChromeOS in schools. This collaborative achievement between Google Cloud and the Norwegian Association of Local and Regional Authorities (KS) is a prime example of public sector efficiency. By conducting a centrally driven DPIA, KS eliminated the need for every single municipality to conduct their own complex assessments, highlighting how Google tools meet stringent GDPR requirements and ensuring that local resources focus on innovation over administrative compliance. By prioritizing trust and partnering with Google, Norway has secured a safe, innovative learning environment for students while saving IT administrators significant time and resources. POSTED IN: Google in Europe Learning & Education AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 10:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</guid>
    </item>
    <item>
      <title>The latest AI news we announced in November</title>
      <link>https://blog.google/technology/ai/google-ai-updates-november-2025/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X For more than 20 years, we’ve invested in machine learning and AI research, tools and infrastructure to build products that make everyday life better for more people. Teams across Google are working on ways to unlock AI’s benefits in fields as wide-ranging as healthcare, crisis response and education. To keep you posted on our progress, we're doing a regular roundup of Google's most recent AI news. Here’s a look back at some of our AI announcements from November. November ushered in a new era of intelligence: with the launch of Gemini 3, we're improving everyone's ability to learn, plan or just get things done. From "vibe coding" with our powerful models, to generating professional grade visuals with Nano Banana Pro, to the autonomous workflows in our new Google Antigravity platform, the distance between a spark of imagination and reality has never been shorter. So, whether you’re a developer building complex agents or a traveler planning for the upcoming holidays with Canvas in AI Mode, these updates turn AI into a proactive partner ready to help you get things done, all season long. We released Gemini 3, AI for a new era of intelligence . Gemini 3 is built to bring any idea to life. It represents the next step in our work to push the frontiers of intelligence, agentic experiences and personalization — so that AI is truly helpful for everyone. Gemini 3 is the best model in the world for multimodal understanding and it's our most powerful agentic and vibe coding model to date. For developers, Gemini 3 Pro outperforms previous versions across major AI benchmarks. Gemini 3’s upgraded smarts and new capabilities are now available in the Gemini app , and you can review our hub with all the Gemini 3 announcements . We made Gemini 3 available in Google Search for our most intelligent search yet . Gemini 3’s state-of-the-art reasoning is now available in Google Search, starting with AI Mode — marking the first time we brought a Gemini model to Search on day one. Gemini 3 grasps depth and nuance, and unlocks new experiences in Search with dynamic visual layouts, interactive tools and simulations tailored specifically for your query. Google AI Pro and Ultra subscribers in nearly 120 countries and territories in English can use Gemini 3 Pro by selecting “Thinking with 3 Pro” from the model drop-down menu in AI Mode. We unveiled Nano Banana Pro, built on Gemini 3 . Nano Banana Pro, our newest image generation and editing model, is built on Gemini 3 and moves beyond spontaneous art into an era of high-fidelity, studio-quality visuals. You now have the choice between the original Nano Banana for fun, fast editing, or Pro for an even more powerful creative partner capable of handling complex tasks demanding the highest quality. To help get you started, we shared seven tips to get the most out of Nano Banana Pro . We introduced Google Antigravity, a new agentic development platform . Antigravity is a platform designed to give developers an AI-powered coding experience that goes beyond simple editing. It delivers a new agent-first interface for deploying agents that autonomously plan, execute and verify complex tasks. Our vision for Antigravity is to enable anyone with an idea to experience liftoff and build that idea into reality. You can try Antigravity for yourself, available today in public preview. We announced that Google Maps is getting smarter with Gemini . With the help of Gemini, you will soon have the first, hands-free, conversational driving experience in Google Maps that allows you to find places, report traffic, ask for suggestions along your route and more using just your voice. Plus, new landmark-based navigation will give you clear directions, so in addition to hearing “turn right in 500 feet,” you’ll also get directions based on helpful landmarks like “turn right after the Thai Siam Restaurant.” Landmark-based navigation is rolling out now on Android and iOS in the U.S, and Gemini in navigation on Google Maps is rolling out everywhere Gemini is available . We announced that Gemini has started rolling out in Android Auto . Android Auto is already available in over 250 million cars on the road, and Gemini is now coming along for the ride to make life on the road even better. You’ll be able to use natural language to add stops, send messages, access emails, create playlists and even brainstorm ideas while driving. Just make sure you have the Gemini app on your phone and look for the tooltip on your car display. We introduced SIMA 2, a significant step toward Artificial General Intelligence (AGI) . SIMA 2 is a major milestone in our work to create general and helpful AI agents. By integrating the advanced capabilities of Gemini into SIMA 2, the model is evolving from an instruction-follower into an interactive gaming companion. Now, SIMA 2 can follow human-language instructions in virtual worlds and also think about its goals, converse with users and improve itself over time, marking an important step in the direction of robotics and AI-embodiment in general. We released WeatherNext 2: our most advanced weather forecasting model . WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour, and we’re already using this breakthrough technology to support weather agencies in making decisions. We marked the 5-year anniversary of AlphaFold cracking the protein folding problem, and spotlighted its ongoing impact . Five years ago AlphaFold 2 solved the protein structure prediction problem. The profound scientific and societal value of this work was recognized in 2024 with the Nobel Prize in Chemistry. In November, we looked back at how AlphaFold has unlocked new avenues of biological research and provided our first major proof point that AI can be a powerful tool to advance science. We announced new ways to plan travel with AI in Search . Our new AI features in Search help you build the perfect itinerary, from snagging a great deal to turning your plans into actual bookings. To get started with planning, try the Canvas tool in AI Mode: Describe the kind of trip you want and what recommendations you need. Then hit "Create Canvas," and watch your custom travel plan come together. We added new AI shopping features in Search and Gemini to help with the holidays . Our biggest upgrade to shopping means you can now use conversational AI and agentic AI to take the hard work out of your holiday shopping. Using AI Mode in Search to shop, you can now describe what you’re looking for and get an intelligently organized response that brings together rich visuals, price, reviews, inventory info, and more. Plus, new AI agents can now call stores to check stock and use agentic checkout to buy items automatically from eligible merchants when the price is right. We announced new commitments to AI learning and education . At our AI for Learning Forum in London, we announced $30 million in new funding for learning. The event brought together experts in education and technology and continued our recent work to develop AI in a way that improves learning outcomes. We also released popular new features to help with daily studying, including the ability to create flashcards and quizzes right in the NotebookLM app . We shared essential tips for using Gemini Live . Gemini Live’s latest updates allow you to have more natural, two-way conversations with AI. Our tips show how to make the most of these updates by tailoring your learning, especially for complex subjects. You can now adjust Gemini's speech speed to learn at your own pace and improve accessibility. You can also practice a new language, rehearse for a job interview or even liven your conversation up with a fun accent. We announced a new $40 billion investment in Texas for AI and cloud infrastructure . CEO of Google and Alphabet Sundar Pichai and Texas Governor Greg Abbott made the announcement at an event in Midlothian, TX. This latest announcement represented the capstone of our 2025 push to make AI investments that unlock economic opportunity, advance scientific breakthroughs and create opportunities that benefit everyone. It includes major investments across America , as well as Europe , Africa and the Asia-Pacific region — alongside a critical US workforce initiative to train 100,000 electrical workers and create 30,000 new apprentices. POSTED IN: AI Gemini Developers Search Google DeepMind Maps Android Shopping Learning & Education Google Labs Company announcements Google in Europe Google.org]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 19:45:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-ai-updates-november-2025/</guid>
    </item>
    <item>
      <title>New research from Google Workspace reveals how young leaders are using AI at work.</title>
      <link>https://blog.google/products/workspace/young-leaders-survey-ai/</link>
      <description><![CDATA[Google Workspace has released findings from our second survey that looks at how people aged 22-39 are using AI at work. Commissioned by Workspace in partnership with the Harris Poll, the findings reveal three key themes: Young leaders want AI that offers more personalized responses — think outputs that aren’t just generic, but use your preferred tone and style. They are taking a very hands-on approach to using this technology, becoming AI architects for their own workflows. AI is helping young leaders feel more confident at work: They’re increasingly relying on it as a tool for professional development. Check out the press release for the full data, as well as insights from Google Workspace’s VP of Product, Yulie Kwon Kim.]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/young-leaders-survey-ai/</guid>
    </item>
  </channel>
</rss>
