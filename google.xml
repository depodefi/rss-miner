<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Thu, 04 Dec 2025 19:59:11 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>Look back on your 2025 with Google Photos Recap</title>
      <link>https://blog.google/products/photos/google-photos-2025-recap/</link>
      <description><![CDATA[Another year is almost in the books, and it’s time to look back at the moments that made it memorable. Last year , we introduced Google Photos Recap to help you rediscover what made your year special. Now, Recap is back for 2025, turning your photos and videos from the past year into a highlight reel you can enjoy and share. This year, we’ve added new ways for you to personalize, create and share your Recap. Look back at the moments that made your year Your Recap highlights memorable photos and moments from your year, paired with fun graphics and cinematic effects. You’ll see familiar photo stats from last year, like your top people and total photo count — and this year we also added a selfie count. And for those in the U.S. with Gemini features in Photos enabled, your Recap will also showcase your standout hobbies and top highlights from the year. We also heard your feedback and added the option to hide specific people or photos. Once you do, simply regenerate for an updated version. Create and share with your favorite apps You can get even more creative with a new integration: Just click the “Edit with CapCut” button at the end of your Recap to easily export it to CapCut, where you can choose from exclusive Google Photos templates so you can make a version that’s uniquely yours. Sharing your favorite moments from your Recap is also easier. At the end of your Recap, you’ll find a new carousel of short videos and collages from your lookback that are made for sharing on social or to the group chat. There’s also a new option to share your Recap directly to your WhatsApp Status. Relive your Recap all month long You can find your Recap in the Memories carousel starting this week. If it’s not there, you may see an option at the top of the app where you can request for Photos to create it. After you’ve watched your Recap, it will remain in the back of your Memories carousel and will be pinned in your Collections tab for all of December. You’ll also see a series of related highlights focused on 2025 in your Memories carousel throughout the month of December, featuring some more of your top moments from the year. Open the Google Photos app to explore, customize and share your 2025 Recap. We're already seeing some of the year's biggest stars giving it a try — check it out! POSTED IN: Photos AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/photos/google-photos-2025-recap/</guid>
    </item>
    <item>
      <title>We’re announcing new health AI funding, while a new report signals a turning point for health in Europe.</title>
      <link>https://blog.google/technology/health/ai-health-fund-eu/</link>
      <description><![CDATA[At the European Health Summit in Brussels, Greg Corrado, Distinguished Scientist at Google, released a new report authored by Implement Consulting Group and commissioned by Google revealing that AI is reversing the long-term trend of slowing scientific productivity, providing a turning point for a European healthcare system grappling with rising costs and workforce shortages. The report highlights how AI is already giving practitioners "time back," such as cutting emergency room wait times by over an hour. Building on this momentum, Google announced at the Summit $5 million in funding from Google.org for Bayes Impact to launch "Impulse Healthcare," a new EU health initiative. The project will empower frontline nurses, doctors, and administrators to build and experiment their own AI solutions on Bayes Impact's open-source platform. The end goal is to scale these practitioner-led innovations across the EU, freeing up precious clinical time to improve patient care.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/health/ai-health-fund-eu/</guid>
    </item>
    <item>
      <title>We’re celebrating Geoffrey Hinton’s Nobel-winning legacy with the University of Toronto.</title>
      <link>https://blog.google/technology/ai/hinton-chair-toronto/</link>
      <description><![CDATA[Today, we are celebrating the extraordinary impact of Nobel Prize-winner Geoffrey Hinton by investing in the future of the field he helped build. Google is proud to support the University of Toronto with $10 million CAD to establish the Hinton Chair in Artificial Intelligence. Geoff’s work on neural networks — spanning his time in academia and his decade here at Google — laid the foundation for modern AI. This chair honors his legacy and will help the university recruit visionary scholars dedicated to the same kind of curiosity-driven, fundamental research that Geoff championed. We are proud to support the next generation of breakthrough innovations and research at University of Toronto — a global leader in AI research.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/hinton-chair-toronto/</guid>
    </item>
    <item>
      <title>Use Circle to Search and Google Lens to spot scam messages.</title>
      <link>https://blog.google/products/search/scam-detection-circle-to-search-lens/</link>
      <description><![CDATA[One trending tactic among scammers involves sending fraudulent text messages, either directly to your phone or through messaging apps and social media sites. These messages often solicit or demand money and link out to scammy sites. To help you spot these scams, we’ve now added new capabilities to Circle to Search and Lens that will help you see the telltale signs so you can avoid getting deceived. Here’s how to use it: To use this with Circle to Search : Simply long press the home button or navigation bar of your Android device Circle the suspicious text This capability is also available through Lens , via the Google app ( Android and iOS ) with three short steps: Take a screenshot of the message Open Lens in the Google app Tap the screenshot After you follow these steps on your device, our systems will use AI and information from the web to assess whether the message is likely a scam. You’ll see an overview with guidance and insights including suggested next steps. This capability is available globally in Circle to Search and Lens, and will appear when our systems have high confidence in the quality of the response.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 02 Dec 2025 19:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/scam-detection-circle-to-search-lens/</guid>
    </item>
    <item>
      <title>Gemini 3 and Nano Banana Pro in Search are coming to more countries around the world.</title>
      <link>https://blog.google/products/search/gemini-3-ai-mode-more-countries/</link>
      <description><![CDATA[We're bringing our most intelligent model, Gemini 3 , to AI Mode in Google Search in nearly 120 countries and territories in English. Starting today, Google AI Pro and Ultra subscribers can begin using Gemini 3 Pro by tapping “Thinking with 3 Pro” in the model drop-down in AI Mode. Thanks to Gemini 3’s state-of-the-art reasoning capabilities, AI Mode can grasp the nuance of your most complex queries. And Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities are unlocking new generative user interfaces – so you can get dynamic visual layouts, interactive tools and custom simulations — all generated on the fly, specifically for your query. We're also bringing our latest generative imagery model, Nano Banana Pro , to AI Mode in more countries in English, starting today with Google AI Pro and Ultra subscribers. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s advanced reasoning and connects to Search's vast knowledge base to help you visualize infographics and more. With this expansion, it's now easier to ask anything and instantly get a richer, more helpful understanding.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 01 Dec 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-3-ai-mode-more-countries/</guid>
    </item>
    <item>
      <title>Here’s how researchers in Asia-Pacific are using AlphaFold</title>
      <link>https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</link>
      <description><![CDATA[Proteins are the microscopic machines that drive nearly every biological process, from how our bodies fight disease to how life itself evolves. Their function is defined by their unique 3D shape, but for decades, mapping that shape was a slow and expensive process. Five years ago, we introduced AlphaFold, an AI system that accurately predicts protein structures based solely on their amino acid sequences, and made it freely available to the world. Today, AlphaFold is used by over three million researchers, with more than a third of them in the Asia-Pacific region alone. From tackling deadly diseases to discovering entirely new forms of life, here are five ways researchers in this region have used AlphaFold to make breakthroughs in medicine and science: 1. Fighting a ‘silent killer’ in Malaysia Melioidosis, a disease caused by the bacterium Burkholderia pseudomallei — most commonly contracted through contaminated soil or water — claims nearly 90,000 lives each year. At the National University of Malaysia, Dr. Su Datt Lam’s team is using AlphaFold to understand how the bacterium’s proteins help it survive and spread , accelerating the development of new drugs to combat this "silent killer." 2. Uncovering new clues to Parkinson’s disease in Singapore Researchers Jackwee Lim and Yinxia Chao at Singapore’s Agency for Science, Technology and Research (A*STAR) and National Neuroscience Institute (NNI) used AlphaFold to create a 3D visualization of a protein linked to Parkinson's, a neurodegenerative disease. This revealed how the body's own immune system can disrupt the protein’s function, opening new paths for earlier diagnosis and targeted therapies. 3. Seeing the invisible drivers of disease in Korea At Korea’s Advanced Institute of Science & Technology (KAIST), Professor Ji-Joon Song’s team studies how disruptions in the way DNA is organized can lead to cancer and other diseases. AlphaFold allowed his team to map previously unseen regions of a key protein, uncovering a hidden interaction site. As he puts it, "AlphaFold is like the internet for structural biology." 4. Discovering a new frontier of protein shapes in Taiwan Dr. Danny Hsu’s team at Academia Sinica used AlphaFold to study a protein of unknown structure, and predicted a "71-torus knot" — an exceptionally complex protein fold. The team later confirmed this prediction in the lab, proving AlphaFold can help scientists discover entirely new phenomena. 5. Finding new life in Japan’s hot springs While studying microbes in the hot springs of Japan, Dr. Syun-ichi Urayama’s team discovered unusual viruses . Using AlphaFold to predict their protein structures, the team confirmed they belonged to a widespread, previously unknown family of life, revealing a new branch of molecular evolution. These five stories from the Asia-Pacific region — where we have over 13,000 research papers citing AlphaFold — represent just a fraction of what scientists have discovered since we introduced the technology. From tackling neglected diseases to revealing evolutionary history, AlphaFold has fundamentally changed biological research, empowering a global community of scientists to accelerate their work and tackle humanity's greatest challenges. Learn more about AlphaFold — including the co-award of the 2024 Nobel Prize in Chemistry to Demis Hassabis and John Jumper for its development — on the Google Deepmind blog . POSTED IN: Google in Asia AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 26 Nov 2025 23:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</guid>
    </item>
    <item>
      <title>Get an in-depth look at Gemini 3 with CEO Sundar Pichai.</title>
      <link>https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</link>
      <description><![CDATA[For the latest episode of the Google AI: Release Notes podcast, host Logan Kilpatrick sat down with CEO Sundar Pichai last week to discuss the extraordinary progress — and future — of AI at Google. Sundar shared the thinking behind going “AI-first” all the way back in 2016, setting up key investments that led the company to this moment. And beyond current releases like Gemini 3 and Nano Banana Pro, he also shared his excitement about long-term bets for the next decade, like quantum computing. "I think in about five years we'll be having breathless excitement about quantum, hopefully, like we are having with AI today,” he said. Watch the full conversation below, or listen to the Google AI: Release Notes podcast on YouTube , Apple Podcasts or Spotify .]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 23:39:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</guid>
    </item>
    <item>
      <title>The Google guide for holiday help</title>
      <link>https://blog.google/technology/ai/holiday-planning-ai-gemini-tips/</link>
      <description><![CDATA[Check out our tips, trends and more for tackling any seasonal stress. Offload tedious tasks to Gemini, get insider recommendations from Google Maps and be sure you’re getting the best price when shopping on Google. Get the help you need — and then get back to enjoying the holiday festivities.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/holiday-planning-ai-gemini-tips/</guid>
    </item>
    <item>
      <title>4 ways to refine your content in Flow</title>
      <link>https://blog.google/technology/google-labs/flow-refine-videos/</link>
      <description><![CDATA[We built Flow to help creatives bring their ideas to life, and so far we’ve seen over 500 million videos created since launching in May. Along the way we’ve heard your feedback: you want more precision and control, with both images and videos. Over the past few weeks, we’ve introduced new features to help give you more creative control in Flow with new refinement and editing capabilities. Here are four ways you can refine and edit your content in Flow: 1. Generate and edit images with Nano Banana Pro. In Flow, you can use images to serve as the characters, subjects and starting points for your clips. These might be images you upload, or ones you create in Flow with our new “Images” tab. This new dedicated workspace for generating and refining images is powered by Imagen and Nano Banana for free users, and subscribers also have access to Nano Banana Pro , our newest state-of-the-art image model which provides improved professional-grade controls like depth of focus, lighting and color grading. With simple prompts, you can change a character's outfit or alter a pose, camera angle or lighting without re-rolling the entire scene. You can also blend elements from multiple reference images to create your perfect frame while preserving the critical details of your ingredients. 2. Prompt by doodling. Instead of wordsmithing the perfect prompt, you can draw or annotate on an image, Flow understands your doodles and incorporates them into your final frame. You can doodle directly in Flow instead of turning to a separate editing app. 3. Insert and remove objects. We’ve heard feedback that sometimes when creating in Flow, you might want your clip to stay exactly the same, except for one missing piece — or one thing you don’t love. Tap the pencil icon on any clip to insert objects directly into videos or remove elements, without changing anything else. Object removal is rolling out next month and will be experimental. 4. Adjust your camera motion with reshoot. Sometimes, the camera angle or motion doesn’t come out exactly as you’d imagined, or you want to play around and see how clips look from different perspectives. We introduced a new feature to help adjust the camera position, orbit, or move the “dolly” in any of your generated videos. The Camera Adjustment feature works best for clips that don’t currently include camera motion. We hope these capabilities help you create even more dynamic content in Flow — with more precision and control. POSTED IN: Google Labs AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 17:40:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/google-labs/flow-refine-videos/</guid>
    </item>
    <item>
      <title>3 things to know about Ironwood, our latest TPU</title>
      <link>https://blog.google/products/google-cloud/ironwood-google-tpu-things-to-know/</link>
      <description><![CDATA[Today's most advanced AI models, like those powering complex thinking and calculations, need speed and efficiency from the hardware that powers them. That's why at Cloud Next in April, we unveiled Ironwood , our seventh-generation Tensor Processing Unit (TPU). Ironwood is our most powerful, capable, and energy-efficient TPU yet, designed to power thinking, inferential AI models at scale. By acting as a hugely efficient parallel processor, Ironwood excels at managing massive calculations and significantly minimizes the internal time required for data to shuttle across the chip. This breakthrough dramatically speeds up complex AI, making models run significantly faster and smoother across our cloud. And now, Ironwood is here for Cloud customers. Here are three things to know about it. 1. It’s purpose-built for the age of inference As the industry’s focus shifts from training frontier models to powering useful, responsive interactions with them, Ironwood provides the essential hardware. It’s custom built for high-volume, low-latency AI inference and model serving. It offers more than 4X better performance per chip for both training and inference workloads compared to our last generation , making Ironwood our most powerful and energy-efficient custom silicon to date. 2. It’s a giant network of power TPUs are a key component of AI Hypercomputer , our integrated supercomputing system designed to boost system-level performance and efficiency across compute, networking, storage and software. At its core, the system groups individual TPUs into interconnected units called pods. With Ironwood, we can scale up to 9,216 chips in a superpod. These chips are linked via a breakthrough Inter-Chip Interconnect (ICI) network operating at 9.6 Tb/s. Part of an Ironwood superpod, directly connecting 9,216 Ironwood TPUs in a single domain. This massive connectivity allows thousands of chips to rapidly communicate and access a staggering 1.77 Petabytes of shared High Bandwidth Memory (HBM), overcoming data bottlenecks for even the most demanding models. This efficiency significantly reduces the compute-hours and energy required for training and running cutting-edge AI services. 3. It’s designed for AI with AI Ironwood is the result of a continuous loop at Google where researchers influence hardware design, and hardware accelerates research. While competitors rely on external vendors, when Google DeepMind needs a specific architectural advancement for a model like Gemini, they collaborate directly with their TPU engineer counterparts. As a result, our models are trained on the newest TPU generations, often seeing significant speedups over previous hardware. Our researchers even use AI to design the next chip generation — a method called AlphaChip — which has used reinforcement learning to generate superior layouts for the last three TPU generations, including Ironwood. POSTED IN: Google Cloud AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/google-cloud/ironwood-google-tpu-things-to-know/</guid>
    </item>
    <item>
      <title>16 Google AI tips for stress-free holiday hosting in 2025</title>
      <link>https://blog.google/products/pixel/google-holiday-hosting-tips-2024/</link>
      <description><![CDATA[If you’re brave enough to host celebrations this holiday season, you don’t have to go it alone. Plenty of Google AI tools can make the process easier and more fun, whether you’ve held countless holiday gatherings or it’s your first time putting on your hosting hat. Try these holiday hosting tips and helpful features, from the planning phase to party time. (And check out more tips and prompts for other holiday planning , too!) Stay organized while you plan 1. Make a holiday party planner Gem in Gemini A Gem is a custom expert you can make for any task within the Gemini app (where our latest Gemini model, Gemini 3 Pro , is rolling out globally). You can give your Gem unique context and revisit your AI expert as you plan. Try instructions like, “You’re an experienced party planner who can help me host a creative holiday dinner for 10 of my closest friends.” Then you can ask for help with punny menu names, invitation wording, themed playlists or icebreakers and more. Each time, the Gem will use that earlier context to inform its response. You can even ask Gemini to help you with instructions for your Gem and share your Gem with other people — perfect if you’re co-hosting! 2. Chat with Gemini Live on the go If you find yourself wondering about decoration DIYs when you’re at the craft store, you don’t have to tap at your screen — just open up Gemini Live in the Gemini app to brainstorm together. Thanks to its camera and screen sharing capabilities, you can have a back-and-forth conversation with Gemini Live about anything you see, and Gemini Live can offer visual guidance as it responds. You can even interrupt if you want to add more details or change the topic. 3. Use Nano Banana Pro for creative pics Nano Banana Pro , our latest image generation and editing model, can help you put a funny, personal touch on your gathering. Use it in the Gemini app or AI Mode in Search to prep invitation materials, like a photo showing you inside a snowglobe, or put your pet in a festive outfit for party reminders. Nano Banana Pro can help you (and your guests) get in the holiday party spirit with visuals for party invitations and reminders with a prompt like “Create a claymation scene showing a dog dressed up like an elf in a winter wonderland.” 4. Dream up decor ideas with Mixboard Mixboard is an AI experiment from Google Labs that helps you visualize and explore your ideas. Let’s say you want to make a holiday-themed tablescape but aren’t sure what it should look like — Mixboard can help you expand and refine the different possibilities using both images and text. 5. Reference planning info with Pixel Screenshots Use Pixel Screenshots on Pixel 9 and Pixel 10 to quickly and easily search your screenshots and find the info you need. You can even create collections of screenshots for easier party planning. So if you’re hosting a get-together, create a collection for that event and save helpful screenshots of recipes, DIY ideas and texts from people you’ve invited. Then, you’ll be able to search things like “ingredients,” “craft supplies” or “arriving late” and find the relevant information from the respective screenshots. 6. Research ideas with Gemini in Chrome Gemini in Chrome has tons of features to make party planning easier. It can work across multiple tabs to consolidate information, like which of the recipes you’re considering are gluten-free. It can also help you quickly find webpages you previously visited, so you don’t have to go digging for inspiration you stumbled upon. And it works with your favorite Google apps without changing tabs, making it easier to do things like create events in your Calendar without leaving the page. 7. Use Search to shop for decor and party favors A big shopping upgrade to Search means you can shop more conversationally — tell AI Mode what decorations or favors you’re looking for as you would to a friend. When you know what you want, you can use agentic capabilities in Search to call local stores on your behalf and see what products are in stock nearby, or let you know when an item online has fallen within your budget — and even buy it for you if the merchant is eligible. Simplify the cooking process 8. Rely on Gemini for Home as your right-hand helper Gemini for Home replaces Google Assistant on your smart displays and speakers and also upgrades devices in your home like your cameras and doorbells. There are plenty of ways to use it for holiday hosting. Just say “Hey Google,” then ask Gemini to add ingredients for a cozy appetizer to your shopping list or set timers as you get to work in the kitchen. (You can even do this with your Pixel Watch 4 using its Raise to Talk feature if your hands are full.) And if you want to have an even more free-flowing conversation while you cook, you can say, "Hey Google, let's chat," to talk to Gemini Live. 1 9. Get quick visual cooking help from Google Lens With Google Lens , you can take a photo of ingredients sitting in front of you and ask your question out loud . For example, you can take a photo of your onions and ask “exactly what kind of onions are these and what’s the best way to cook them?” 10. Use Gemini to navigate dietary restrictions To make sure your guests don’t leave hungry, ask your Gem for help finding recipes everyone can enjoy based on dietary restrictions. Or try double-checking with Gemini Live that a specific ingredient really is gluten-free before you start whisking! Enjoy the party with your guests 11. Solve party emergencies with AI Overviews Spills don’t have to be a disaster: AI Overviews in Search can get you instant solutions when every second counts. Search for something like "is salt or baking soda better for stain removal?” to get the information you need with links to learn more — helping you save the day and get back to your guests in no time. 12. Handle logistics in the Google Home app Our redesigned Google Home app lets you keep the good vibes going from your phone. The app houses your favorite Nest and other smart home devices and features: You can adjust your Nest Thermostat; unlock the front door as guests arrive and lower the lights to set the mood. The consolidated Home tab now lets you swipe through your pinned Favorites, devices by type, and more. We’ve added full support for the Nest x Yale lock and lock passcode management. We’ve added full support for Nest Thermostats (2015 or later) and features like schedules and hot water boost. 13. Use Magic Cue to quickly answer texts No need to field the same questions over and over as the party gets underway — Magic Cue on Pixel 10 anticipates your needs and links information across your apps. So instead of typing the same response to every “what’s your address again?” text, Magic Cue can offer a helpful suggestion with the answer so you can get back to hosting. 14. Grab the perfect group photo with Pixel It’s very possible that you’re going to be the one corralling everyone into a group photo and taking the picture. Luckily, Pixel 10 has lots of features to help you take stellar group pics, like Add Me , which can combine two pictures taken during the same session and in the same scene so everyone — including the person who took the original shot — is in the photo. Tie it all up in a bow 15. Make easy Google Photos edits to perfect your pics New AI features in Google Photos can help you touch up party pictures before sharing them with your guests. With personalized edits, you can request fixes like opening people’s eyes mid-blink. You can also make edits simply by asking — describe the edits you want using your voice or text. 16. Craft thank you cards with Pixel and Google Photos After all the work you put into your gathering, you deserve the lightest-lift thank you cards possible. For just one idea, use “Help me edit” in Google Photos’ editor to add a festive background to party pictures! POSTED IN: AI Gemini Search Pixel Photos Google Nest Shopping]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 21 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/pixel/google-holiday-hosting-tips-2024/</guid>
    </item>
    <item>
      <title>Develop a deeper understanding with interactive images in Gemini.</title>
      <link>https://blog.google/outreach-initiatives/education/gemini-interactive-images/</link>
      <description><![CDATA[Learning science consistently shows us that true learning requires active engagement. This is fundamental to how Gemini helps you learn. Going beyond simple text and static images, we're now rolling out interactive images to the Gemini app — a new capability designed to help you visually explore complex academic concepts. Imagine studying the digestive system or the parts of the cell. Instead of just seeing a label, you can now tap or click directly on a specific part of the diagram to unlock an interactive panel. This panel provides immediate definitions, detailed explanations and content to deep-dive into. By being able to interact with images, Gemini is transforming studying from passive viewing into active exploration. Now, with certain images, you’ll be able to gain more information about topics and ask follow-up questions. This represents an important step in making learning more visual, dynamic and accessible.]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 20:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/education/gemini-interactive-images/</guid>
    </item>
    <item>
      <title>How we’re bringing AI image verification to the Gemini app</title>
      <link>https://blog.google/technology/ai/ai-image-verification-gemini-app/</link>
      <description><![CDATA[At Google, we’ve long invested in ways to provide you with helpful context about information you see online. Now, as generative media becomes increasingly prevalent and high-fidelity, we are deploying tools to help you more easily determine whether the content you're interacting with was created or edited using AI. Starting today, we’re making it easier for everyone to verify if an image was generated with or edited by Google AI right in the Gemini app, using SynthID, our digital watermarking technology that embeds imperceptible signals into AI-generated content. We introduced SynthID in 2023. Since then, over 20 billion AI-generated pieces of content have been watermarked using SynthID, and we have been testing our SynthID Detector , a verification portal, with journalists and media professionals. How it works If you see an image and want to confirm it has been made by Google AI, upload it to the Gemini app and ask a question such as: "Was this created with Google AI?" or “Is this AI-generated?” Gemini will check for the SynthID watermark and use its own reasoning to return a response that gives you more context about the content you encounter online. What’s next This launch builds on our history of providing context about images in Google Search and exploring new research innovations like Backstory from Google DeepMind. Looking ahead, we will continue to invest in more ways to empower you to determine the origin and history of content online. Soon, we’ll expand SynthID verification to support additional formats beyond images, such as video and audio, and bring these capabilities to more surfaces, such as Search. In addition to our own tools, we are collaborating with industry partners to advance content transparency and authenticity standards across our product ecosystem — including YouTube, Search, Pixel and Photos — through our role on the steering committee of the Coalition for Content Provenance and Authenticity (C2PA) . As part of this, rolling out this week, images generated by Nano Banana Pro (Gemini 3 Pro Image) in the Gemini app , Vertex AI and Google Ads will have C2PA metadata embedded, providing further transparency into how these images were created. We look forward to expanding this capability to more products and surfaces in the coming months. Over time, we will also extend our verification approach to support C2PA content credentials, meaning you'll be able to check the original source of content created by models and products that exist outside of Google’s ecosystem. This work is central to our commitment to bold and responsible AI. We look forward to further contributing to the future of AI transparency. POSTED IN: AI Gemini App Google DeepMind]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/ai-image-verification-gemini-app/</guid>
    </item>
    <item>
      <title>Build with Nano Banana Pro, our Gemini 3 Pro Image model</title>
      <link>https://blog.google/technology/developers/gemini-3-pro-image-developers/</link>
      <description><![CDATA[Today, we’re releasing Nano Banana Pro (Gemini 3 Pro Image), a higher-fidelity model built on Gemini 3 Pro for developers to access studio-quality image generation. This follows our release of Nano Banana (Gemini 2.5 Flash Image) just a few months ago. Since then, we’ve loved seeing the community put its key features to work — from character consistency to photo restoration , and even using its capabilities to make local edits in an infinite canvas. This state-of-the-art image generation and editing model is starting to roll out in paid preview to build a new wave of intelligent, multimodal applications with the Gemini API in Google AI Studio and Vertex AI for enterprises. This model unlocks high-fidelity images with higher accuracy in text rendering and robust world knowledge, supercharged by the model's ability to use grounding with Google Search to retrieve data based on the user's prompt. Gemini 3 Pro Image excels on Text to Image AI benchmarks. We are also expanding the reach of Gemini 3 Pro Image across the developer ecosystem. In Google Antigravity — our new agentic development platform — coding agents can now directly leverage these image generation capabilities to generate detailed UI mockups for user review or even new visual assets before implementing in code. Additionally, leading creative platforms are integrating the model, including Adobe and Figma. High fidelity and controls If you’re building advanced tools that require precision, Gemini 3 Pro Image gives you control over the physics (lighting, camera, focus, color grading) and composition of the image to ensure professional-quality outputs. A silhouette lost in a sea of golden bokeh and morning mist. Prompt: Replace volumetric lighting with bokeh With 2K and 4k resolution available, you can ensure outputs meet resolution standards required for professional production. Effortlessly create cohesive advertisements by combining diverse elements such as product images, logos, and references. Achieve consistent resemblance for up to five individuals, integrate six high-fidelity shots, or blend as many as fourteen standard inputs into a single, polished ad. Try our demo app that allows you to pair logos with products to create your own mockup designs. Demo app that brings product design to life with reference images. Sequences shortened. Improved text rendering and localization Gemini 3 Pro Image offers a significant leap forward from 2.5 Flash Image, transforming abstract image generation into functional assets. It excels in handling logic and language, and delivers state-of-the-art text rendering, producing clear, accurate text integrated in your images. Creative food photography where each word is artistically spelled out using the actual ingredients associated with that food. Prompt: Make 8 sophisticated minimalistic logos, each is a fun food word, and make letters from realistic food to express the meaning of this word. composition: a rendering of all logos on a single solid white background It's also an ideal solution for developing marketing collateral, educational content and numerous other applications. Try the model’s capabilities in our comic book generator app in Google AI Studio, where you can create original multi-page comic books featuring you and a friend, complete with advanced text rendering and stylization. Demo app that creates a comic book in your chosen language based on photos and selected genre. Sequences shortened. With Gemini 3 Pro Image, we’ve removed the barrier between image generation and localization logic. This advanced model grasps the semantic context of an image, enabling effortless language changes on elements like menus, signs, or documents utilizing image-to-image generation keeping original artistic style or layout. A beverage campaign concept showcasing accurate translation and rendering of English text into French. Prompt: Translate to French Access to the world’s knowledge Gemini 3 Pro Image connects a vast knowledge base to produce more factual assets over previous image generation models. Additionally, when enabled, grounding with Google Search connects the model to real-time web content for data-driven outputs. This is particularly valuable for applications requiring precise representations, such as biological diagrams or historical maps. Try this for yourself with our demo app where you can dynamically create infographics on any topic tailored to your audience. Bike care and maintenance infographic generated from a demo app that creates educational infographics. Go bananas and start building today This new model release incorporates much of the input you have already shared with us, but we aren’t stopping here. To ensure clear provenance in AI-generated media, we have integrated SynthID digital watermarks directly into every image created or edited with Gemini 3 Pro Image to denote its AI-generated or edited origin. Start by exploring our collection of apps that use Gemini 3 Pro Image to spark your imagination and see what’s possible. Once you’re inspired, remix these demo apps or integrate the model directly into your own projects via the Gemini API in Google AI Studio and Vertex AI for enterprise use. For technical details along the way, check out the documentation , prompt guide , cookbook or visit the developer forum to get help and share feedback. Use Gemini 2.5 Flash Image for faster, lower cost image generation, or 3 Pro Image for higher quality image generation, with higher cost and latency. POSTED IN: Developers AI Gemini Models]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/gemini-3-pro-image-developers/</guid>
    </item>
    <item>
      <title>Introducing Nano Banana Pro</title>
      <link>https://blog.google/technology/ai/nano-banana-pro/</link>
      <description><![CDATA[Just a few months ago we released Nano Banana , our Gemini 2.5 Flash Image model. From restoring old photos to generating mini figurines , Nano Banana was a big step in image editing that empowered casual creators to express their creativity. Today, we’re introducing Nano Banana Pro (Gemini 3 Pro Image) , our new state-of-the art image generation and editing model. Built on Gemini 3 Pro , Nano Banana Pro uses Gemini’s state-of-the-art reasoning and real-world knowledge to visualize information better than ever before. How Nano Banana Pro helps you bring any idea or design to life Nano Banana Pro can help you visualize any idea and design anything — from prototypes, to representing data as infographics, to turning handwritten notes into diagrams. With Nano Banana Pro, now you can: Generate more accurate, context-rich visuals based on enhanced reasoning, world knowledge and real-time information With Gemini 3’s advanced reasoning, Nano Banana Pro doesn’t just create beautiful images, it also helps you create more helpful content. You can get accurate educational explainers to learn more about a new subject, like context-rich infographics and diagrams based on the content you provide or facts from the real world. Nano Banana Pro can also connect to Google Search's vast knowledge base to help you create a quick snapshot for a recipe or visualize real-time information like weather or sports. An infographic of the common house plant, String of Turtles, with information on origins, care essentials and growth patterns. Prompt: Create an infographic about this plant focusing on interesting information. Step-by-step infographic for making Elaichi Chai (cardamom tea), demonstrating the ability to visualize recipes and real-world information. Prompt: Create an infographic that shows how to make elaichi chai We used Nano Banana Pro to pull in real-time weather via Search grounding to build a pop-art infographic. Generate better visuals with more accurate, legible text directly in the image in multiple languages Nano Banana Pro is the best model for creating images with correctly rendered and legible text directly in the image, whether you’re looking for a short tagline, or a long paragraph. Gemini 3 is great at understanding depth and nuance, which unlocks a world of possibilities with image editing and generation — especially with text. Now you can create more detailed text in mockups or posters with a wider variety of textures, fonts and calligraphy. With Gemini’s enhanced multilingual reasoning, you can generate text in multiple languages, or localize and translate your content so you can scale internationally and/or share content more easily with friends and family. A black and white storyboard sketch showing an establishing shot, medium shot, close-up, and POV shot for a film scene. Prompt: Create a storyboard for this scene The word 'BERLIN' integrated into the architecture of a city block, spanning across multiple buildings. Prompt: View of a cozy street in Berlin on a bright sunny day, stark shadows. the old houses are oddly shaped like letters that spell out "BERLIN" Colored in Blue, Red, White and black. The houses still look like houses and the resemblance to letters is subtle. Calligraphy inspired by meaning, showcasing the ability to generate expressive text with a wider variety of textures and fonts. Prompt: make 8 minimalistic logos, each is an expressive word, and make letters convey a message or sound visually to express the meaning of this word in a dramatic way. composition: flat vector rendering of all logos in black on a single white background A beverage campaign concept showcasing accurate translation and rendering of English text into Korean. Prompt: translate all the English text on the three yellow and blue cans into Korean, while keeping everything else the same A graphic design featuring the word 'TYPOGRAPHY' with a retro, screen-printed texture. Prompt: A vibrant, eye-catching "TYPOGRAPHY" design on a textured off-white background. The letters are bold, blocky, extra condensed and create a 3D effect with overlapping layers of bright blue and hot pink, each with a halftone dot pattern, evoking a retro print aesthetic. 16:9 aspect ratio Blending text and texture in a creative way by integrating the phrase into a woodchopping scene. Prompt: Create an image showing the phrase "How much wood would a woodchuck chuck if a woodchuck could chuck wood" made out of wood chucked by a woodchuck. Create high-fidelity visuals with upgraded creative capabilities Consistency by design: With Nano Banana Pro, you can blend more elements than ever before, using up to 14 images and maintaining the consistency and resemblance of up to 5 people. Whether turning sketches into products or blueprints into photorealistic 3D structures, you can now bridge the gap between concept and creation. Apply your desired visual look and feel to your mockups with ease, ensuring your branding remains seamless and consistent across every touchpoint. Maintaining the consistency of up to 14 inputs, including multiple characters, across a complex composition. Prompt: A medium shot of the 14 fluffy characters sitting squeezed together side-by-side on a worn beige fabric sofa and on the floor. They are all facing forwards, watching a vintage, wooden-boxed television set placed on a low wooden table in front of the sofa. The room is dimly lit, with warm light from a window on the left and the glow from the TV illuminating the creatures' faces and fluffy textures. The background is a cozy, slightly cluttered living room with a braided rug, a bookshelf with old books, and rustic kitchen elements in the background. The overall atmosphere is warm, cozy, and amused. Craft lifestyle scenes by combining multiple elements. Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format and change the dress on the mannequin to the dress in the image Create surreal landscapes by combining multiple input elements. Prompt: Combine these images into one appropriately arranged cinematic image in 16:9 format A high-fashion editorial shot set in a desert landscape that maintains the consistency and resemblance of the people from the 6 input photos. Prompt: Put these five people and this dog into a single image, they should fit into a stunning award-winning shot in the style if [sic] a fashion editorial. The identity of all five people and their attire and the dog must stay consistent throughout but they can and should be seen from different angles and distances in [sic] as is most natural and suitable to the scene. Make the colour and lighting look natural on them all, they look like they naturally fit into this fashion show. Studio-quality creative controls: With Nano Banana Pro's new capabilities we are putting advanced creative controls directly into your hands. Select, refine and transform any part of an image with improved localized editing. Adjust camera angles, change the focus and apply sophisticated color grading, or even transform scene lighting (e.g. changing day to night or creating a bokeh effect). Your creations are ready for any platform, from social media to print, thanks to a range of available aspect ratios and available 2K and 4K resolution Change the look and feel of an image for a range of platforms by adapting the aspect ratio. Prompt: change aspect ratio to 1:1 by reducing background. The character, remains exactly locked in its current position Lighting and focus controls applied to transform a scene from day to night. Prompt: Turn this scene into nighttime Obscure or enlighten a section of your image with lighting controls to achieve specific dramatic effects. Prompt: Generate an image with an intense chiaroscuro effect. The man should retain his original features and expression. Introduce harsh, directional light, appearing to come from above and slightly to the left, casting deep, defined shadows across the face. Only slivers of light illuminating his eyes and cheekbones, the rest of the face is in deep shadow. Bring out the details of your composition by adjusting the depth of field or focal point (e.g., focusing on the flowers). Prompt: Focus on the flowers How you can try Nano Banana Pro today Across our products and services, you now have a choice: the original Nano Banana for fast, fun editing, or Nano Banana Pro for complex compositions requiring the highest quality and visually sophisticated results. Consumers and students : Rolling out globally in the Gemini app when you select ‘Create images’ with the ‘Thinking’ model. Our free-tier users will receive limited free quotas, after which they will revert to the original Nano Banana model. Google AI Plus, Pro and Ultra subscribers receive higher quotas. For AI Mode in Search, Nano Banana Pro is available in the U.S. for Google AI Pro and Ultra subscribers. For NotebookLM , Nano Banana Pro is also available for subscribers globally. Professionals : We're upgrading image generation in Google Ads to Nano Banana Pro to put cutting-edge creative and editing power directly into the hands of advertisers globally. It’s also rolling out starting today to Workspace customers in Google Slides and Vids . Developers and enterprise : Starting to roll out in the Gemini API and Google AI Studio , and in Google Antigravity to create rich UX layouts & mockups; enterprises can start building in Vertex AI for scaled creation today and it’s coming soon to Gemini Enterprise. Creatives: Starting to roll out to Google AI Ultra subscribers in Flow , our AI filmmaking tool, to give creatives, filmmakers and marketers even more precision and control over their frames and scenes. How to identify AI-generated images in the Gemini app We believe it’s critical to know when an image is AI-generated. This is why all media generated by Google’s tools are embedded with our imperceptible SynthID digital watermark. Today, we are putting a powerful verification tool directly in consumers’ hands: you can now upload an image into the Gemini app and simply ask if it was generated by Google AI, thanks to SynthID technology. We are starting with English language prompts for images, and will expand to more languages, audio and video soon. In addition to SynthID, we will maintain a visible watermark (the Gemini sparkle) on images generated by free and Google AI Pro tier users, to make images even more easy to detect as Google AI-generated. Recognizing the need for a clean visual canvas for professional work, we will remove the visible watermark from images generated by Google AI Ultra subscribers and within the Google AI Studio developer tool. You can find out more about how we’re increasing transparency in AI content with SynthID in our blog post . POSTED IN: AI Gemini Models Gemini App Search]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 15:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/nano-banana-pro/</guid>
    </item>
    <item>
      <title>How Westinghouse is reenergizing nuclear power with — and for — AI</title>
      <link>https://blog.google/products/google-cloud/westinghouse/</link>
      <description><![CDATA[Earlier this year, Westinghouse publicly shared its plan with the White House to have 10 of its state-of-the-art AP1000 nuclear reactors under construction by 2030. The new reactors would help address growing demands on the U.S. energy grid from AI and other sources, and when completed, they would provide enough combined power to electrify 7.5 million households. That’s roughly every household in the five largest U.S. cities plus a few data centers. Yet reactor construction has slowed down in recent decades due to complex approvals and building processes. To reach its ambitious goal, Westinghouse has partnered with Google Cloud to develop a custom AI-powered platform using specialized models from both Google and Westinghouse — itself a leader in AI for energy production — that helps optimize and accelerate reactor construction. So far, early pilots of the platform have shown significant time and cost savings, and the companies are also exploring ways for AI to help enhance nuclear operations and safety. Here’s a closer look . AI fuels itself The partnership is driven by a pressing global challenge of meeting the ever-increasing energy demands with carbon free power. Dr. Lou Martinez Sancho, Westinghouse's CTO and Executive Vice President of R&D and Innovation, framed the core idea as "energy for AI and AI for energy." Nuclear is notable for offering clean, reliable power at an immense scale from a small footprint. With the United States projected to need 400 gigawatts of new power by 2040 — a 32% increase from current usage — conventional construction timelines are insufficient. The Vogtle Electric Generating Plant in Georgia was home to the first AP1000 reactors, completed in 2023 and 2024. (Credit: Georgia Power) A foundation for AI Westinghouse CEO Dan Sumner has stressed that AI-driven decision-making optimization is essential to making nuclear power a viable investment for utilities and addressing energy urgency. A major factor in the partnership’s success has been Westinghouse's AI readiness. The company already established its own proprietary AI infrastructure, Hive, designed specifically to meet nuclear's stringent regulatory and export-control frameworks. They also developed Bertha, a generative AI assistant that instantly accesses 75 years of meticulous nuclear knowledge and documentation at Westinghouse. Google engineers were impressed that a 140-year-old company had quietly assembled the exact foundation needed to deploy AI securely in a heavily regulated environment. Building on data The core AI innovation targets the construction process — historically 60% of reactor costs. Until recently, construction management relied on spreadsheets and paper documents, leading to delays that cascaded across thousands of interdependent tasks. But with AI, Westinghouse turned decades of documentation to its advantage. The new AI system combines the companies’ AI models and prediction tools with Westinghouse’s WNEXUS, a 3D digital twin of its reactors. When combined with current and historic data, the system can predict bottlenecks, optimize construction task sequences, adjust staffing levels, and account for external factors like supply chain constraints. Vogtle units 3 and 4 joined a pair of reactors from the 1980s. Westinghouse aims to have 10 more reactors under construction in the United States by the end of this decade — with AI helping accelerate those timelines. (Credit: Georgia Power) AI as a "technology brick" The optimization technology for new reactor construction is built using a "technology brick approach" Martinez Sancho said, meaning it has applications far beyond the initial project. The same AI tools are already being applied to reduce licensing processes and operations timing. By finding the fastest path through maintenance and refueling tasks, the AI helps minimize reactor downtime. Westinghouse views AI not as a tool for simple cost optimization, but instead as a catalyst for a revolution that transforms the energy sector and turns decades of documentation into an asset that accelerates progress. POSTED IN: Google Cloud AI]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 14:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/google-cloud/westinghouse/</guid>
    </item>
    <item>
      <title>At our Research@ Poland event we shared how AI is helping us solve big challenges.</title>
      <link>https://blog.google/technology/research/ai-collaboration-poland-2025/</link>
      <description><![CDATA[During a keynote at our Research@ event in Poland last week, Yossi Matias, Head of Google Research shared how his team’s work is guided by the " magic cycle " — where real-world challenges drive foundational research and scientific breakthroughs. Our latest Research@ event gathered hundreds of researchers, academics, policymakers and partners to explore how we continue this cycle, turning complex research into real-world solutions that serve people in their daily lives. A clear theme emerged across every conversation: collaboration. From deploying Google Earth AI to help tackle public health and disaster response challenges, creating new methods for brain mapping and supporting responsible AI literacy and education , to developing AI that can act as a virtual collaborator for scientists — partnership has been key, and we’re excited to see what we can build together next.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 19 Nov 2025 09:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/research/ai-collaboration-poland-2025/</guid>
    </item>
    <item>
      <title>Google Search with Gemini 3: Our most intelligent search yet</title>
      <link>https://blog.google/products/search/gemini-3-search-ai-mode/</link>
      <description><![CDATA[Today, we introduced Gemini 3 , our most intelligent model with state-of-the-art reasoning, deep multimodal understanding and powerful agentic capabilities. It’s now available in Google Search, starting with AI Mode — marking the first time we’ve brought a Gemini model to Search on day one. Gemini 3 brings incredible reasoning power to Search because it’s built to grasp unprecedented depth and nuance for your hardest questions. It also unlocks new generative UI experiences so you can get dynamic visual layouts with interactive tools and simulations — generated specifically for you. Here’s how Gemini 3 is supercharging Search. Gemini 3: Our most intelligent model, right in Search Starting today, Google AI Pro and Ultra subscribers in the U.S. can use Gemini 3 Pro, our first model in the Gemini 3 family of models, by selecting “Thinking” from the model drop-down menu in AI Mode. With Gemini 3, you can tackle your toughest questions and learn more interactively because it better understands the intent and nuance of your request. And soon, we’ll bring Gemini 3 in AI Mode to everyone in the U.S. with higher limits for users with the Google AI Pro and Ultra plans. Thanks to Gemini 3’s advanced reasoning, Google Search’s query fan-out technique is getting a major upgrade. Now, not only can it perform even more searches to uncover relevant web content, but because Gemini more intelligently understands your intent it can find new content that it may have previously missed. This means Search can help you find even more credible, highly relevant content for your specific question. And in the coming weeks, we’re also enhancing our automatic model selection in Search with Gemini 3. This means Search will intelligently route your most challenging questions in AI Mode and AI Overviews to this frontier model — while continuing to use faster models for simpler tasks. This will be rolling out to Google AI Pro and Ultra subscribers in the U.S. Generative UI: Visual layouts, interactive tools and simulations in AI Mode Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities are also unlocking more bespoke generative user interfaces. Now, Gemini 3 in AI Mode can dynamically create the ideal visual layout for responses on the fly — featuring interactive tools and simulations — tailored to your query. To do this, Gemini 3 analyzes your question and creates the most helpful layout, building a custom response with visual elements — like images, tables and grids — so the final output isn’t just informative, but clear and actionable. When the model detects that an interactive tool will help you better understand the topic, it uses its generative capabilities to code a custom simulation or tool in real-time and adds it into your response. Say you’re learning about the physics behind the three-body problem. Instead of just reading about it, you can now get an interactive simulation, allowing you to manipulate variables and see the gravitational interactions play out. Or perhaps you're researching mortgage loans: Gemini 3 in AI Mode can make you a custom-built interactive loan calculator directly in the response so you can compare two different options and see which offers the most long-term savings. And to help you continue exploring, all responses have prominent links to high-quality content across the web. For a deeper dive, check out Google’s foundational generative UI research . We’ll continue to refine the experience over time, and we look forward to your feedback as you start to use these interactive tools and simulations in Search. Now, it's even easier to ask anything and instantly get a richer, more helpful understanding. We’re excited for you to try this more interactive and capable Search. POSTED IN: Search Gemini Models AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 18 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-3-search-ai-mode/</guid>
    </item>
    <item>
      <title>A new era of intelligence with Gemini 3</title>
      <link>https://blog.google/products/gemini/gemini-3/</link>
      <description><![CDATA[A note from Google and Alphabet CEO Sundar Pichai: Nearly two years ago we kicked off the Gemini era, one of our biggest scientific and product endeavors ever undertaken as a company. Since then, it’s been incredible to see how much people love it. AI Overviews now have 2 billion users every month. The Gemini app surpasses 650 million users per month, more than 70% of our Cloud customers use our AI, 13 million developers have built with our generative models, and that is just a snippet of the impact we’re seeing. And we’re able to get advanced capabilities to the world faster than ever, thanks to our differentiated full stack approach to AI innovation — from our leading infrastructure to our world-class research and models and tooling, to products that reach billions of people around the world. Every generation of Gemini has built on the last, enabling you to do more. Gemini 1’s breakthroughs in native multimodality and long context window expanded the kinds of information that could be processed — and how much of it. Gemini 2 laid the foundation for agentic capabilities and pushed the frontiers on reasoning and thinking , helping with more complex tasks and ideas, leading to Gemini 2.5 Pro topping LMArena for over six months. And now we’re introducing Gemini 3, our most intelligent model, that combines all of Gemini’s capabilities together so you can bring any idea to life. It’s state-of-the-art in reasoning, built to grasp depth and nuance — whether it’s perceiving the subtle clues in a creative idea, or peeling apart the overlapping layers of a difficult problem. Gemini 3 is also much better at figuring out the context and intent behind your request, so you get what you need with less prompting. It’s amazing to think that in just two years, AI has evolved from simply reading text and images to reading the room. And starting today, we’re shipping Gemini at the scale of Google. That includes Gemini 3 in AI Mode in Search with more complex reasoning and new dynamic experiences. This is the first time we are shipping Gemini in Search on day one. Gemini 3 is also coming today to the Gemini app , to developers in AI Studio and Vertex AI , and in our new agentic development platform, Google Antigravity — more below. Like the generations before it, Gemini 3 is once again advancing the state of the art. In this new chapter, we’ll continue to push the frontiers of intelligence, agents, and personalization to make AI truly helpful for everyone. We hope you like Gemini 3, we'll keep improving it, and look forward to seeing what you build with it. Much more to come! Introducing Gemini 3: our most intelligent model that helps you bring any idea to life Demis Hassabis, CEO of Google DeepMind and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect, Google, on behalf of the Gemini team Today we’re taking another big step on the path toward AGI and releasing Gemini 3. It’s the best model in the world for multimodal understanding and our most powerful agentic and vibe coding model yet, delivering richer visualizations and deeper interactivity — all built on a foundation of state-of-the-art reasoning. We’re beginning the Gemini 3 era by releasing Gemini 3 Pro in preview and making it available today across a suite of Google products so you can use it in your daily life to learn, build and plan anything. We’re also introducing Gemini 3 Deep Think — our enhanced reasoning mode that pushes Gemini 3 performance even further — and giving access to safety testers before making it available to Google AI Ultra subscribers. State-of-the-art reasoning with unprecedented depth and nuance Gemini 3 Pro can bring any idea to life with its state-of-the-art reasoning and multimodal capabilities. It significantly outperforms 2.5 Pro on every major AI benchmark. It tops the LMArena Leaderboard with a breakthrough score of 1501 Elo. It demonstrates PhD-level reasoning with top scores on Humanity’s Last Exam (37.5% without the usage of any tools) and GPQA Diamond (91.9%). It also sets a new standard for frontier models in mathematics, achieving a new state-of-the-art of 23.4% on MathArena Apex . Beyond text, Gemini 3 Pro redefines multimodal reasoning with 81% on MMMU-Pro and 87.6% on Video-MMMU. It also scores a state-of-the-art 72.1% on SimpleQA Verified, showing great progress on factual accuracy. This means Gemini 3 Pro is highly capable at solving complex problems across a vast array of topics like science and mathematics with a high degree of reliability. Gemini 3 is state-of-the-art across a range of key AI benchmarks. See details on our evaluation methodology . Gemini 3 Pro also brings a new level of depth and nuance to every interaction. Its responses are smart, concise and direct, trading cliché and flattery for genuine insight — telling you what you need to hear, not just what you want to hear. It acts as a true thought partner that gives you new ways to understand information and express yourself, from translating dense scientific concepts by generating code for high-fidelity visualizations to creative brainstorming. Gemini 3 can code a visualization of plasma flow in a tokamak and write a poem capturing the physics of fusion. Gemini 3 Deep Think Gemini 3 Deep Think mode pushes the boundaries of intelligence even further, delivering a step-change in Gemini 3’s reasoning and multimodal understanding capabilities to help you solve even more complex problems. In testing, Gemini 3 Deep Think outperforms Gemini 3 Pro’s already impressive performance on Humanity’s Last Exam (41.0% without the use of tools) and GPQA Diamond (93.8%). It also achieves an unprecedented 45.1% on ARC-AGI-2 (with code execution, ARC Prize Verified), demonstrating its ability to solve novel challenges. Gemini 3 Deep Think mode excels on some of the most challenging AI benchmarks. See details on our evaluation methodology . Gemini 3 helps you learn, build and plan anything Learn anything Gemini was built from the start to seamlessly synthesize information about any topic across multiple modalities, including text, images, video, audio and code. Gemini 3 pushes the frontier of multimodal reasoning to help you learn in ways that make sense for you by combining its state-of-the-art reasoning, vision and spatial understanding, leading multilingual performance, and 1 million-token context window. For example, if you want to learn how to cook in your family tradition, Gemini 3 can decipher and translate handwritten recipes in different languages into a shareable family cookbook. Or if you want to learn about a new topic, you can give it academic papers, long video lectures or tutorials and it can generate code for interactive flashcards, visualizations or other formats that will help you master the material. It can even analyze videos of your pickleball match, identify areas where you can improve and generate a training plan for overall form improvements. Gemini 3 can help you learn and preserve family cooking traditions. Try it in Gemini Canvas. Gemini 3 can help you analyze complex information like research papers and can generate code for an interactive guide. Get expert-level sports analysis on your pickleball match to help improve your game. To help you make better sense of information on the web, AI Mode in Search now uses Gemini 3 to enable new generative UI experiences like immersive visual layouts and interactive tools and simulations, all generated completely on the fly based on your query. Learn a complex topic like how RNA polymerase works with generative UI in AI Mode in Search. Build anything Building on the success of 2.5 Pro, Gemini 3 delivers on the promise of bringing any idea to life for developers. It’s exceptional at zero-shot generation and handles complex prompts and instructions to render richer, more interactive web UI. Gemini 3 is the best vibe coding and agentic coding model we’ve ever built – making our products more autonomous and boosting developer productivity. It tops the WebDev Arena leaderboard by scoring an impressive 1487 Elo. It also scores 54.2% on Terminal-Bench 2.0, which tests a model’s tool use ability to operate a computer via terminal and it greatly outperforms 2.5 Pro on SWE-bench Verified (76.2%), a benchmark that measures coding agents. You can now build with Gemini 3 in Google AI Studio, Vertex AI, Gemini CLI and our new agentic development platform, Google Antigravity. It’s also available in third-party platforms like Cursor, GitHub, JetBrains, Manus, Replit and more. Code a retro 3D spaceship game with richer visualizations and improved interactivity. Try it in AI Studio. Bring your imagination to life by building, deconstructing and remixing detailed 3D voxel art using code. Try it in AI Studio. Build a playable sci-fi world with shaders using Gemini 3. Try it in AI Studio. You can vibe code richer, more interactive web UI and apps with Gemini 3. Introducing a new agent-first development experience As model intelligence accelerates with Gemini 3, we have the opportunity to reimagine the entire developer experience. Today we’re releasing Google Antigravity , our new agentic development platform that enables developers to operate at a higher, task-oriented level. Using Gemini 3’s advanced reasoning, tool use and agentic coding capabilities, Google Antigravity transforms AI assistance from a tool in a developer’s toolkit into an active partner. While the core of Google Antigravity is a familiar AI IDE experience, its agents have been elevated to a dedicated surface and given direct access to the editor, terminal and browser. Now, agents can autonomously plan and execute complex, end-to-end software tasks simultaneously on your behalf while validating their own code. In addition to Gemini 3 Pro, Google Antigravity also comes tightly coupled with our latest Gemini 2.5 Computer Use model for browser control and our top-rated image editing model Nano Banana (Gemini 2.5 Image). Google Antigravity uses Gemini 3 to drive an end-to-end agentic workflow for a flight tracker app. The agent independently plans, codes the application and validates its execution through browser-based computer use. Plan anything Since introducing the agentic era with Gemini 2, we’ve made a lot of progress, not only advancing Gemini’s coding agent abilities, but also improving its ability to reliably plan ahead over longer horizons. Gemini 3 demonstrates this by topping the leaderboard on Vending-Bench 2 , which tests longer horizon planning by managing a simulated vending machine business. Gemini 3 Pro maintains consistent tool usage and decision-making for a full simulated year of operation, driving higher returns without drifting off task. Gemini 3 Pro demonstrates better long-horizon planning to generate significantly higher returns compared to other frontier models. This means Gemini 3 can better help you get things done in everyday life. By combining deeper reasoning with improved, more consistent tool use, Gemini 3 can take action on your behalf by navigating more complex, multi-step workflows from start to finish — like booking local services or organizing your inbox — all while under your control and guidance. Google AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini Agent today. We’ve learned a lot improving Gemini’s agentic capabilities, and we’re excited to see how you use it as we expand to more Google products soon. Gemini Agent can help you organize your Gmail inbox. Try it now in the Gemini app for Google AI Ultra subscribers . Building Gemini 3 responsibly Gemini 3 is our most secure model yet, and has undergone the most comprehensive set of safety evaluations of any Google AI model to date. The model shows reduced sycophancy, increased resistance to prompt injections and improved protection against misuse via cyberattacks. In addition to our in-house testing for the critical domains in our Frontier Safety Framework , we've also partnered on evaluations with world-leading subject matter experts, provided early access to bodies like the UK AISI, and obtained independent assessments from industry experts like Apollo, Vaultis, Dreadnode and more. For more information, see the Gemini 3 model card . The next era of Gemini This is just the start of the Gemini 3 era. As of today, Gemini 3 starts rolling out: For everyone in the Gemini app and for Google AI Pro and Ultra subscribers in AI Mode in Search For developers in the Gemini API in AI Studio, our new agentic development platform, Google Antigravity; and Gemini CLI For enterprises in Vertex AI and Gemini Enterprise For Gemini 3 Deep Think mode, we’re taking extra time for safety evaluations and input from safety testers before making it available to Google AI Ultra subscribers in the coming weeks. We plan to release additional models to the Gemini 3 series soon so you can do more with AI. We look forward to getting your feedback and seeing what you learn, build and plan with Gemini. POSTED IN: Gemini Models AI Google DeepMind]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 18 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/gemini/gemini-3/</guid>
    </item>
    <item>
      <title>Investing in America 2025</title>
      <link>https://blog.google/inside-google/company-announcements/investing-in-america-2025/</link>
      <description><![CDATA[Google's investments across the U.S are helping enable this extraordinary time for American innovation. Through major investments in technical infrastructure, research and development — along with expanded energy capacity for an AI-driven economy and workforce development and education programs — we will help the U.S. continue to lead the world in AI. These investments also unlock substantial economic opportunity for American businesses — advancing scientific breakthroughs, fortifying cybersecurity for the U.S., and creating new career opportunities for millions of Americans. In recent months, we’ve been bringing this new era of American innovation to life in communities across the country, with more to come.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 17 Nov 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/inside-google/company-announcements/investing-in-america-2025/</guid>
    </item>
  </channel>
</rss>
