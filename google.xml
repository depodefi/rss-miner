<?xml version="1.0" ?>
<rss xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0">
  <channel>
    <title>Google AI Blog</title>
    <link>https://blog.google/technology/ai/</link>
    <description><![CDATA[Latest news from Google AI]]></description>
    <language>en-US</language>
    <lastBuildDate>Thu, 11 Dec 2025 10:04:45 GMT</lastBuildDate>
    <generator>rfeed v1.1.1</generator>
    <docs>https://github.com/svpino/rfeed/blob/master/README.md</docs>
    <item>
      <title>These developers are changing lives with Gemma 3n</title>
      <link>https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X When Gemma 3n was released, we hoped developers would use its on-device, multimodal capabilities to make a difference in people’s lives. With more than 600 projects submitted to the Gemma 3n Impact Challenge on Kaggle, the community delivered on that promise. Today, we’re excited to introduce the winners: First Place: Gemma Vision Gemma Vision is an AI assistant designed for visually impaired people. The developer’s brother, who is blind, played a vital role in ensuring features were genuinely helpful for the blind community. Because holding a phone can be impractical while using a cane, the system was designed to process visuals from a phone camera strapped to the user’s chest. Functions can be triggered using a 8BitDo Micro controller or voice commands, allowing users to perform actions without navigating touchscreen menus. This project also won the Special Technology Prize for Google AI Edge , a platform for deploying models on-device. It deployed Gemma 3n using the MediaPipe LLM Inference API and leveraged features like streamed responses in the flutter_gemma package to make the experience fluid. Second Place: Vite Vere Offline Vite Vere helps foster autonomy for people with cognitive disabilities. Originally developed using the Gemini API, this project leveraged Gemma 3n to make the digital companion work offline. By transforming images to simple instructions that can then be read aloud using the local device’s text-to-speech engine, the app enables users to navigate daily tasks. Third Place: 3VA For decades, Eva, a brilliant graphic designer with cerebral palsy, was limited to simple commands like “want food now.” This project fine-tuned Gemma 3n to translate pictograms into rich expressions that better reflect Eva’s voice. The team trained the model locally using Apple’s MLX framework, demonstrating a cost-effective way to develop personalized Augmentative and Alternative Communication (AAC) technology. Fourth Place: Sixth Sense for Security Guards Unlike traditional video monitoring systems that just detect motion, this project used Gemma 3n to provide human-level context and distinguish benign events from genuine threats. By integrating a lightweight YOLO-NAS model to detect initial movement and send it to Gemma 3n for processing, the system can handle high-bandwidth video feeds (up to 360fps and 16 cameras) in real time. The Unsloth Prize: Dream Assistant Voice assistants frequently fail users with speech impairments. This project used Unsloth , a library for efficient fine-tuning, to train Gemma 3n on an individual’s audio recordings. The result is a custom AI assistant that understands the user’s unique speech patterns and enables voice control over device functions. The Ollama Prize: LENTERA This project demonstrates how to bring AI to disconnected regions by transforming affordable hardware into offline microservers. Lentera broadcasts a local WiFi hotspot, allowing users to connect their devices to an educational hub running Gemma 3n via Ollama , a platform for local model deployment. The LeRobot Prize: Graph-based Cost Learning and Gemma 3n for Sensing Robotic exploration is often bottlenecked by the time spent sensing rather than moving. To solve this, the team built a novel “scanning-time-first” pipeline on top of LeRobot , a robotics framework developed by Hugging Face. This project used Gemma 3n to create plans while an inductive graph-based matrix completion (IGMC) model predicted latencies, demonstrating the viability of embodied AI at the edge. The NVIDIA Jetson Prize: My (Jetson) Gemma Integrating AI into our physical environment requires systems that are both responsive and energy-efficient. This project used a smart CPU-GPU hybrid processing strategy to deploy a context-aware voice interface on an NVIDIA Jetson Orin , demonstrating how helpful AI can move beyond screens to assist users in the real world. From accessibility to crisis response, these projects show what's possible with Gemma 3n. Many others deserve recognition, so join us as we highlight a developer story every day on @googleaidevs over the coming month. POSTED IN: Developers AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 10 Dec 2025 17:15:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/developers-changing-lives-with-gemma-3n/</guid>
    </item>
    <item>
      <title>Learn more about AI in the workplace in our new research report.</title>
      <link>https://blog.google/products/workspace/gemini-ai-workplace-research-report/</link>
      <description><![CDATA[A new global survey of executives, decision makers and knowledge workers reveals that organizations truly transforming with AI are seeing real results that move their businesses and allow employees to focus on meaningful work. The biggest gains from adopting AI aren’t about saving time — they’re about expanding potential. Highly transformed organizations surveyed report that AI: Increases innovation by 57% Decreases time spent on mundane tasks by 39% Improves work creativity by 65% Read the full research report, “ Beyond AI Optimism: Five ways to move your business from saving time to sparking innovation ,” or visit the Workspace blog for key takeaways.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 17:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/gemini-ai-workplace-research-report/</guid>
    </item>
    <item>
      <title>2025 at Google</title>
      <link>https://blog.google/technology/ai/look-back-2025/</link>
      <description><![CDATA[It’s (almost) a wrap on 2025! As we prepare for a great new year, let’s take a quick rewind and remember some of this past year’s most exciting launches, biggest moments and more.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 09 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/look-back-2025/</guid>
    </item>
    <item>
      <title>How we’re supporting the next generation of innovators</title>
      <link>https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</link>
      <description><![CDATA[For decades, Google has supported Computer Science Education Week (CSEdWeek) to demystify coding and computational thinking. This year during CSEdWeek, we’re taking part in the global Hour of AI alongside our partners Code.org and the Computer Science Teachers Association (CSTA), to help educators and students better understand, use and create with AI. Towards this goal, we’re launching a new quest in our gamified experience, AI Quests , bringing AI literacy to classrooms with a hands-on approach, and we’re announcing over $5 million in Google.org funding for computer science teaching. Launching a new quest Hundreds of Googler volunteers around the world are visiting classrooms this week to lead AI Quests , our game-based learning series developed with the Stanford Accelerator for Learning . Today, we’re releasing a new quest where students step into the shoes of researchers and use an AI model to detect eye disease and prevent blindness. This quest is inspired by our real-world research on diabetic retinopathy . It can be accessed for free along with our existing flood forecasting quest and accompanying resources for teachers. We’re also working closely with partners to bring AI Quests to more classrooms in 2026. One collaboration is with the Raspberry Pi Foundation and Google DeepMind, which has expanded their Experience AI program — recognized by UNESCO for promoting responsible AI education — to millions of students with funding from Google.org and integrated AI Quests into their curriculum. Investing in educators We’re announcing over $5 million in new Google.org funding to bolster computer science teaching in the age of AI. This builds on our recent $30 million global commitment to learning and foundational research, as well as the over $240 million we have provided to advance computer science education globally. The new funding will help organizations like California State University, Dominguez Hills to prepare teachers to deliver foundational computer science and AI curriculum to K-12 students and CSTA to publish the revised K–12 Computer Science Standards through a modern, accessible web-presence. While coding tasks may change in the AI era, the foundational principles of computer science remain more vital than ever. POSTED IN: Learning & Education Google.org AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/education/next-gen-computer-science-innovators/</guid>
    </item>
    <item>
      <title>Transforming Nordic classrooms through responsible AI partnerships</title>
      <link>https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</link>
      <description><![CDATA[As AI transforms the classroom, school districts in Northern Europe are setting a global standard for innovation. Beyond mere adoption, districts across Iceland, Norway, and Sweden are prioritizing responsible use. By partnering with educators and administrators, they are ensuring Google’s AI tools are integrated thoughtfully, securely, and with real value for students. Providing personalized learning in Icelandic schools Building on the widespread adoption of Google Classroom in Iceland, we partnered with the Ministry of Education to launch a pilot with 300 teachers. By integrating Gemini for Education and NotebookLM, the program aims to enhance personalized learning and boost AI literacy. Beyond developing fluency, the pilot identifies the resources teachers need for ethical use, while upskilling them to streamline administrative tasks and pinpoint student learning gaps. Saving teachers time while building AI literacy in Sweden Through a partnership with school districts across Sweden to bring Gemini for Education to nearly 30,000 students and faculty members, schools are embracing AI’s potential to transform learning. Teachers are using Gemini to create high-quality, tailored materials — a shift one educator describes as "revolutionary" because it allows them to invest more time with their students. According to Johan Kellén, teacher & ICT Coordinator at Linköping Municipality, it takes so long to produce good teaching material — you usually have to construct it yourself and ensure it is up to date and current, and adapted to each class and sometimes each student. This is where Gemini is able to help. Districts hosted workshops to empower educators to use Gemini and foster dialogue with older students about utilizing Guided Learning mode as a study aid. They believe AI literacy is a shared responsibility. Leading the charge for safe, secure digital learning in Norway In a landmark move for digital privacy, Norway completed a national Data Protection Impact Assessment (DPIA), greenlighting the use of Google Workspace for Education and ChromeOS in schools. This collaborative achievement between Google Cloud and the Norwegian Association of Local and Regional Authorities (KS) is a prime example of public sector efficiency. By conducting a centrally driven DPIA, KS eliminated the need for every single municipality to conduct their own complex assessments, highlighting how Google tools meet stringent GDPR requirements and ensuring that local resources focus on innovation over administrative compliance. By prioritizing trust and partnering with Google, Norway has secured a safe, innovative learning environment for students while saving IT administrators significant time and resources. POSTED IN: Google in Europe Learning & Education AI]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 08 Dec 2025 10:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-europe/transforming-nordic-classrooms-through-responsible-ai-partnerships/</guid>
    </item>
    <item>
      <title>The latest AI news we announced in November</title>
      <link>https://blog.google/technology/ai/google-ai-updates-november-2025/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X For more than 20 years, we’ve invested in machine learning and AI research, tools and infrastructure to build products that make everyday life better for more people. Teams across Google are working on ways to unlock AI’s benefits in fields as wide-ranging as healthcare, crisis response and education. To keep you posted on our progress, we're doing a regular roundup of Google's most recent AI news. Here’s a look back at some of our AI announcements from November. November ushered in a new era of intelligence: with the launch of Gemini 3, we're improving everyone's ability to learn, plan or just get things done. From "vibe coding" with our powerful models, to generating professional grade visuals with Nano Banana Pro, to the autonomous workflows in our new Google Antigravity platform, the distance between a spark of imagination and reality has never been shorter. So, whether you’re a developer building complex agents or a traveler planning for the upcoming holidays with Canvas in AI Mode, these updates turn AI into a proactive partner ready to help you get things done, all season long. We released Gemini 3, AI for a new era of intelligence . Gemini 3 is built to bring any idea to life. It represents the next step in our work to push the frontiers of intelligence, agentic experiences and personalization — so that AI is truly helpful for everyone. Gemini 3 is the best model in the world for multimodal understanding and it's our most powerful agentic and vibe coding model to date. For developers, Gemini 3 Pro outperforms previous versions across major AI benchmarks. Gemini 3’s upgraded smarts and new capabilities are now available in the Gemini app , and you can review our hub with all the Gemini 3 announcements . We made Gemini 3 available in Google Search for our most intelligent search yet . Gemini 3’s state-of-the-art reasoning is now available in Google Search, starting with AI Mode — marking the first time we brought a Gemini model to Search on day one. Gemini 3 grasps depth and nuance, and unlocks new experiences in Search with dynamic visual layouts, interactive tools and simulations tailored specifically for your query. Google AI Pro and Ultra subscribers in nearly 120 countries and territories in English can use Gemini 3 Pro by selecting “Thinking with 3 Pro” from the model drop-down menu in AI Mode. We unveiled Nano Banana Pro, built on Gemini 3 . Nano Banana Pro, our newest image generation and editing model, is built on Gemini 3 and moves beyond spontaneous art into an era of high-fidelity, studio-quality visuals. You now have the choice between the original Nano Banana for fun, fast editing, or Pro for an even more powerful creative partner capable of handling complex tasks demanding the highest quality. To help get you started, we shared seven tips to get the most out of Nano Banana Pro . We introduced Google Antigravity, a new agentic development platform . Antigravity is a platform designed to give developers an AI-powered coding experience that goes beyond simple editing. It delivers a new agent-first interface for deploying agents that autonomously plan, execute and verify complex tasks. Our vision for Antigravity is to enable anyone with an idea to experience liftoff and build that idea into reality. You can try Antigravity for yourself, available today in public preview. We announced that Google Maps is getting smarter with Gemini . With the help of Gemini, you will soon have the first, hands-free, conversational driving experience in Google Maps that allows you to find places, report traffic, ask for suggestions along your route and more using just your voice. Plus, new landmark-based navigation will give you clear directions, so in addition to hearing “turn right in 500 feet,” you’ll also get directions based on helpful landmarks like “turn right after the Thai Siam Restaurant.” Landmark-based navigation is rolling out now on Android and iOS in the U.S, and Gemini in navigation on Google Maps is rolling out everywhere Gemini is available . We announced that Gemini has started rolling out in Android Auto . Android Auto is already available in over 250 million cars on the road, and Gemini is now coming along for the ride to make life on the road even better. You’ll be able to use natural language to add stops, send messages, access emails, create playlists and even brainstorm ideas while driving. Just make sure you have the Gemini app on your phone and look for the tooltip on your car display. We introduced SIMA 2, a significant step toward Artificial General Intelligence (AGI) . SIMA 2 is a major milestone in our work to create general and helpful AI agents. By integrating the advanced capabilities of Gemini into SIMA 2, the model is evolving from an instruction-follower into an interactive gaming companion. Now, SIMA 2 can follow human-language instructions in virtual worlds and also think about its goals, converse with users and improve itself over time, marking an important step in the direction of robotics and AI-embodiment in general. We released WeatherNext 2: our most advanced weather forecasting model . WeatherNext 2 can generate forecasts 8x faster and with resolution up to 1-hour, and we’re already using this breakthrough technology to support weather agencies in making decisions. We marked the 5-year anniversary of AlphaFold cracking the protein folding problem, and spotlighted its ongoing impact . Five years ago AlphaFold 2 solved the protein structure prediction problem. The profound scientific and societal value of this work was recognized in 2024 with the Nobel Prize in Chemistry. In November, we looked back at how AlphaFold has unlocked new avenues of biological research and provided our first major proof point that AI can be a powerful tool to advance science. We announced new ways to plan travel with AI in Search . Our new AI features in Search help you build the perfect itinerary, from snagging a great deal to turning your plans into actual bookings. To get started with planning, try the Canvas tool in AI Mode: Describe the kind of trip you want and what recommendations you need. Then hit "Create Canvas," and watch your custom travel plan come together. We added new AI shopping features in Search and Gemini to help with the holidays . Our biggest upgrade to shopping means you can now use conversational AI and agentic AI to take the hard work out of your holiday shopping. Using AI Mode in Search to shop, you can now describe what you’re looking for and get an intelligently organized response that brings together rich visuals, price, reviews, inventory info, and more. Plus, new AI agents can now call stores to check stock and use agentic checkout to buy items automatically from eligible merchants when the price is right. We announced new commitments to AI learning and education . At our AI for Learning Forum in London, we announced $30 million in new funding for learning. The event brought together experts in education and technology and continued our recent work to develop AI in a way that improves learning outcomes. We also released popular new features to help with daily studying, including the ability to create flashcards and quizzes right in the NotebookLM app . We shared essential tips for using Gemini Live . Gemini Live’s latest updates allow you to have more natural, two-way conversations with AI. Our tips show how to make the most of these updates by tailoring your learning, especially for complex subjects. You can now adjust Gemini's speech speed to learn at your own pace and improve accessibility. You can also practice a new language, rehearse for a job interview or even liven your conversation up with a fun accent. We announced a new $40 billion investment in Texas for AI and cloud infrastructure . CEO of Google and Alphabet Sundar Pichai and Texas Governor Greg Abbott made the announcement at an event in Midlothian, TX. This latest announcement represented the capstone of our 2025 push to make AI investments that unlock economic opportunity, advance scientific breakthroughs and create opportunities that benefit everyone. It includes major investments across America , as well as Europe , Africa and the Asia-Pacific region — alongside a critical US workforce initiative to train 100,000 electrical workers and create 30,000 new apprentices. POSTED IN: AI Gemini Developers Search Google DeepMind Maps Android Shopping Learning & Education Google Labs Company announcements Google in Europe Google.org]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 19:45:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/google-ai-updates-november-2025/</guid>
    </item>
    <item>
      <title>New research from Google Workspace reveals how young leaders are using AI at work.</title>
      <link>https://blog.google/products/workspace/young-leaders-survey-ai/</link>
      <description><![CDATA[Google Workspace has released findings from our second survey that looks at how people aged 22-39 are using AI at work. Commissioned by Workspace in partnership with the Harris Poll, the findings reveal three key themes: Young leaders want AI that offers more personalized responses — think outputs that aren’t just generic, but use your preferred tone and style. They are taking a very hands-on approach to using this technology, becoming AI architects for their own workflows. AI is helping young leaders feel more confident at work: They’re increasingly relying on it as a tool for professional development. Check out the press release for the full data, as well as insights from Google Workspace’s VP of Product, Yulie Kwon Kim.]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/workspace/young-leaders-survey-ai/</guid>
    </item>
    <item>
      <title>Gemini 3 Pro: the frontier of vision AI</title>
      <link>https://blog.google/technology/developers/gemini-3-pro-vision/</link>
      <description><![CDATA[Your browser does not support the audio element. Listen to article This content is generated by Google AI. Generative AI is experimental [[duration]] minutes Voice Speed Voice Speed 0.75X 1X 1.5X 2X Gemini 3 Pro represents a generational leap from simple recognition to true visual and spatial reasoning. It is our most capable multimodal model ever, delivering state-of-the-art performance across document, spatial, screen and video understanding. This model sets new highs on vision benchmarks such as MMMU Pro and Video MMMU for complex visual reasoning, as well as use-case-specific benchmarks across document, spatial, screen and long video understanding. 1. Document understanding Real-world documents are messy, unstructured, and difficult to parse — often filled with interleaved images, illegible handwritten text, nested tables, complex mathematical notation and non-linear layouts. Gemini 3 Pro represents a major leap forward in this domain, excelling across the entire document processing pipeline — from highly accurate Optical Character Recognition (OCR) to complex visual reasoning. Intelligent perception To truly understand a document, a model must accurately detect and recognize text, tables, math formulas, figures and charts regardless of noise or format. A fundamental capability is "derendering" — the ability to reverse-engineer a visual document back into structured code (HTML, LaTeX, Markdown) that would recreate it. As illustrated below, Gemini 3 demonstrates accurate perception across diverse modalities including converting an 18th-century merchant log into a complex table, or transforming a raw image with mathematical annotation into precise LaTeX code. Example 1: Handwritten Complex Table from 18th century Albany Merchant’s Handbook Example 2: Reconstructing equations from an image Example 3: Reconstructing Florence Nightingale's original Polar Area Diagram into an interactive chart (with a toggle!) Sophisticated reasoning Users can rely on Gemini 3 to perform complex, multi-step reasoning across tables and charts — even in long reports. In fact, the model notably outperforms the human baseline on the CharXiv Reasoning benchmark (80.5%). To illustrate this, imagine a user analyzing the 62-page U.S. Census Bureau " Income in the United States: 2022 " report with the following prompt: “Compare the 2021–2022 percent change in the Gini index for "Money Income" versus "Post-Tax Income", and what caused the divergence in the post-tax measure, and in terms of "Money Income", does it show the lowest quintile's share rising or falling?” Swipe through the images below to see the model's step-by-step reasoning. Visual Extraction: To answer the Gini Index Comparison question, Gemini located and cross-referenced this info in Figure 3 about “Money Income decreased by 1.2 percent” and in Table B-3 about “Post-Tax Income increased by 3.2 percent” Causal Logic: Crucially, Gemini 3 does not stop at the numbers; it correlates this gap with the text’s policy analysis, correctly identifying Lapse of ARPA Policies and the end of Stimulus Payments are the main causes. Numerical Comparison: To compare the lowest quantile’s share rising or falling, Gemini3 looked at table A-3, and compared the number of 2.9 and 3.0, and concluded that “the share of aggregate household income held by the lowest quintile was rising.” Final Model Answer 2. Spatial understanding Gemini 3 Pro is our strongest spatial understanding model so far. Combined with its strong reasoning, this enables the model to make sense of the physical world. Pointing capability: Gemini 3 has the ability to point at specific locations in images by outputting pixel-precise coordinates. Sequences of 2D points can be strung together to perform complex tasks, such as estimating human poses or reflecting trajectories over time. Open vocabulary references: Gemini 3 identifies objects and their intent using an open vocabulary. The most direct application is robotics: the user can ask a robot to generate spatially grounded plans like, “Given this messy table, come up with a plan on how to sort the trash.” This also extends to AR/XR devices, where the user can request an AI assistant to “Point to the screw according to the user manual.” 3. Screen understanding Gemini 3.0 Pro’s spatial understanding really shines through its screen understanding of desktop and mobile OS screens. This reliability helps make computer use agents robust enough to automate repetitive tasks. UI understanding capabilities can also enable tasks like QA testing, user onboarding and UX analytics. The following computer use demo shows the model perceiving and clicking with high precision. 4. Video understanding Gemini 3 Pro takes a massive leap forward in how AI understands video, the most complex data format we interact with. It is dense, dynamic, multimodal and rich with context. High frame rate understanding: We have optimized the model to be much stronger at understanding fast-paced actions when sampling at >1 frames-per-second. Gemini 3 Pro can capture rapid details — vital for tasks like analyzing golf swing mechanics. By processing video at 10 FPS—10x the default speed—Gemini 3 Pro catches every swing and shift in weight, unlocking deep insights into player mechanics. 2. Video reasoning with “thinking” mode: We upgraded "thinking" mode to go beyond object recognition toward true video reasoning. The model can now better trace complex cause-and-effect relationships over time. Instead of just identifying what is happening, it understands why it is happening. 3. Turning long videos into action: Gemini 3 Pro bridges the gap between video and code. It can extract knowledge from long-form content and immediately translate it into functioning apps or structured code. 5. Real-world applications Here are a few ways we think various fields will benefit from Gemini 3’s capabilities. Education Gemini 3.0 Pro’s enhanced vision capabilities drive significant gains in the education field, particularly for diagram-heavy questions central to math and science. It successfully tackles the full spectrum of multimodal reasoning problems found from middle school through post-secondary curriculums. This includes visual reasoning puzzles (like Math Kangaroo ) and complex chemistry and physics diagrams. Gemini 3’s visual intelligence also powers the generative capabilities of Nano Banana Pro . By combining advanced reasoning with precise generation, the model, for example, can help users identify exactly where they went wrong in a homework problem. Prompt: “Here is a photo of my homework attempt. Please check my steps and tell me where I went wrong. Instead of explaining in text, show me visually on my image.” (Note: Student work is shown in blue; model corrections are shown in red). [ See prompt in Google AI Studio ] Medical and biomedical imaging Gemini 3 Pro 1 stands as our most capable general model for medical and biomedical imagery understanding, achieving state-of-the-art performance across major public benchmarks in MedXpertQA-MM (a difficult expert-level medical reasoning exam), VQA-RAD (radiology imagery Q&A) and MicroVQA (multimodal reasoning benchmarks for microscopy based biological research). Input image from MicroVQA - a benchmark for microscopy-based biological research Law and finance Gemini 3 Pro’s enhanced document understanding helps professionals in finance and law tackle highly complex workflows. Finance platforms can seamlessly analyze dense reports filled with charts and tables, while legal platforms benefit from the model's sophisticated document reasoning. 6. Media resolution control Gemini 3 Pro improves the way it processes visual inputs by preserving the native aspect ratio of images. This drives significant quality improvements across the board. Additionally, developers gain granular control over performance and cost via the new media_resolution parameter. This allows you to tune visual token usage to balance fidelity against consumption: High resolution: Maximizes fidelity for tasks requiring fine detail, such as dense OCR or complex document understanding. Low resolution: Optimizes for cost and latency on simpler tasks, such as general scene recognition or long-context tasks. For specific recommendations, refer to our Gemini 3.0 Documentation Guide . Build with Gemini 3 Pro We are excited to see what you build with these new capabilities. To get started, check out our developer documentation or play with the model in Google AI Studio today. POSTED IN: Developers AI Gemini Models]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 05 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/developers/gemini-3-pro-vision/</guid>
    </item>
    <item>
      <title>Look back on your 2025 with Google Photos Recap</title>
      <link>https://blog.google/products/photos/google-photos-2025-recap/</link>
      <description><![CDATA[Another year is almost in the books, and it’s time to look back at the moments that made it memorable. Last year , we introduced Google Photos Recap to help you rediscover what made your year special. Now, Recap is back for 2025, turning your photos and videos from the past year into a highlight reel you can enjoy and share. This year, we’ve added new ways for you to personalize, create and share your Recap. Look back at the moments that made your year Your Recap highlights memorable photos and moments from your year, paired with fun graphics and cinematic effects. You’ll see familiar photo stats from last year, like your top people and total photo count — and this year we also added a selfie count. And for those in the U.S. with Gemini features in Photos enabled, your Recap will also showcase your standout hobbies and top highlights from the year. We also heard your feedback and added the option to hide specific people or photos. Once you do, simply regenerate for an updated version. Create and share with your favorite apps You can get even more creative with a new integration: Just click the “Edit with CapCut” button at the end of your Recap to easily export it to CapCut, where you can choose from exclusive Google Photos templates so you can make a version that’s uniquely yours. Sharing your favorite moments from your Recap is also easier. At the end of your Recap, you’ll find a new carousel of short videos and collages from your lookback that are made for sharing on social or to the group chat. There’s also a new option to share your Recap directly to your WhatsApp Status. Relive your Recap all month long You can find your Recap in the Memories carousel starting this week. If it’s not there, you may see an option at the top of the app where you can request for Photos to create it. After you’ve watched your Recap, it will remain in the back of your Memories carousel and will be pinned in your Collections tab for all of December. You’ll also see a series of related highlights focused on 2025 in your Memories carousel throughout the month of December, featuring some more of your top moments from the year. Open the Google Photos app to explore, customize and share your 2025 Recap. We're already seeing some of the year's biggest stars giving it a try — check it out! POSTED IN: Photos AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/photos/google-photos-2025-recap/</guid>
    </item>
    <item>
      <title>We’re announcing new health AI funding, while a new report signals a turning point for health in Europe.</title>
      <link>https://blog.google/technology/health/ai-health-fund-eu/</link>
      <description><![CDATA[At the European Health Summit in Brussels, Greg Corrado, Distinguished Scientist at Google, released a new report authored by Implement Consulting Group and commissioned by Google revealing that AI is reversing the long-term trend of slowing scientific productivity, providing a turning point for a European healthcare system grappling with rising costs and workforce shortages. The report highlights how AI is already giving practitioners "time back," such as cutting emergency room wait times by over an hour. Building on this momentum, Google announced at the Summit $5 million in funding from Google.org for Bayes Impact to launch "Impulse Healthcare," a new EU health initiative. The project will empower frontline nurses, doctors, and administrators to build and experiment their own AI solutions on Bayes Impact's open-source platform. The end goal is to scale these practitioner-led innovations across the EU, freeing up precious clinical time to improve patient care.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 12:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/health/ai-health-fund-eu/</guid>
    </item>
    <item>
      <title>We’re celebrating Geoffrey Hinton’s Nobel-winning legacy with the University of Toronto.</title>
      <link>https://blog.google/technology/ai/hinton-chair-toronto/</link>
      <description><![CDATA[Today, we are celebrating the extraordinary impact of Nobel Prize-winner Geoffrey Hinton by investing in the future of the field he helped build. Google is proud to support the University of Toronto with $10 million CAD to establish the Hinton Chair in Artificial Intelligence. Geoff’s work on neural networks — spanning his time in academia and his decade here at Google — laid the foundation for modern AI. This chair honors his legacy and will help the university recruit visionary scholars dedicated to the same kind of curiosity-driven, fundamental research that Geoff championed. We are proud to support the next generation of breakthrough innovations and research at University of Toronto — a global leader in AI research.]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 03 Dec 2025 00:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/hinton-chair-toronto/</guid>
    </item>
    <item>
      <title>Use Circle to Search and Google Lens to spot scam messages.</title>
      <link>https://blog.google/products/search/scam-detection-circle-to-search-lens/</link>
      <description><![CDATA[One trending tactic among scammers involves sending fraudulent text messages, either directly to your phone or through messaging apps and social media sites. These messages often solicit or demand money and link out to scammy sites. To help you spot these scams, we’ve now added new capabilities to Circle to Search and Lens that will help you see the telltale signs so you can avoid getting deceived. Here’s how to use it: To use this with Circle to Search : Simply long press the home button or navigation bar of your Android device Circle the suspicious text This capability is also available through Lens , via the Google app ( Android and iOS ) with three short steps: Take a screenshot of the message Open Lens in the Google app Tap the screenshot After you follow these steps on your device, our systems will use AI and information from the web to assess whether the message is likely a scam. You’ll see an overview with guidance and insights including suggested next steps. This capability is available globally in Circle to Search and Lens, and will appear when our systems have high confidence in the quality of the response.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 02 Dec 2025 19:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/scam-detection-circle-to-search-lens/</guid>
    </item>
    <item>
      <title>Gemini 3 and Nano Banana Pro in Search are coming to more countries around the world.</title>
      <link>https://blog.google/products/search/gemini-3-ai-mode-more-countries/</link>
      <description><![CDATA[We're bringing our most intelligent model, Gemini 3 , to AI Mode in Google Search in nearly 120 countries and territories in English. Starting today, Google AI Pro and Ultra subscribers can begin using Gemini 3 Pro by tapping “Thinking with 3 Pro” in the model drop-down in AI Mode. Thanks to Gemini 3’s state-of-the-art reasoning capabilities, AI Mode can grasp the nuance of your most complex queries. And Gemini 3’s unparalleled multimodal understanding and powerful agentic coding capabilities are unlocking new generative user interfaces – so you can get dynamic visual layouts, interactive tools and custom simulations — all generated on the fly, specifically for your query. We're also bringing our latest generative imagery model, Nano Banana Pro , to AI Mode in more countries in English, starting today with Google AI Pro and Ultra subscribers. Built on Gemini 3 Pro, Nano Banana Pro uses Gemini’s advanced reasoning and connects to Search's vast knowledge base to help you visualize infographics and more. With this expansion, it's now easier to ask anything and instantly get a richer, more helpful understanding.]]></description>
      <author>Google AI</author>
      <pubDate>Mon, 01 Dec 2025 20:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/search/gemini-3-ai-mode-more-countries/</guid>
    </item>
    <item>
      <title>Here’s how researchers in Asia-Pacific are using AlphaFold</title>
      <link>https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</link>
      <description><![CDATA[Proteins are the microscopic machines that drive nearly every biological process, from how our bodies fight disease to how life itself evolves. Their function is defined by their unique 3D shape, but for decades, mapping that shape was a slow and expensive process. Five years ago, we introduced AlphaFold, an AI system that accurately predicts protein structures based solely on their amino acid sequences, and made it freely available to the world. Today, AlphaFold is used by over three million researchers, with more than a third of them in the Asia-Pacific region alone. From tackling deadly diseases to discovering entirely new forms of life, here are five ways researchers in this region have used AlphaFold to make breakthroughs in medicine and science: 1. Fighting a ‘silent killer’ in Malaysia Melioidosis, a disease caused by the bacterium Burkholderia pseudomallei — most commonly contracted through contaminated soil or water — claims nearly 90,000 lives each year. At the National University of Malaysia, Dr. Su Datt Lam’s team is using AlphaFold to understand how the bacterium’s proteins help it survive and spread , accelerating the development of new drugs to combat this "silent killer." 2. Uncovering new clues to Parkinson’s disease in Singapore Researchers Jackwee Lim and Yinxia Chao at Singapore’s Agency for Science, Technology and Research (A*STAR) and National Neuroscience Institute (NNI) used AlphaFold to create a 3D visualization of a protein linked to Parkinson's, a neurodegenerative disease. This revealed how the body's own immune system can disrupt the protein’s function, opening new paths for earlier diagnosis and targeted therapies. 3. Seeing the invisible drivers of disease in Korea At Korea’s Advanced Institute of Science & Technology (KAIST), Professor Ji-Joon Song’s team studies how disruptions in the way DNA is organized can lead to cancer and other diseases. AlphaFold allowed his team to map previously unseen regions of a key protein, uncovering a hidden interaction site. As he puts it, "AlphaFold is like the internet for structural biology." 4. Discovering a new frontier of protein shapes in Taiwan Dr. Danny Hsu’s team at Academia Sinica used AlphaFold to study a protein of unknown structure, and predicted a "71-torus knot" — an exceptionally complex protein fold. The team later confirmed this prediction in the lab, proving AlphaFold can help scientists discover entirely new phenomena. 5. Finding new life in Japan’s hot springs While studying microbes in the hot springs of Japan, Dr. Syun-ichi Urayama’s team discovered unusual viruses . Using AlphaFold to predict their protein structures, the team confirmed they belonged to a widespread, previously unknown family of life, revealing a new branch of molecular evolution. These five stories from the Asia-Pacific region — where we have over 13,000 research papers citing AlphaFold — represent just a fraction of what scientists have discovered since we introduced the technology. From tackling neglected diseases to revealing evolutionary history, AlphaFold has fundamentally changed biological research, empowering a global community of scientists to accelerate their work and tackle humanity's greatest challenges. Learn more about AlphaFold — including the co-award of the 2024 Nobel Prize in Chemistry to Demis Hassabis and John Jumper for its development — on the Google Deepmind blog . POSTED IN: Google in Asia AI]]></description>
      <author>Google AI</author>
      <pubDate>Wed, 26 Nov 2025 23:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/around-the-globe/google-asia/researchers-asia-pacific-alphafold-five-years/</guid>
    </item>
    <item>
      <title>Get an in-depth look at Gemini 3 with CEO Sundar Pichai.</title>
      <link>https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</link>
      <description><![CDATA[For the latest episode of the Google AI: Release Notes podcast, host Logan Kilpatrick sat down with CEO Sundar Pichai last week to discuss the extraordinary progress — and future — of AI at Google. Sundar shared the thinking behind going “AI-first” all the way back in 2016, setting up key investments that led the company to this moment. And beyond current releases like Gemini 3 and Nano Banana Pro, he also shared his excitement about long-term bets for the next decade, like quantum computing. "I think in about five years we'll be having breathless excitement about quantum, hopefully, like we are having with AI today,” he said. Watch the full conversation below, or listen to the Google AI: Release Notes podcast on YouTube , Apple Podcasts or Spotify .]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 23:39:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/sundar-pichai-ai-release-notes-podcast/</guid>
    </item>
    <item>
      <title>The Google guide for holiday help</title>
      <link>https://blog.google/technology/ai/holiday-planning-ai-gemini-tips/</link>
      <description><![CDATA[Check out our tips, trends and more for tackling any seasonal stress. Offload tedious tasks to Gemini, get insider recommendations from Google Maps and be sure you’re getting the best price when shopping on Google. Get the help you need — and then get back to enjoying the holiday festivities.]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 18:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/ai/holiday-planning-ai-gemini-tips/</guid>
    </item>
    <item>
      <title>4 ways to refine your content in Flow</title>
      <link>https://blog.google/technology/google-labs/flow-refine-videos/</link>
      <description><![CDATA[We built Flow to help creatives bring their ideas to life, and so far we’ve seen over 500 million videos created since launching in May. Along the way we’ve heard your feedback: you want more precision and control, with both images and videos. Over the past few weeks, we’ve introduced new features to help give you more creative control in Flow with new refinement and editing capabilities. Here are four ways you can refine and edit your content in Flow: 1. Generate and edit images with Nano Banana Pro. In Flow, you can use images to serve as the characters, subjects and starting points for your clips. These might be images you upload, or ones you create in Flow with our new “Images” tab. This new dedicated workspace for generating and refining images is powered by Imagen and Nano Banana for free users, and subscribers also have access to Nano Banana Pro , our newest state-of-the-art image model which provides improved professional-grade controls like depth of focus, lighting and color grading. With simple prompts, you can change a character's outfit or alter a pose, camera angle or lighting without re-rolling the entire scene. You can also blend elements from multiple reference images to create your perfect frame while preserving the critical details of your ingredients. 2. Prompt by doodling. Instead of wordsmithing the perfect prompt, you can draw or annotate on an image, Flow understands your doodles and incorporates them into your final frame. You can doodle directly in Flow instead of turning to a separate editing app. 3. Insert and remove objects. We’ve heard feedback that sometimes when creating in Flow, you might want your clip to stay exactly the same, except for one missing piece — or one thing you don’t love. Tap the pencil icon on any clip to insert objects directly into videos or remove elements, without changing anything else. Object removal is rolling out next month and will be experimental. 4. Adjust your camera motion with reshoot. Sometimes, the camera angle or motion doesn’t come out exactly as you’d imagined, or you want to play around and see how clips look from different perspectives. We introduced a new feature to help adjust the camera position, orbit, or move the “dolly” in any of your generated videos. The Camera Adjustment feature works best for clips that don’t currently include camera motion. We hope these capabilities help you create even more dynamic content in Flow — with more precision and control. POSTED IN: Google Labs AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 17:40:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/technology/google-labs/flow-refine-videos/</guid>
    </item>
    <item>
      <title>3 things to know about Ironwood, our latest TPU</title>
      <link>https://blog.google/products/google-cloud/ironwood-google-tpu-things-to-know/</link>
      <description><![CDATA[Today's most advanced AI models, like those powering complex thinking and calculations, need speed and efficiency from the hardware that powers them. That's why at Cloud Next in April, we unveiled Ironwood , our seventh-generation Tensor Processing Unit (TPU). Ironwood is our most powerful, capable, and energy-efficient TPU yet, designed to power thinking, inferential AI models at scale. By acting as a hugely efficient parallel processor, Ironwood excels at managing massive calculations and significantly minimizes the internal time required for data to shuttle across the chip. This breakthrough dramatically speeds up complex AI, making models run significantly faster and smoother across our cloud. And now, Ironwood is here for Cloud customers. Here are three things to know about it. 1. It’s purpose-built for the age of inference As the industry’s focus shifts from training frontier models to powering useful, responsive interactions with them, Ironwood provides the essential hardware. It’s custom built for high-volume, low-latency AI inference and model serving. It offers more than 4X better performance per chip for both training and inference workloads compared to our last generation , making Ironwood our most powerful and energy-efficient custom silicon to date. 2. It’s a giant network of power TPUs are a key component of AI Hypercomputer , our integrated supercomputing system designed to boost system-level performance and efficiency across compute, networking, storage and software. At its core, the system groups individual TPUs into interconnected units called pods. With Ironwood, we can scale up to 9,216 chips in a superpod. These chips are linked via a breakthrough Inter-Chip Interconnect (ICI) network operating at 9.6 Tb/s. Part of an Ironwood superpod, directly connecting 9,216 Ironwood TPUs in a single domain. This massive connectivity allows thousands of chips to rapidly communicate and access a staggering 1.77 Petabytes of shared High Bandwidth Memory (HBM), overcoming data bottlenecks for even the most demanding models. This efficiency significantly reduces the compute-hours and energy required for training and running cutting-edge AI services. 3. It’s designed for AI with AI Ironwood is the result of a continuous loop at Google where researchers influence hardware design, and hardware accelerates research. While competitors rely on external vendors, when Google DeepMind needs a specific architectural advancement for a model like Gemini, they collaborate directly with their TPU engineer counterparts. As a result, our models are trained on the newest TPU generations, often seeing significant speedups over previous hardware. Our researchers even use AI to design the next chip generation — a method called AlphaChip — which has used reinforcement learning to generate superior layouts for the last three TPU generations, including Ironwood. POSTED IN: Google Cloud AI]]></description>
      <author>Google AI</author>
      <pubDate>Tue, 25 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/google-cloud/ironwood-google-tpu-things-to-know/</guid>
    </item>
    <item>
      <title>16 Google AI tips for stress-free holiday hosting in 2025</title>
      <link>https://blog.google/products/pixel/google-holiday-hosting-tips-2024/</link>
      <description><![CDATA[If you’re brave enough to host celebrations this holiday season, you don’t have to go it alone. Plenty of Google AI tools can make the process easier and more fun, whether you’ve held countless holiday gatherings or it’s your first time putting on your hosting hat. Try these holiday hosting tips and helpful features, from the planning phase to party time. (And check out more tips and prompts for other holiday planning , too!) Stay organized while you plan 1. Make a holiday party planner Gem in Gemini A Gem is a custom expert you can make for any task within the Gemini app (where our latest Gemini model, Gemini 3 Pro , is rolling out globally). You can give your Gem unique context and revisit your AI expert as you plan. Try instructions like, “You’re an experienced party planner who can help me host a creative holiday dinner for 10 of my closest friends.” Then you can ask for help with punny menu names, invitation wording, themed playlists or icebreakers and more. Each time, the Gem will use that earlier context to inform its response. You can even ask Gemini to help you with instructions for your Gem and share your Gem with other people — perfect if you’re co-hosting! 2. Chat with Gemini Live on the go If you find yourself wondering about decoration DIYs when you’re at the craft store, you don’t have to tap at your screen — just open up Gemini Live in the Gemini app to brainstorm together. Thanks to its camera and screen sharing capabilities, you can have a back-and-forth conversation with Gemini Live about anything you see, and Gemini Live can offer visual guidance as it responds. You can even interrupt if you want to add more details or change the topic. 3. Use Nano Banana Pro for creative pics Nano Banana Pro , our latest image generation and editing model, can help you put a funny, personal touch on your gathering. Use it in the Gemini app or AI Mode in Search to prep invitation materials, like a photo showing you inside a snowglobe, or put your pet in a festive outfit for party reminders. Nano Banana Pro can help you (and your guests) get in the holiday party spirit with visuals for party invitations and reminders with a prompt like “Create a claymation scene showing a dog dressed up like an elf in a winter wonderland.” 4. Dream up decor ideas with Mixboard Mixboard is an AI experiment from Google Labs that helps you visualize and explore your ideas. Let’s say you want to make a holiday-themed tablescape but aren’t sure what it should look like — Mixboard can help you expand and refine the different possibilities using both images and text. 5. Reference planning info with Pixel Screenshots Use Pixel Screenshots on Pixel 9 and Pixel 10 to quickly and easily search your screenshots and find the info you need. You can even create collections of screenshots for easier party planning. So if you’re hosting a get-together, create a collection for that event and save helpful screenshots of recipes, DIY ideas and texts from people you’ve invited. Then, you’ll be able to search things like “ingredients,” “craft supplies” or “arriving late” and find the relevant information from the respective screenshots. 6. Research ideas with Gemini in Chrome Gemini in Chrome has tons of features to make party planning easier. It can work across multiple tabs to consolidate information, like which of the recipes you’re considering are gluten-free. It can also help you quickly find webpages you previously visited, so you don’t have to go digging for inspiration you stumbled upon. And it works with your favorite Google apps without changing tabs, making it easier to do things like create events in your Calendar without leaving the page. 7. Use Search to shop for decor and party favors A big shopping upgrade to Search means you can shop more conversationally — tell AI Mode what decorations or favors you’re looking for as you would to a friend. When you know what you want, you can use agentic capabilities in Search to call local stores on your behalf and see what products are in stock nearby, or let you know when an item online has fallen within your budget — and even buy it for you if the merchant is eligible. Simplify the cooking process 8. Rely on Gemini for Home as your right-hand helper Gemini for Home replaces Google Assistant on your smart displays and speakers and also upgrades devices in your home like your cameras and doorbells. There are plenty of ways to use it for holiday hosting. Just say “Hey Google,” then ask Gemini to add ingredients for a cozy appetizer to your shopping list or set timers as you get to work in the kitchen. (You can even do this with your Pixel Watch 4 using its Raise to Talk feature if your hands are full.) And if you want to have an even more free-flowing conversation while you cook, you can say, "Hey Google, let's chat," to talk to Gemini Live. 1 9. Get quick visual cooking help from Google Lens With Google Lens , you can take a photo of ingredients sitting in front of you and ask your question out loud . For example, you can take a photo of your onions and ask “exactly what kind of onions are these and what’s the best way to cook them?” 10. Use Gemini to navigate dietary restrictions To make sure your guests don’t leave hungry, ask your Gem for help finding recipes everyone can enjoy based on dietary restrictions. Or try double-checking with Gemini Live that a specific ingredient really is gluten-free before you start whisking! Enjoy the party with your guests 11. Solve party emergencies with AI Overviews Spills don’t have to be a disaster: AI Overviews in Search can get you instant solutions when every second counts. Search for something like "is salt or baking soda better for stain removal?” to get the information you need with links to learn more — helping you save the day and get back to your guests in no time. 12. Handle logistics in the Google Home app Our redesigned Google Home app lets you keep the good vibes going from your phone. The app houses your favorite Nest and other smart home devices and features: You can adjust your Nest Thermostat; unlock the front door as guests arrive and lower the lights to set the mood. The consolidated Home tab now lets you swipe through your pinned Favorites, devices by type, and more. We’ve added full support for the Nest x Yale lock and lock passcode management. We’ve added full support for Nest Thermostats (2015 or later) and features like schedules and hot water boost. 13. Use Magic Cue to quickly answer texts No need to field the same questions over and over as the party gets underway — Magic Cue on Pixel 10 anticipates your needs and links information across your apps. So instead of typing the same response to every “what’s your address again?” text, Magic Cue can offer a helpful suggestion with the answer so you can get back to hosting. 14. Grab the perfect group photo with Pixel It’s very possible that you’re going to be the one corralling everyone into a group photo and taking the picture. Luckily, Pixel 10 has lots of features to help you take stellar group pics, like Add Me , which can combine two pictures taken during the same session and in the same scene so everyone — including the person who took the original shot — is in the photo. Tie it all up in a bow 15. Make easy Google Photos edits to perfect your pics New AI features in Google Photos can help you touch up party pictures before sharing them with your guests. With personalized edits, you can request fixes like opening people’s eyes mid-blink. You can also make edits simply by asking — describe the edits you want using your voice or text. 16. Craft thank you cards with Pixel and Google Photos After all the work you put into your gathering, you deserve the lightest-lift thank you cards possible. For just one idea, use “Help me edit” in Google Photos’ editor to add a festive background to party pictures! POSTED IN: AI Gemini Search Pixel Photos Google Nest Shopping]]></description>
      <author>Google AI</author>
      <pubDate>Fri, 21 Nov 2025 16:00:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/products/pixel/google-holiday-hosting-tips-2024/</guid>
    </item>
    <item>
      <title>Develop a deeper understanding with interactive images in Gemini.</title>
      <link>https://blog.google/outreach-initiatives/education/gemini-interactive-images/</link>
      <description><![CDATA[Learning science consistently shows us that true learning requires active engagement. This is fundamental to how Gemini helps you learn. Going beyond simple text and static images, we're now rolling out interactive images to the Gemini app — a new capability designed to help you visually explore complex academic concepts. Imagine studying the digestive system or the parts of the cell. Instead of just seeing a label, you can now tap or click directly on a specific part of the diagram to unlock an interactive panel. This panel provides immediate definitions, detailed explanations and content to deep-dive into. By being able to interact with images, Gemini is transforming studying from passive viewing into active exploration. Now, with certain images, you’ll be able to gain more information about topics and ask follow-up questions. This represents an important step in making learning more visual, dynamic and accessible.]]></description>
      <author>Google AI</author>
      <pubDate>Thu, 20 Nov 2025 20:30:00 GMT</pubDate>
      <guid isPermaLink="true">https://blog.google/outreach-initiatives/education/gemini-interactive-images/</guid>
    </item>
  </channel>
</rss>
